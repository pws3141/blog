---
title: "Getting Started with {mlr3}"
subtitle: "03 Hyperparameter Optimisation"
author:
  - name: Paul Smith
date: "2025-03-21"
categories: [code, r, machine learning, mlr3]
image: "./fig/mlr3_logo.svg"
format:
  html:
    code-fold: false
execute:
  df-print: default
---

# Introduction

I am attempting to learn how to use `{mlr3}` [@lang2019mlr3], by reading through the book
[Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)
[@bischl2024usingmlr3].

In this post, I am working through the exercises given in [Chapter
4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html)
of the book [@becker2024hyperparameter], which covers hyperparameter optimisation (HPO).
This includes the following:

- **Q1:** Tunes `regr.ranger` on `mtcars` using random search and 3-fold CV
- **Q2:** Evaluates tuned model with nested resampling (3-fold CV outer, holdout inner)
- **Q3:** Benchmarks tuned XGBoost vs logistic regression on `spam` using Brier score
        - Uses `AutoTuner`, predefined tuning spaces, and `benchmark()` for comparison

My previous posts cover:

- [Part one](../05-mlr3_basic_modelling/index.qmd):
    - Create a classification tree model to predict diabetes.
    - Look at the confusion matrix and create measures without using {mlr3measures}.
    - Change the thresholds in the model.
- [Part two](../08-mlr3_evaluation_benchmarking/index.qmd):
    - Repeated cross-validation resampling.
    - Using a custom resampling strategy.
    - Creating a function that produces a ROC curve.


## Prerequisites

```{r}
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(mlr3tuning)
library(mlr3tuningspaces)
library(ggplot2)
library(patchwork)
library(data.table)
options(datatable.print.nrows = 20)
```

Suppress all messaging unless it's a warning:^[The packages in `{mlr3}` that
make use of optimization, i.e., `{mlr3tuning}` or `{mlr3select}`, use the
logger of their base package `{bbotk}`.]

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

# Exercises

1. Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of `lrn("regr.ranger")` on `tsk("mtcars")`. Use a simple random search with 50 evaluations. Evaluate with a 3-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.
2. Evaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold CV for the outer resampling.
3. Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.
4. (\*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:

        a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.
        b. Identify the best configuration.
        c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).
        d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.

## Question 1

Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of
`lrn("regr.ranger")` on `tsk("mtcars")`. Use a simple random search with 50
evaluations. Evaluate with a 3-fold CV and the root mean squared error.
Visualize the effects that each hyperparameter has on the performance via
simple marginal plots, which plot a single hyperparameter versus the
cross-validated MSE.

### Answer

Let's load the task and look at the properties of the `mtry`,
`sample.fraction`, and `num.trees` parameters.

```{r}
tsk_mtcars <- tsk("mtcars")
lrn("regr.ranger")$param_set$data[id %in% c("mtry", "sample.fraction", "num.trees")]
```

The hyperparameters I'm looking at are:

- `mtry`: number of variables considered at each tree split.
- `num.trees`: number of trees in the forest.
- `sample.fraction`: fraction of observations used to train each tree.

Now I will set up the tuning of the `mtry`, `sample.fraction`, and `num.trees` hyperparameters.

```{r}
learner <- lrn("regr.ranger",
               mtry = to_tune(p_int(1, 10)),
               num.trees = to_tune(20, 2000),
               sample.fraction = to_tune(0.1, 1))
learner
```

Setting up an instance to terminate the tuner after 50 evaluations, and to use 3-fold CV.

```{r}
instance <- ti(task = tsk_mtcars,
               learner = learner,
               resampling = rsmp("cv", folds = 3),
               # rmse gives interpretability in the original units (MPG) rather than squared units
               measures = msr("regr.rmse"), 
               terminator = trm("evals", n_evals = 50))
instance
```

The tuning step uses 3-fold cross-validation:

- In each evaluation, two-thirds of the data is used for training,
- One-third is used for validation (i.e. to compute the RMSE).
- This is repeated for 50 random configurations (as specified by the terminator).


Now I can set up the tuning process (random search).

```{r}
tuner <- tnr("random_search")
#tuner$param_set
tuner
```

And trigger the tuning process.

```{r}
set.seed(333)
tuner$optimize(instance)
instance$result
instance$result$learner_param_vals
```

All 50 of the random search evaluations are stored in the `archive` slot of the
`instance` object.
```{r}
as.data.table(instance$archive)[, .(mtry, sample.fraction, num.trees, regr.rmse)]
```

Now let's visualise the effect of each hyperparameter using marginal plots.

```{r}
#| fig-width: 8

autoplot(instance, type = "marginal", cols_x = c("mtry", "sample.fraction", "num.trees"))
```

As during the HPO stage, I used 3-fold CV, the model has not seen the full
data all at once. So, now I'll train the model using the optimised
hyperparameters.

```{r}
lrn_ranger_tuned <- lrn("regr.ranger")
lrn_ranger_tuned$param_set$values = instance$result_learner_param_vals
lrn_ranger_tuned$train(tsk_mtcars)$model
```

:::{.callout-note collapse="true"}
# Summary of question 1 (hyperparameter tuning with random search)

I tuned the `regr.ranger` learner on the `mtcars` dataset, focusing on three hyperparameters:

- `mtry`: number of variables considered at each split,
- `num.trees`: number of trees in the forest,
- `sample.fraction`: fraction of the dataset used for each tree.

**Steps I took:**

1. **Exploration**
   I inspected the available parameters for the learner using `$param_set`.

2. **Learner setup**
   I defined the tuning ranges with `to_tune()`:
   - `mtry` from 1 to 10,
   - `num.trees` from 1 to 100,000,
   - `sample.fraction` from 0.1 to 1.

3. **Tuning instance**
   I created a `TuningInstanceSingleCrit` using:
   - the `mtcars` task,
   - 3-fold cross-validation for resampling,
   - root mean squared error (RMSE) as the evaluation metric,
   - a limit of 50 evaluations using a random search strategy.

4. **Running the tuner**
   I used `tnr("random_search")` and called `$optimize()` to run the search. I then extracted the best hyperparameter combination and the archive of evaluated configurations.

5. **Visualising results**
   I used marginal plots to visualise the effect of each hyperparameter on the cross-validated RMSE.

6. **Training the final model**
   I retrained the `regr.ranger` model on the full dataset using the best parameters found.

:::

## Question 2

Evaluate the performance of the model created in Exercise 1 with nested
resampling. Use a holdout validation for the inner resampling and a 3-fold
CV for the outer resampling.

### Answer

OK, so here we need an outer and inner resampling strategy.
The outer resampling strategy will be a 3-fold CV, and the inner resampling
strategy will be a holdout validation.

![An illustration of nested resampling. The large blocks represent 3-fold CV for the outer resampling for model evaluation and the small blocks represent 4-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}

@fig-nested-resampling-example shows an example of a nest resampling strategy
(with 3-fold CV on the outer and 4-fold CV on the inner nest). Here, we
need to do something slightly different as we are using the holdout resampling
strategy on the inner nest.

1. Outer resampling start
   - Perform 3-fold cross-validation on the full dataset.
   - For each outer fold, split the data into:
     - Training set (light blue blocks)
     - Test set (dark blue block)

2. Inner resampling
   - Within each outer training set, perform holdout validation (assuming 70/30 training-test split).
   - This inner split is used for tuning hyperparameters (not evaluation).

3. HPO – Hyperparameter tuning
   - Evaluate different hyperparameter combinations by fitting models on the inner training set and evaluating performance on the inner validation (holdout) set.
   - Select the best hyperparameter configuration based on performance on the inner holdout set. 

4. Training
   - Fit the model on the entire outer training set using the tuned hyperparameters.

5. Evaluation
   - Evaluate the trained model on the outer test set (unseen during tuning).

6. Outer resampling repeats
   - Repeat steps 2–5 for each of the 3 outer folds.

7. Aggregation
   - Average the 3 outer test performance scores.
   - This gives an unbiased estimate of the model’s generalisation performance with tuning.

I will use `AutoTuner` to do nested resampling, as that is what is done in
[Section
4.3.1](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)
of the mlr3 tutorial.

```{r}
## create auto_tuner to resample a random forest 
# with 3-fold CV in outer resampling and 
# holdout validation in inner resampling
at <- auto_tuner(
         tuner = tnr("random_search"), #<1>
         learner = lrn("regr.ranger",
                       mtry = to_tune(1, 1e1),
                       num.trees = to_tune(1, 1e5),
                       sample.fraction = to_tune(0.1, 1)),
         # inner resampling
         resampling = rsmp("holdout", ratio = 0.7),
         measure = msr("regr.rmse"),
         terminator = trm("evals", n_evals = 50)
        )

# resampling step
rr <- resample(tsk("mtcars"),
               at, 
               # outer resampling
               rsmp("cv", folds = 3), 
               store_models = TRUE) #<2>

rr
```
1. The tuners and learners are the same as in the previous exercise, I'm just
   defining them again here for clarity.
2. Set `store_models = TRUE` so that the `AutoTuner` models (fitted on the
   outer training data) are stored,

Now I aggregate across the three outer folds to get the final performance.
```{r}
rr$aggregate()
```

The inner tuning results can also be accessed, returning the optimal configurations (across all outer folds) and the full tuning archives.

```{r}
# optimal configurations
extract_inner_tuning_results(rr)
# full tuning archives
extract_inner_tuning_archives(rr)
```

:::{.callout-note collapse="true"}
# Summary of question 2 (nested resampling)

I evaluated the performance of the tuned model using nested resampling to obtain an unbiased estimate.

**Steps I took:**

1. **Resampling strategy**
   I used:
   - outer resampling: 3-fold cross-validation,
   - inner resampling: holdout validation with a 70/30 split.

2. **AutoTuner setup**
   I reused the same `regr.ranger` learner and parameter ranges as in Question 1, wrapped in an `AutoTuner`. The tuning again used 50 evaluations of random search and MSE as the measure.

3. **Resample execution**
   I called `resample()` with the outer CV and the `AutoTuner`, setting `store_models = TRUE` to keep the fitted models from each outer fold.

4. **Aggregating performance**
   I used `$aggregate()` to average the MSE across the outer test folds.

5. **Inspecting inner results**
   I extracted the best configurations and full tuning logs from each inner loop using `extract_inner_tuning_results()` and `extract_inner_tuning_archives()`.

:::

## Question 3

Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.

### Answer

I’ll use the built-in `spam` task -- since the outcome is categorical, this is a classification task.

```{r}
tsk_spam <- tsk("spam")
```

First I'll set up the logistic regression model (with no tuning).

```{r}
# requires probs to compute the brier score
lrn_logreg <- lrn("classif.log_reg", predict_type = "prob")
```

:::{.column-margin}

The XGBoost model has lots of hyperparameters:
```{r}
lrn("classif.xgboost")$param_set$ids()
```

The main ones are:

| Hyperparameter         | Description                                               | Type     |
|------------------------|-----------------------------------------------------------|----------|
| `eta`                  | Learning rate (shrinkage)                                 | numeric  |
| `max_depth`            | Maximum depth of trees                                    | integer  |
| `nrounds`              | Number of boosting rounds (trees)                         | integer  |
| `colsample_bytree`     | Fraction of features randomly sampled per tree            | numeric  |
| `subsample`            | Fraction of rows sampled per tree                         | numeric  |
| `min_child_weight`     | Minimum sum of instance weights in a child node           | numeric  |
| `gamma`                | Minimum loss reduction to make a split                    | numeric  |
| `lambda`               | L2 regularisation term on weights                         | numeric  |
| `alpha`                | L1 regularisation term on weights                         | numeric  |

A typical tuning strategy for XGBoost might involve:

1. Starting with basic tree shape and learning rate:
   - `max_depth`
   - `eta`
   - `nrounds`

2. Adding sampling and regularisation to control overfitting:
   - `subsample`
   - `colsample_bytree`
   - `min_child_weight`
   - `gamma`

3. Fine-tuning regularisation terms if needed:
   - `lambda` (L2)
   - `alpha` (L1)

:::

For the XGBoost learner, I'm going to use a predefined search space from
`{mlr3tuningspaces}`. First, I'll give a list of these predefined spaces.

```{r}
mlr_tuning_spaces$keys()[grepl("xgboost", mlr_tuning_spaces$keys())]
```

I will use the `classif.xgboost.default` space.
```{r}
space = lts("classif.xgboost.default")
space
```

Plugging this into `auto_tuner()` creates an `AutoTuner` object. I'm going to use
5-fold CV in the inner resampling and a terminator based on run time (of 60
seconds).

```{r}
# create terminator with time budget of 60 secs
trm_rt = trm("run_time")
trm_rt$param_set$values$secs = 60

# create xgboost learner with prob predict_type
# 'prob' required for brier score
lrn_xgb = lrn("classif.xgboost", predict_type = "prob")

at_xgb <- auto_tuner(learner = lrn_xgb,
                    resampling = rsmp("cv", folds = 5),
                    measure = msr("classif.bbrier"),
                    terminator = trm_rt,
                    tuner = tnr("random_search"),
                    search_space = space)
at_xgb
```

Now I can set up the outer resampling strategy (4-fold CV).

```{r}
outer_rsmp <- rsmp("cv", folds = 4)
```

I can create a benchmark grid and run it for the task to compare the two learners.

```{r}
# Benchmark both learners
design = benchmark_grid(
  tasks = tsk_spam,
  learners = list(lrn_logreg, at_xgb),
  resamplings = outer_rsmp
)
design

# run the benchmark design
set.seed(101)
bmr = benchmark(design)
# the score for each of the 4-fold CV outer folds
bmr$score(msr("classif.bbrier"))[, 
         .(learner_id, resampling_id, iteration, classif.bbrier)
         ]
# the aggregate score for each learner
bmr$aggregate(msr("classif.bbrier"))[,
         .(learner_id, resampling_id, classif.bbrier)
         ]
```

I can use `autoplot` to plot these results.
```{r}
autoplot(bmr, measure = msr("classif.bbrier"))
```

So, XGBoost performs better than the logistic regression model on the `spam`
task. But, the XGBoost model is much more computationally expensive, takes
longer to train, and is less interpretable. So, the choice of model is a trade
off between performance and interpretability.

:::{.callout-note collapse="true"}
# Summary of question 3 (XGBoost vs. logistic regression)

I benchmarked a tuned XGBoost model against an untuned logistic regression model on the `spam` classification task using the Brier score.

**Steps I took:**

1. **Loading the task**  
   I used the built-in `tsk("spam")`.

2. **Logistic regression setup**  
   I defined a `classif.log_reg` learner with `predict_type = "prob"` to enable Brier score calculation.

3. **XGBoost setup with tuning**  
   I used `classif.xgboost` with `predict_type = "prob"` and the predefined tuning space `lts("classif.xgboost.default")` from `{mlr3tuningspaces}`.

4. **AutoTuner for XGBoost**  
   I created an `AutoTuner` with:
   - 5-fold CV for inner resampling,  
   - 60-second time budget via `trm("run_time")`,  
   - random search tuner,  
   - Brier score as the measure.

5. **Outer resampling**  
   I used 4-fold CV for the outer loop.

6. **Benchmark setup and execution**  
   I created a benchmark grid comparing both learners on the task, ran the benchmark, and scored the results using the Brier score.

7. **Results**  
   I looked at individual fold scores using `bmr$score()` and aggregate performance using `bmr$aggregate()`. I also visualised the comparison with `autoplot()`.
:::

## Question4

Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:

    a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.
    b. Identify the best configuration.
    c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).
    d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.

### Answer

#### Example on `mtcars` dataset

Let's start by trying this out on the `mtcars` dataset, using the `regr.rpart` learner. I'll make it simpler by using the `to_tune()` function, which means I don't have to define the search space manually to be *e.g.* an integer, double *etc.*.

First, I'll load the task and learner, and then look at the hyperparameters of this learner.

```{r}
#| cache: false
task <- tsk("mtcars")
learner <- lrn("regr.rpart")
# look at the hyperparameters
as.data.table(learner$param_set)[, .(id, class, lower, upper)]
```

:::{.column-margin}

| ID           | Description                                                             | Typical Range           | Notes                                    |
|--------------|-------------------------------------------------------------------------|--------------------------|-------------------------------------------|
| `cp`         | Complexity parameter: controls cost of adding splits                    | [0.001, 0.1] (log-scale) | Lower values allow deeper trees           |
| `maxdepth`   | Maximum depth of the tree                                               | 1–30                     | Prevents trees from growing too deep      |
| `minsplit`   | Minimum number of observations required to attempt a split              | 2–20                     | Higher values make trees more conservative |
| `minbucket`  | Minimum number of observations in any terminal node                     | 1–10                     | If not set, defaults to `minsplit / 3`    |

: Common hyperparameters to tune for `regr.rpart` learner.

:::

**Part 1:** creating a search_space and evaluating the learner

I'll now create a search space to tune the four hyperparameters `cp`,
`maxdepth`, `minsplit`, and `minbucket`, using `to_tune()`.

```{r}
#| cache: false

# use to_tune() to create the search space
learner$param_set$values$cp <- to_tune(1e-4, 0.1)
learner$param_set$values$maxdepth <- to_tune(1, 30)
learner$param_set$values$minsplit <- to_tune(2, 20)
learner$param_set$values$minbucket <- to_tune(1, 10)

param_ids <- names(Filter(function(x) inherits(x, "TuneToken"), learner$param_set$values))
# filter param_set bu param_names
learner$param_set$values[param_ids]
```

Now I'll tune the learner on this search space using a random search with 50 evaluations and 3-fold CV. 

```{r}
#| cache: false

# create random search tuner
tuner <- tnr("random_search")
# create measure
measure <- msr("regr.rmse")
# create resampling strategy
resampling <- rsmp("cv", folds = 3)
# create terminator
terminator <- trm("evals", n_evals = 50)

# create tuning instance and run tuner
set.seed(333)
instance <- mlr3tuning::tune( # <1>
  tuner = tuner,
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure,
  terminator = terminator
)

```
1. I need to remember to call the correct `tune()` - the one from `{mlr3tuning}` instead of from `{e1701}`. If the latter is used, you get an error like `argument "train.x" is missing, with no default`.

**Part 2:** Identifying the best configuration

We can see the result be looking at the `instance` object.

```{r}
instance
```

The best configuration is stored in the `result_learner_param_vals` slot of the `instance` object.

```{r}
# get the best configuration
best_config1 <- instance$result_learner_param_vals[param_ids]
unlist(best_config1)
```

The root mean-squared error is stored in `instance$result_y`:

```{r}
# get the RMSE
best_rmse1 <- instance$result_y
best_rmse1
```


**Part 3:** create a smaller search space around the best configuration

```{r}
# obtain new upper and lower bounds
# helper function to adjust bounds

adjust_bounds <- function(param_id, best_config, learner, shrink = 0.25) {
  param = learner$param_set$values[[param_id]]$content
  bounds = list(lower = param$lower, upper = param$upper)
  best = best_config[[param_id]]
  range = bounds$upper - bounds$lower
  lower_bound <- best - shrink * range
  upper_bound <- best + shrink * range
  # if class is ParamInt, then set lower to be ceiling and upper to be floor
  # obtain class of the parameter
  param_class <- learner$param_set$params[id == param_id, setNames(cls, id)]
  if (param_class == "ParamInt") {
    lower_bound = ceiling(lower_bound)
    upper_bound = floor(upper_bound)
  }
  lower_new = max(lower_bound, bounds$lower)
  upper_new = min(upper_bound, bounds$upper)
  list(lower = lower_new, upper = upper_new)
}

adjusted_bounds <- lapply(param_ids, 
                          adjust_bounds,
                          best_config = best_config1,
                          learner = learner)

names(adjusted_bounds) <- param_ids

# update learner
learner$param_set$values$cp <- to_tune(adjusted_bounds$cp$lower, 
                                       adjusted_bounds$cp$upper)
learner$param_set$values$maxdepth <- to_tune(adjusted_bounds$maxdepth$lower,
                                             adjusted_bounds$maxdepth$upper)
learner$param_set$values$minsplit <- to_tune(adjusted_bounds$minsplit$lower,
                                             adjusted_bounds$minsplit$upper)
learner$param_set$values$minbucket <- to_tune(adjusted_bounds$minbucket$lower,
                                              adjusted_bounds$minbucket$upper)
# check the new search space
Filter(function(x) inherits(x, "TuneToken"), learner$param_set$values)
```

**Part 4:** evaluate the learner on the new search space

```{r}
# create new tuning instance and run tuner
set.seed(222)
instance <- mlr3tuning::tune(
  tuner = tuner,
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure,
  terminator = terminator
)
```

Get the best configuration from this:

```{r}
# get the best configuration
best_config2 <- instance$result_learner_param_vals[param_ids]
unlist(best_config2)
```

and the RMSE:

```{r}
# get the RMSE
best_rmse2 <- instance$result_y
best_rmse2
```

Cool, so I have a new best configuration. Does it perform better than the first?

:::{.column-margin}
```{r}
# Best configuration from the first tuning
best_config1
# RMSE from the first configuration
best_rmse1
# Best configuration from the second tuning
best_config2
# RMSE from the second configuration
best_rmse2
```

```{r}
# compare the two RMSEs
best_rmse2 - best_rmse1
```

No, it doesn't. I can repeat this process iteratively on smaller subspaces. At
the end, choose the best configuration by selecting the one with the lowest
RMSE.

The next step is to create a function that does this for me.

#### Creating a function

So, this function has the following inputs:
- `task`: the task to tune;
- `learner`: the learner to tune;
- `search_space`: the search space to tune;
    - this should be a data.table of the hyperparameters to tune, with class, and the lower and upper bounds;
- `resampling`: the resampling strategy to use;
- `measure`: the measure to use;
- `random_search_stages`: the number of random search stages to perform;
- `random_search_size`: the number of random evaluations to perform in each stage.

The function will return the best configuration found across all stages.

First, I created a checker function to make sure all the inputs are valid.^[Actually I created this last, but it's required before my function so I'll put it here]

```{r}
.inputChecks <- function(task, learner, search_space, resampling, measure, 
                         random_search_stages, random_search_size) {

        ## error checking
        # check input classes
        stopifnot(inherits(task, "Task"))
        stopifnot(inherits(learner, "Learner"))
        stopifnot(inherits(resampling, "Resampling"))
        stopifnot(inherits(measure, "Measure"))
        # check search space: required columns exist and are numeric where needed
        required_cols <- c("id", "lower", "upper")
        missing_cols <- setdiff(required_cols, colnames(search_space))
        if (length(missing_cols) > 0) {
          stop("search_space must contain columns: ", paste(missing_cols, collapse = ", "))
        }

        if (!all(sapply(search_space[, .(lower, upper)], is.numeric))) {
          stop("Columns 'lower' and 'upper' in search_space must be numeric")
        }
        # check ids match those in learner
        invalid_ids <- setdiff(search_space$id, learner$param_set$ids())
        if (length(invalid_ids) > 0) {
          stop("Invalid parameter IDs in search_space: ", paste(invalid_ids, collapse = ", "))
        }
        # check iterations inputs
        stopifnot(is.numeric(random_search_stages), random_search_stages >= 1)
        stopifnot(is.numeric(random_search_size), random_search_size >= 1)
}
```

Now, let's create the function.

```{r}
iterative_random_tuner <- function(task, learner, search_space,
                                   resampling, measure,
                                   random_search_stages, random_search_size,
                                   verbose = TRUE, seed = TRUE) {

  .inputChecks(task, learner, search_space, resampling, measure,
               random_search_stages, random_search_size)

  param_ids <- search_space$id
  tuner <- tnr("random_search")
  terminator <- trm("evals", n_evals = random_search_size)

  # initialise search space on learner
  learner$param_set$values[param_ids] <- Map(to_tune,
                                             search_space$lower,
                                             search_space$upper)

  best_config_list <- list()

  if (seed) set.seed(22)
  for (stage in seq_len(random_search_stages)) {
    if (stage > 1) {
      # update search space based on previous best
      adjusted_bounds <- lapply(param_ids, adjust_bounds,
                                best_config = best_config,
                                learner = learner)
      names(adjusted_bounds) <- param_ids
      learner$param_set$values[param_ids] <- Map(to_tune,
                                                 lapply(adjusted_bounds, `[[`, "lower"),
                                                 lapply(adjusted_bounds, `[[`, "upper"))
    }

    # tune
    instance <- mlr3tuning::tune(
      tuner = tuner,
      task = task,
      learner = learner,
      resampling = resampling,
      measure = measure,
      terminator = terminator
    )

    best_config <- unlist(instance$result_learner_param_vals[param_ids])
    best_score <- instance$result_y

    best_config_list[[stage]] <- list(config = best_config, score = best_score)

    if (verbose) cat(sprintf("Stage %d score: %.3f\n", stage, best_score))
  }

  # label the list
  names(best_config_list) <- paste0("stage_", seq_along(best_config_list))

  # return best config only
  best_index <- if (measure$minimize) {
    which.min(sapply(best_config_list, `[[`, "score"))
  } else {
    which.max(sapply(best_config_list, `[[`, "score"))
  }

  best_config_list[[best_index]]
}
```

Now I can run this function on the `mtcars` dataset.

```{r}
# create search space
search_space <- rbindlist(list(
  list(id = "cp", lower = 1e-4, upper = 1),
  list(id = "maxdepth", lower = 1,    upper = 30),
  list(id = "minsplit", lower = 2,    upper = 20),
  list(id = "minbucket", lower = 1,   upper = 10)
))

best_config <- iterative_random_tuner(
        task = task,
        learner = learner,
        search_space = search_space,
        resampling = rsmp("cv", folds = 3),
        measure = msr("regr.rmse"),
        random_search_stages = 5,
        random_search_size = 50
)

best_config
```

Nice one.

# Summary

Great. Let's summarise what I've done in this post.

**Exercise 1: Hyperparameter Tuning with Random Search**

- Tunes `regr.ranger` on `mtcars` dataset
- Parameters tuned: `mtry`, `num.trees`, `sample.fraction`
- Uses 3-fold CV, 50 random evaluations, MSE as the measure
- Visualises marginal effects of hyperparameters
- Retrains final model on full data using best hyperparameters

**Exercise 2: Nested Resampling**

- Evaluates tuned model’s performance with nested resampling
- Outer loop: 3-fold CV; Inner loop: holdout validation (70/30)
- Uses `AutoTuner` with same hyperparameter setup as Q1
- Aggregates performance across outer test folds
- Extracts inner tuning results and archives

**Exercise 3: Benchmarking XGBoost vs Logistic Regression**

- Task: binary classification on `spam` dataset
- Logistic regression used as untuned baseline
- XGBoost tuned using `mlr3tuningspaces::lts("classif.xgboost.default")`
- Inner loop: 5-fold CV with 60 sec time budget; Outer: 4-fold CV
- Evaluates models using Brier score
- Compares learners via tables and visualisation

# Fin

