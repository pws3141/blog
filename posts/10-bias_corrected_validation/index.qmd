---
title: "Model Fitting and Validation"
subtitle: "Model Validation using the Bootstrap"
author:
  - name: Paul Smith
date: "2025-04-26"
categories: [code, r, statistics, model fitting, model validation]
format:
  html:
    code-fold: false
execute:
  df-print: default
---

This is *Part Three* of an $N$^[where $N$ is TBC] part series on model fitting and validation.

- [Part One](../06-stepwise_datasplitting/index.qmd) considers some theoretical issues with data-splitting and using stepwise selection
- [Part Two](../07-stepwise_datasplitting_simulation/index.qmd) looks at simulating the effect of data-splitting and stepwise selection in model building and validation on the `lung` dataset, including:
    - The variability of model terms chosen and the coefficient estimates from data-splitting and using stepwise selection;
    - The variability in the C-statistic obtained from the test set.

# Introduction

In this part, I look at using the bootstrap to validate a model. Much of this
work is inspired by the excellent book by @harrell2001regression. Using the
bootstrap to validate a model means that data-splitting is not required, and
instead both model building and validation can be performed on the full
dataset. As discussed by Harrell in
a [post](https://www.fharrell.com/post/split-val/),

> Data are too precious to not be used in model development/parameter
estimation. Resampling methods allow the data to be used for both development
and validation, and they do a good job in estimating the likely future
performance of a model.

:::{.callout-note collapse="true"}
# The hierarchy of validation methods

In @harrell2001regression[Chapter 5.2], a hierarchy of validation methods is
given, from worst to best.^[Specifically, "one suggested hierarchy", so I'm
unsure whether this is Harrell's suggested one or not...]


1. Attempting several validations (internal or external) and reporting only the one that “worked”
2. Reporting apparent performance on the training dataset (no validation)
3. Reporting predictive accuracy on an undersized independent test sample
4. Internal validation using data-splitting where at least one of the training
and test samples is not huge and the investigator is not aware of the
arbitrariness of variable selection done on a single sample
5. Strong internal validation using 100 repeats of 10-fold cross-validation or
   several hundred bootstrap resamples, repeating all analysis steps involving
   $Y$ afresh at each re-sample and the arbitrariness of selected “important
variables” is reported (if variable selection is used)
6. External validation on a large test sample, done by the original research
team
7. Re-analysis by an independent research team using strong internal validation of the original dataset
8. External validation using new test data, done by an independent research
team
9. External validation using new test data generated using different
   instruments/technology, done by an independent research team

In this post, I am looking at option (5). Currently at work we often rely on (3) or (4).

:::


## The method

I will only consider complete-case data for now, and will consider two different scenarios.

1. Model fitting where we already "know" the factors in the model.
2. Model building and fitting, where we use stepwise to select the factors
   in the model.

:::{.callout-note collapse="true"}
# Why stepwise?

I am using stepwise here for simplicity. The lasso would be better (although it still [shares many of stepwise's flaws](../06-stepwise_datasplitting/index.qmd)). Using expert judgement to choose variables is much better, and means it is much harder to hide behind the computer.

I'm waiting with baited breath for guidance from the [*STRengthening Analytical
Thinking for Observational Studies*
(STRATOS)](https://stratos-initiative.org/group_2) initiative on variable
selection techniques for multivariate analysis, but I'm not sure it's coming
anytime soon [@sauerbrei2020state]. 

:::

The method for model validation in this article is based on @harrell2001regression[Chapter 5.3.5], which describes the process of calculating an optimism-corrected accuracy metric using bootstrap resampling as:

> From the original $X$ and $Y$ in the sample of size $n$, draw a sample with replacement also of size $n$. Derive a model in the bootstrap sample and apply it without change to the original sample. The accuracy index from the bootstrap sample minus the index computed on the original sample is an estimate of optimism. This process is repeated for $100$ or so bootstrap replications to obtain an average optimism, which is subtracted from the final model fit’s apparent accuracy to obtain the overfitting-corrected estimate.

He goes on...

> Note that bootstrapping validates the *process* that was used to fit the original model (as does cross-validation). It provides an estimate of the expected value of the optimism, which when subtracted from the original index, provides an estimate of the expected bias-corrected index. If stepwise variable selection is part of the bootstrap process (*as it must be if the final model is developed that way*), and not all resamples (samples with replacement or training samples in cross-validation) resulted in the same model (which is almost always the case), this internal validation process actually provides an unbiased estimate of the future performance of the process used to identify markers and scoring systems; it does not validate a single final model. But resampling does tend to provide good estimates of the future performance of the final model that was selected using the same procedure repeated in the resamples.

A good introduction to model building and internal validation is given in @collins2024evaluation. See *Box 2* for a simple explanation of the bootstrapping internal validation method.

I have clarified this procedure where appropriate in @sec-analysis.

## The data

The examples will be performed on the complete-case of
[`pbc`](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/pbc.html)
data from the `{survival}` package [@therneau2024survival].^[In my next post
I'll be considering the full dataset and use multiple imputation to handle the
missing data.]

```{r}
#| output: false

library(data.table)
options(datatable.print.nrows = 20) # max # rows before truncating (default = 100)
library(survival)
library(rms) # for 'validate'
library(mice)

# load pbc dataset
data(pbc)
pbc <- as.data.table(pbc)
pbc[, status := fifelse(status == 2, 1, 0)]

## used in 'step()' below
# creating upper and lower model for stepwise selection
# create upper scope (everything except 'id' and outcome variables)
upper_names <- paste(setdiff(names(pbc), c("time", "status", "id")), 
                             collapse = " + ")
scope <- list(
          upper = reformulate(upper_names),
          lower = ~1
        )

# complete case
pbc_complete <- na.omit(pbc)
```

This is a dataset of patients with primary biliary cirrhosis,
and has `r nrow(pbc_complete)` observations (compared to `r nrow(pbc)`
observations in the full `pbc` dataset).
Throughout this post I will use $B = 100$ bootstrap iterations.

```{r}
# number of bootstrap iterations
B <- 100
```

# Analysis {#sec-analysis}


| Notation | Definition |
|----------|------------|
| $X \in \mathbb{R}^{n \times p}$ | dataset containing missingness (`pbc`) |
| $\tilde{X} \in \mathbb{R}^{n \times \tilde{p}}$ | complete-case dataset (`pbc_complete`) |
| $X^{(i)}$ (or $\tilde{X}^{(i)}$) | $i$-th bootstrap dataset (or complete-case dataset), $i = 1,\dots,B$ |

## Choosing factors manually

In this section, I consider building a model and validating it, when the factors are chosen in the model *a-priori*.

### Model building {#sec-manual-building}

$$
\begin{align}
  \text{Data:}\quad&\tilde{X}\\
  &\\
  &\downarrow \quad \color{gray}{\text{Factors chosen a-priori}}\\
  &\\
  \text{Model:}\quad&\mathcal{M}
\end{align}
$$

#### The example

I will build a model using the following factors: `age`, `ascites`, `edema`, `albumin`.

```{r}
# fit a Cox PH model to pbc
cox1 <- coxph(Surv(time, status) ~ age + ascites + edema + albumin, 
                  data = pbc_complete)
summary(cox1)
```

### Model validation

First, $B$ bootstrap datasets are created, and models fitted to each of these:

$$
\begin{align}
  \text{Data:}\quad\quad\quad&\tilde{X}&&&&\\
  \\
  &\downarrow\\
  \\
  \text{Bootstrapped data:}\quad\quad&\tilde{X}^{(1)} \quad &&\tilde{X}^{(2)} \quad &&\ldots \quad &&\tilde{X}^{(B)}\\
  \\
  &\downarrow && \downarrow  &&\ldots &&\downarrow \quad\color{gray}{\text{Same factors as original model building (chosen a-priori)}}\\
  \\
  \text{Models:}\quad\quad &\mathcal{M}^{(1)} \quad &&\mathcal{M}^{(2)} \quad &&\ldots \quad &&\mathcal{M}^{(B)}\\
\end{align}
$$

Then, calculating the optimism-corrected C-statistic involves:

1. Calculate the C-statistic obtained from the original model $\mathcal{M}$ (from @sec-manual-building), fitted to the complete-case dataset $\tilde{X}$, denoted $c$.
2. For $b = 1, 2, \ldots, B$:
    i. Calculate the C-statistic from the $b^\text{th}$ bootstrap model $\mathcal{M}^{(b)}$, fitted to the $b^\text{th}$ bootstrap dataset $\tilde{X}^{(b)}$, denoted $c^{(b)}$.
    ii. Calculate the C-statistic from the $b^\text{th}$ bootstrap model $\mathcal{M}^{(b)}$, fitted to the original dataset, $\tilde{X}$, denoted $c^{(b)}_X$.
    iii. Calculate the optimism for the $b^\text{th}$ bootstrap model $o^{(b)} = c^{(b)} - c^{(b)}_X$.
3. Calculate the mean optimism, $\bar{o} = \frac{1}{B} \sum_{b=1}^{B} o^{(b)}$.
4. Calculate the optimism-corrected C-statistic,
$$
  c_\text{opt} = c - \bar{o}.
$$

#### The example

First, I obtain the (optimistic) C-statistic.

```{r}
c1 <- concordance(cox1)$concordance
```

Then, we follow Steps (2) - (4) above to obtain the optimism-corrected C-statistic.

```{r}
# number of bootstrap samples
B <- 100


set.seed(10)

c_boots1 <- rbindlist(
  lapply(seq_len(B), function(b) {
      
      # 2a. create bootstrap sample
      idx <- sample(nrow(pbc_complete), replace = TRUE)
      pbc_boot <- pbc_complete[idx]
      
      # 2b. fit model
      cox_boot <- coxph(Surv(time, status) ~ age + ascites + edema + albumin, 
                        data = pbc_boot)

      # 2c. 
      # calculate concordance on bootstrap sample
      c_stat_boot <- concordance(cox_boot)
      
      # 2d.
      # predict on original data
      lp_X <- predict(cox_boot, newdata = pbc_complete, type = "lp")
      # calculate concordance on original data
      c_stat_X <- concordance(Surv(time, status) ~ lp_X, 
                              data = pbc_complete, reverse = TRUE)
      c_boot <- c_stat_boot$concordance
      c_x <- c_stat_X$concordance
      
      # save c stats and calculate optimism
      data.table(c_boot = c_boot, c_x = c_x, optimism = c_boot - c_x)
    }
  )
)

print(c_boots1, topn = 5)
```

Now, we can compute the mean optimism, $\bar{o}$, and the overfitting-corrected estimate of accuracy.

```{r}
# 3. mean difference
optimism_mean1 <- mean(c_boots1$optimism)

# 4. overfitting-corrected estimate of accuracy
c_opt1 <- c1 - optimism_mean1
```
So, the original C-statistic was `r round(c1, 4)`, compared to the bias-corrected estimate of `r round(c_opt1, 4)`.

:::{.callout-note}
# The quick code

This can be done in a few lines lines, using the `{rms}` package.

```{r}
# validate
set.seed(10)
cox1_cph <- rms::cph(Surv(time, status) ~ age + ascites + edema + albumin, 
                         data = pbc_complete, x = TRUE, y = TRUE) # <1>
bias_corrected_metrics1 <- rms::validate(cox1_cph, B = 100)
bias_corrected_metrics1
```
1. `rms::validate()` requires the model to be fit using `rms::cph()` instead of `survival::coxph()`.

For a detailed explanation of the `validate` output, see John Haman's
[website](https://randomeffect.net/post/2021/05/02/the-rms-validate-function/).

```{r}
c1_rms <- (bias_corrected_metrics1["Dxy", "index.corrected"] + 1) / 2
c1_rms
```
:::

## Analysis Using Stepwise Selection

The model building stage (@sec-complete-building) is done using stepwise selection, and the model validation stage (@sec-complete-validation) is done using bootstrapping. The model building and validation stages are shown below.


### Model building {#sec-complete-building}

Here, I perform stepwise selection on the complete-case dataset, starting from the null model.

$$
\begin{align}
  \text{Data:}\quad&\tilde{X}\\
  &\\
  &\downarrow \quad \color{gray}{\text{Stepwise selection}}\\
  &\\
  \text{Model:}\quad&\mathcal{M}
\end{align}
$$

#### The example {#sec-analysis-stepwise-building-example}

:::{.panel-tabset}

## The code

```{r}
#| code-fold: false


# fit a Cox PH model to pbc: start with the null model
cox_null <- coxph(Surv(time, status) ~ 1, data = pbc_complete)

# use stepwise selection (minimising AIC)
# creating upper and lower model for stepwise selection
# create upper scope (everything except 'id' and outcome variables)
upper_names <- paste(setdiff(names(pbc), c("time", "status", "id")), 
                             collapse = " + ")
scope <- list(
          upper = reformulate(upper_names),
          lower = ~1
        )

cox2 <- step(cox_null, scope = scope, trace = 0)
```

## The output

```{r}
# look at the model
summary(cox2)
```

:::

### Model validation {#sec-complete-validation}

The model validation stage is done using bootstrapping. The bootstrap procedure -- starting from the complete-case dataset $\tilde{X}$ (which is the same as used in the model building stage) -- is as follows.

First, $B$ bootstrap datasets are created, and models fitted to each of these (using the same stepwise procedure as in @sec-complete-building):

$$
\begin{align}
  \text{Data:}\quad\quad\quad&\tilde{X}&&&&\\
  \\
  &\downarrow\\
  \\
  \text{Bootstrapped data:}\quad\quad&\tilde{X}^{(1)} \quad &&\tilde{X}^{(2)} \quad &&\ldots \quad &&\tilde{X}^{(B)}\\
  \\
  &\downarrow && \downarrow  &&\ldots &&\downarrow \quad\color{gray}{\text{Stepwise selection}}\\
  \\
  \text{Models:}\quad\quad &\mathcal{M}^{(1)} \quad &&\mathcal{M}^{(2)} \quad &&\ldots \quad &&\mathcal{M}^{(B)}\\
\end{align}
$$

Then, calculating the optimism-corrected C-statistic involves:

1. Calculate the C-statistic obtained from the original model $\mathcal{M}$ (from @sec-complete-building), fitted to the complete-case dataset $\tilde{X}$, denoted $c$.
2. For $b = 1, 2, \ldots, B$:
    i. Calculate the C-statistic from the $b^\text{th}$ bootstrap model $\mathcal{M}^{(b)}$, fitted to the $b^\text{th}$ bootstrap dataset $\tilde{X}^{(b)}$, denoted $c^{(b)}$.
    ii. Calculate the C-statistic from the $b^\text{th}$ bootstrap model $\mathcal{M}^{(b)}$, fitted to the original dataset, $\tilde{X}$, denoted $c^{(b)}_X$.
    iii. Calculate the optimism for the $b^\text{th}$ bootstrap model $o^{(b)} = c^{(b)} - c^{(b)}_X$.
3. Calculate the mean optimism, $\bar{o} = \frac{1}{B} \sum_{b=1}^{B} o^{(b)}$.
4. Calculate the optimism-corrected C-statistic,
$$
  c_\text{opt} = c - \bar{o}.
$$

#### The example

First, I obtain the (optimistic) C-statistic^[The C-statistic obtained from the original model $\mathcal{M}$ (from @sec-complete-building), fitted to the complete-case dataset $\tilde{X}$.], as described in Step (1).

```{r}
c <- concordance(cox2)$concordance
```

Then, follow Step (2) to obtain the C-statistic from the $b^\text{th}$ bootstrap model $\mathcal{M}^{(b)}$, fitted to both the bootstrap dataset,$\tilde{X}^{(b)}$, and the original dataset, $\tilde{X}$. From these the $b^\text{th}$ bootstrap model optimism can be calculated.

```{r}
set.seed(10)

# start time of execution
total_time_start <- Sys.time()
      
c_boots <- rbindlist(
  lapply(seq_len(B), function(b) {
      
      # start time of inner execution
      start_time <- Sys.time()

      # 2a. create bootstrap sample
      idx <- sample(nrow(pbc_complete), replace = TRUE)
      pbc_boot <- pbc_complete[idx]
      
      # 2b. fit null model
      cox_boot_null <- coxph(Surv(time, status) ~ 1, data = pbc_boot)
      # use stepwise selection (minimising AIC)
      cox_boot <- step(cox_boot_null, scope = scope, trace = 0)

      # 2c. 
      c_boot <- concordance(cox_boot)$concordance
      
      # 2d.
      # predict on original data
      lp_X <- predict(cox_boot, newdata = pbc_complete, type = "lp")
      # calculate concordance on original data
      c_x <- concordance(Surv(time, status) ~ lp_X, 
                              data = pbc_complete, reverse = TRUE)$concordance
      
      # end time of inner execution
      end_time <- Sys.time()
      time_taken <- end_time - start_time
      
      # save c stats and calculate optimism
      data.table(c_boot = c_boot, c_x = c_x, optimism = c_boot - c_x,
                 time = time_taken)
    }
  )
)

# end time of inner execution
total_time_end <- Sys.time()
total_time_taken <- total_time_end - total_time_start

print(c_boots, topn = 5)
```

This process took `r round(as.numeric(total_time_taken), 2)` `r units(total_time_taken)` in total.^[On my Macbook Air M3.]

Now, I compute the mean optimism, $\bar{o}$, and the overfitting-corrected estimate of accuracy.

```{r}
# (3) mean optimism
optimism_mean <- mean(c_boots$optimism)

# (4) optimism-corrected estimate of accuracy
c_opt <- c - optimism_mean
```

So, the original C-statistic was $c =$ `r round(c, 4)`, compared to the
bias-corrected estimate of $c_\text{opt}=$ `r round(c_opt, 4)`. The mean of the
optimism was $\bar{o} =$ `r round(optimism_mean, 4)`.

:::{.callout-important collapse="true"}
# Using `rms::validate()`

The `validate()` function cannot be used here; it doesn't validate the *whole
process*, as the stepwise procedure is not repeated. That is, `validate` assumes
the model is fixed for every bootstrap iteration. This means that the mean
optimism will be smaller than the one obtained above, and therefore the
'optimism-adjusted' C-statistic will be higher than it should be.

The model found from the stepwise procedure in @sec-analysis-stepwise-building-example is,
```{r}
formula(cox2)
```

So, assuming this model form is constant throughout: 

```{r}
# validate
set.seed(10)
cox2_cph <- rms::cph(formula(cox2),
                         data = pbc_complete, x = TRUE, y = TRUE) # <1>
bias_corrected_metrics2 <- rms::validate(cox2_cph, B = 100)
bias_corrected_metrics2
c2_rms <- (bias_corrected_metrics2["Dxy", "index.corrected"] + 1) / 2
c2_rms
```
:::

# Summary

**What I've done:** for both a predefined model and using a stepwise model building
procedure, I have calculated the apparent performance C-statistic, and then
adjusted for optimism. For the predefined model, it is possible to calculate
the optimism easily using `rms::validate()`. However, when using a model building
procedure it is required to do this manually as the whole process needs to be
included in each bootstrap iteration.

**Next steps:** model building and validation in the presence of missing data,
using multiple imputation.




# Fin
