[
  {
    "objectID": "posts/07-stepwise_datasplitting_simulation/index.html",
    "href": "posts/07-stepwise_datasplitting_simulation/index.html",
    "title": "Model Fitting and Validation",
    "section": "",
    "text": "This is Part Two of an \\(N\\)1 part series on model fitting and validation. Part One can be found here."
  },
  {
    "objectID": "posts/07-stepwise_datasplitting_simulation/index.html#prerequisites-and-data-cleaning",
    "href": "posts/07-stepwise_datasplitting_simulation/index.html#prerequisites-and-data-cleaning",
    "title": "Model Fitting and Validation",
    "section": "Prerequisites and data cleaning",
    "text": "Prerequisites and data cleaning\nThe lung dataset is from the {survival} package (Therneau 2024), and I will use {data.table} (Barrett et al. 2024) as I like the syntax and it’s fast.\n\nTherneau, Terry M. 2024. A Package for Survival Analysis in r. https://CRAN.R-project.org/package=survival.\n\nBarrett, Tyson, Matt Dowle, Arun Srinivasan, Jan Gorecki, Michael Chirico, Toby Hocking, and Benjamin Schwendinger. 2024. Data.table: Extension of ‘Data.frame‘. https://CRAN.R-project.org/package=data.table.\n\nlibrary(survival)\nlibrary(broom) # for 'tidy'\nlibrary(data.table)\noptions(datatable.print.nrows = 20) # set max # rows before truncating (default = 100)\nlibrary(ggplot2)\nlung &lt;- as.data.table(lung)\nlung\n\n      inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n     &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n  1:     3   306      2    74     1       1       90       100     1175      NA\n  2:     3   455      2    68     1       0       90        90     1225      15\n  3:     3  1010      1    56     1       0       90        90       NA      15\n  4:     5   210      2    57     1       1       90        60     1150      11\n  5:     1   883      2    60     1       0      100        90       NA       0\n ---                                                                           \n224:     1   188      1    77     1       1       80        60       NA       3\n225:    13   191      1    39     1       0       90        90     2350      -5\n226:    32   105      1    75     2       2       60        70     1025       5\n227:     6   174      1    66     1       1       90       100     1075       1\n228:    22   177      1    58     2       1       80        90     1060       0\n\n\nThe lung dataset has status encoded in a non-standard way (as 1 for censored and 2 for dead). I will change this to 0 and 1 respectively. I will also ensure the data contains no missingness by using na.omit().\n\n#recode 'status' as 1 if event and 0 if censored\nlung_recode &lt;- copy(lung)[, status := as.integer(status == 2)]\n\n# remove NA's (st 'ampute' works)\nlung_complete &lt;- na.omit(lung_recode)\n\nlung_complete[, `:=`(sex = as.factor(sex), ph.ecog = as.factor(ph.ecog))]\n\nlung_complete\n\n      inst  time status   age    sex ph.ecog ph.karno pat.karno meal.cal\n     &lt;num&gt; &lt;num&gt;  &lt;int&gt; &lt;num&gt; &lt;fctr&gt;  &lt;fctr&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n  1:     3   455      1    68      1       0       90        90     1225\n  2:     5   210      1    57      1       1       90        60     1150\n  3:    12  1022      0    74      1       1       50        80      513\n  4:     7   310      1    68      2       2       70        60      384\n  5:    11   361      1    71      2       2       60        80      538\n ---                                                                    \n163:    11   203      0    71      2       1       80        90     1025\n164:    13   191      0    39      1       0       90        90     2350\n165:    32   105      0    75      2       2       60        70     1025\n166:     6   174      0    66      1       1       90       100     1075\n167:    22   177      0    58      2       1       80        90     1060\n     wt.loss\n       &lt;num&gt;\n  1:      15\n  2:      11\n  3:       0\n  4:      10\n  5:       1\n ---        \n163:       0\n164:      -5\n165:       5\n166:       1\n167:       0"
  },
  {
    "objectID": "posts/07-stepwise_datasplitting_simulation/index.html#the-model-factors",
    "href": "posts/07-stepwise_datasplitting_simulation/index.html#the-model-factors",
    "title": "Model Fitting and Validation",
    "section": "The model factors",
    "text": "The model factors\nFirst, let’s look at the proportion that each factor has been chosen to be in the 100 models.\n\n\n\n\nwhich_variables &lt;- models[, unique(term)]\n\nproportion_variables &lt;- rbindlist(\n      lapply(1:length(which_variables), \n             function(i) {\n              prop_tmp &lt;- nrow(models[term == which_variables[i]]) / n_sim\n              data.table(which_variables[i], prop_tmp)\n                    }\n             )\n      )\n\n# categorical factors in each model are present in the `models` dataset multiple times\n# e.g. 'ph.ecog1', 'ph.ecog2', etc.\n# create a \"expl_factor\" group by removing trailing digits\nproportion_variables[\n  , expl_factor := sub(\"[0-9]+$\", \"\", V1)\n]\n\n# collapse by the explanatory factor\ngrouped_proportions &lt;- proportion_variables[\n  , .(prop = unique(prop_tmp)),\n  by = expl_factor\n]\n\ngrouped_proportions[order(prop, decreasing = TRUE)]\n\n\nListing 4: Obtaining the proportion that each factor is chosen to be in the models.\n\n\n\n\n   expl_factor  prop\n        &lt;char&gt; &lt;num&gt;\n1:     ph.ecog  0.94\n2:         sex  0.91\n3:        inst  0.72\n4:     wt.loss  0.68\n5:    ph.karno  0.64\n6:   pat.karno  0.30\n7:         age  0.22\n8:    meal.cal  0.04\n\n\nNow I’ll consider each model and the factors obtained using the stepwise procedure. First – in Listing 5 – I’ll create a terms_string variable that gives a string containing all the terms in each of the 100 models. Then – in Listing 6 – I’ll group these models together to see how many different combination of factors have been chosen from the 100 stepwise selection procedures.\n\n\n\n\nmodel_variables &lt;- models[, .(\n  terms_string = paste(\n    unique(sub(\"[0-9]+$\", \"\", term)),\n    collapse = \", \"\n  )\n), by = .id]\n\nmodel_variables\n\n\nListing 5: Creating string of all the factors in each of the models.\n\n\n\n\n       .id                                     terms_string\n     &lt;int&gt;                                           &lt;char&gt;\n  1:     1                     inst, sex, ph.ecog, ph.karno\n  2:     2            inst, sex, ph.ecog, ph.karno, wt.loss\n  3:     3                 sex, ph.ecog, pat.karno, wt.loss\n  4:     4       sex, ph.ecog, ph.karno, pat.karno, wt.loss\n  5:     5                inst, age, sex, ph.ecog, ph.karno\n ---                                                       \n 96:    96             age, sex, ph.ecog, ph.karno, wt.loss\n 97:    97            inst, sex, ph.ecog, ph.karno, wt.loss\n 98:    98 inst, sex, ph.ecog, ph.karno, pat.karno, wt.loss\n 99:    99                      age, sex, ph.ecog, ph.karno\n100:   100                           sex, ph.ecog, ph.karno\n\n\nGrouping these models together gives the follow:\n\n\n\n\nproportion_model_variables &lt;- model_variables[\n      , .(prop = nrow(.SD) / nrow(model_variables))\n      , by = terms_string]\n\n#proportion_model_variables\nproportion_model_variables[order(prop, decreasing = TRUE)]\n\n\nListing 6: Grouping models together to show the number of different models obtained from using stepwise selection.\n\n\n\n\n                                        terms_string  prop\n                                              &lt;char&gt; &lt;num&gt;\n 1:            inst, sex, ph.ecog, ph.karno, wt.loss  0.14\n 2:                      inst, sex, ph.ecog, wt.loss  0.13\n 3: inst, sex, ph.ecog, ph.karno, pat.karno, wt.loss  0.11\n 4:                     inst, sex, ph.ecog, ph.karno  0.06\n 5:                inst, age, sex, ph.ecog, ph.karno  0.04\n---                                                       \n30:           inst, sex, ph.ecog, pat.karno, wt.loss  0.01\n31:                                        pat.karno  0.01\n32:                     inst, sex, ph.ecog, meal.cal  0.01\n33:  inst, sex, ph.ecog, ph.karno, meal.cal, wt.loss  0.01\n34:             age, sex, ph.ecog, ph.karno, wt.loss  0.01\n\n\nThat is, from 100 simulations of splitting the data into a training and test set, I’ve obtained 34 different models!\n\nThe factor coefficients\nOK – let’s look at the variability of the coefficient estimates for each factor. First, I’ll get summary statistics for each factor coefficient. A box-plot showing this variability is given in Figure 1, and the kernel densities of the estimates are shown in Figure 2.\n\n# Remove any rows that have NA in the estimate\nmodels_clean &lt;- subset(models, !is.na(estimate))\n\nmodels_clean[, as.list(summary(estimate)), by = term]\n\n         term          Min.       1st Qu.        Median          Mean\n       &lt;char&gt;         &lt;num&gt;         &lt;num&gt;         &lt;num&gt;         &lt;num&gt;\n 1:      inst -0.0509354446 -0.0365704124 -0.0302755030 -0.0320112247\n 2:      sex2 -1.1315746564 -0.7029760419 -0.5881394025 -0.6093326104\n 3:  ph.ecog1  0.1420228124  0.5092835045  0.7024669149  0.7242113000\n 4:  ph.ecog2  0.8063634936  1.3599992736  1.7791183312  1.7739151854\n 5:  ph.ecog3  1.8659038585  2.5201583000  3.0779479537  3.0501194170\n 6:  ph.karno -0.0211069194  0.0252318873  0.0304998545  0.0305489979\n 7:   wt.loss -0.0292142915 -0.0210200718 -0.0186741004 -0.0187607396\n 8: pat.karno -0.0280318096 -0.0223547207 -0.0183404498 -0.0193037902\n 9:       age  0.0208960667  0.0220780244  0.0227820171  0.0242739341\n10:  meal.cal -0.0009556685 -0.0007011441 -0.0006010875 -0.0006590115\n          3rd Qu.          Max.\n            &lt;num&gt;         &lt;num&gt;\n 1: -0.0262214394 -0.0216371920\n 2: -0.5037040983 -0.3564017818\n 3:  0.9017954598  1.4072391610\n 4:  2.1107234044  2.8765227030\n 5:  3.5501307812  4.2490415152\n 6:  0.0372071507  0.0516899027\n 7: -0.0160832211 -0.0122667120\n 8: -0.0169197955 -0.0142537345\n 9:  0.0248300821  0.0391007125\n10: -0.0005589549 -0.0004782023\n\n\n\nggplot(models_clean, aes(x=term, y=estimate)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(\n     axis.title = element_blank(),\n     # remove the grid lines\n     panel.grid.major = element_blank() ,\n     panel.grid.minor = element_blank() ,\n     # explicitly set the horizontal lines \n     panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n        )\n\n\n\n\n\n\n\nFigure 1: Box plots showing the variability in the factor coefficient estimates.\n\n\n\n\n\nWe can see from Figure 1 (and by referring back to Listing 4) that the factors that are chosen the most times have the largest varibility in their coefficient estimates.\n\nggplot(models_clean, aes(x = estimate)) +\n  geom_density(fill = \"grey\", alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\") +\n  labs(\n    title = \"Density Plot of Factor Coefficients\",\n    x = \"Coefficient Estimate\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n     axis.text.y = element_blank(),\n     axis.title.y=element_blank(),\n     # remove the grid lines\n     panel.grid.major = element_blank() ,\n     panel.grid.minor = element_blank() ,\n     # explicitly set the horizontal lines \n     panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n  )\n\n\n\n\n\n\n\nFigure 2: Density plots of the estimates for each factor coefficient."
  },
  {
    "objectID": "posts/07-stepwise_datasplitting_simulation/index.html#the-c-index-on-the-test-sets",
    "href": "posts/07-stepwise_datasplitting_simulation/index.html#the-c-index-on-the-test-sets",
    "title": "Model Fitting and Validation",
    "section": "The C-Index on the test sets",
    "text": "The C-Index on the test sets\nFinally, let’s look at how the C-index varies for each training/test split and respective model. The code in Listing 7 gives the C-index values obtained from the 100 models, and the histogram is given in Figure 3.\n\n\n\n\nc_stats &lt;- models[, .(c_stats = unique(c)), by = .id]\nc_stats[order(c_stats, decreasing = TRUE)]\n\n\nListing 7: Code to output the C-index, in descending order.\n\n\n\n\n       .id   c_stats\n     &lt;int&gt;     &lt;num&gt;\n  1:    52 0.7121514\n  2:     6 0.6835700\n  3:    25 0.6673729\n  4:    58 0.6621475\n  5:    50 0.6596509\n ---                \n 96:    53 0.5090000\n 97:     7 0.5019802\n 98:    18 0.4983819\n 99:    66 0.4790356\n100:     2 0.4610318\n\n\n\nggplot(c_stats, aes(x = c_stats)) +\n        stat_bin(bins = 30) +\n        xlab(\"C-Index\") + ylab(\"\") +\n        theme_minimal() +\n        theme(\n           # remove the grid lines\n           panel.grid.major = element_blank() ,\n           panel.grid.minor = element_blank() ,\n           # explicitly set the horizontal lines \n           panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n        )\n\n\n\n\n\n\n\nFigure 3: Histogram of the C-index values obtained on the test sets of the models.\n\n\n\n\n\nThese C-statistics varies quite a bit, from worse-than-random (a minimum of 0.46) to OK (a maximum of 0.71). Some of this variability could be accounted for by:\n\nSome predictors might violate proportional hazards, hurting model performance.\nSmall test set size reduces the reliability of C-index estimates."
  },
  {
    "objectID": "posts/04-highcharter_bar/index.html",
    "href": "posts/04-highcharter_bar/index.html",
    "title": "A Bidirectional Bar Chart using Highcharter",
    "section": "",
    "text": "This is just a quick post, as a follow on to my previous highcharter posts (one and two).\nI wanted a bar chart that looks like Figure 1, which was made using Flourish.\nCode\nlibrary(data.table)\nlibrary(highcharter)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo"
  },
  {
    "objectID": "posts/04-highcharter_bar/index.html#the-data",
    "href": "posts/04-highcharter_bar/index.html#the-data",
    "title": "A Bidirectional Bar Chart using Highcharter",
    "section": "The data",
    "text": "The data\nThe data looks like this.\n\n\nCode\nbar_data &lt;- data.table(\n  donation_and_transplantation_summary = c(\n    \"Waiting for an organ\",\n    \"Opt-Out from the ODR\",\n    \"Deceased organ donor transplants\",\n    \"Died on the waiting list\",\n    \"Living organ donor transplants\",\n    \"Living donors\",\n    \"Opt-in to the Organ Donor Register (ODR)\",\n    \"Eligible deceased donors\",\n    \"Deceased donors\"\n  ),\n  black_percent = c(-11, -7, -10, -8, -4, -3, -2, -3, -1),\n  asian_percent = c(19, 18, 17, 13, 9, 8, 7, 7, 4)\n)\n\nbar_data\n\n\n       donation_and_transplantation_summary black_percent asian_percent\n                                     &lt;char&gt;         &lt;num&gt;         &lt;num&gt;\n1:                     Waiting for an organ           -11            19\n2:                     Opt-Out from the ODR            -7            18\n3:         Deceased organ donor transplants           -10            17\n4:                 Died on the waiting list            -8            13\n5:           Living organ donor transplants            -4             9\n6:                            Living donors            -3             8\n7: Opt-in to the Organ Donor Register (ODR)            -2             7\n8:                 Eligible deceased donors            -3             7\n9:                          Deceased donors            -1             4"
  },
  {
    "objectID": "posts/04-highcharter_bar/index.html#the-chart",
    "href": "posts/04-highcharter_bar/index.html#the-chart",
    "title": "A Bidirectional Bar Chart using Highcharter",
    "section": "The chart",
    "text": "The chart\n\n\nCode\nhighchart() |&gt;\n  # Set the chart type\n  hc_chart(type = \"bar\") |&gt;\n  # Provide x-axis categories (the labels for each bar)\n  hc_xAxis(\n    list(\n      categories = bar_data$donation_and_transplantation_summary,\n      reversed = FALSE\n    )\n    #list( # mirror axis on right side\n    #  opposite = TRUE,\n    #  categories = bar_data$donation_and_transplantation_summary,\n    #  reversed = FALSE,\n    #  linkedTo = 0\n    #  )\n    ) |&gt;\n  hc_yAxis(\n    gridLineColor = \"#f2f5f3\",\n    labels = list(\n      # positive values on both sides, appended with '%'\n      formatter = JS(\"function() { return Math.abs(this.value) + '%'; }\")\n    ),\n    plotBands = list( \n      list(\n      color = '#e2e1e1', from = 0, to = 9,\n      label = list(\n        text = \"9% Asian population\",\n        align = 'left',\n        y = -1,\n        x = 10\n        )\n      ),\n      list(\n      color = '#e2e1e1', from = -4, to = 0,\n      label = list(\n        text = \"4% Black population\",\n        align = 'right',\n        y = -1\n        )      \n      )\n    )\n  ) |&gt;\n  hc_plotOptions(\n    series = list(\n      # put bars for the same category on the same line\n      stacking = \"normal\",\n      pointPadding = 0.01,  # Padding between each column or bar, in x axis units... Defaults to 0.1.\n      groupPadding = 0.05  # Padding between each value groups, in x axis units... Defaults to 0.2.\n    )\n  ) |&gt;\n  hc_title(\n    text = \"&lt;span style='color:#62b19c;'&gt;Black&lt;/span&gt; and \n            &lt;span style='color:#bd82d5;'&gt;Asian&lt;/span&gt; \n            ethnic minorities are over-represented in transplant statistics\",\n    useHTML = TRUE,\n    style = list(\n      fontWeight = 'bold'\n    ),\n    align = \"left\"\n  )  |&gt;\n  hc_subtitle(\n    text = \"Percentage of these groups relative to the population of England and Wales in 2023/24\",\n    align = 'left'\n  ) |&gt;\n  hc_credits(\n    enabled = TRUE,\n    text = \"Ethnicity Differences in Organ Donation and Transplantion report for 2023/24 and 2021 population census estimates.\",\n    href = \"https://www.odt.nhs.uk/statistics-and-reports/annual-report-on-ethnicity-differences/\"  \n    ) |&gt;\n  hc_annotations(\n    list(\n        draggable = '',\n        labelOptions = list(\n          shape = 'connector',\n          justify = FALSE,\n          crop = TRUE,\n          style = list(\n            fontSize = \"10px\",\n            textOutline = \"1px white\",\n            fontWeight = \"normal\",\n            color = \"#4a4a4a\" \n            )\n        ),\n        labels =\n          list(point = list(xAxis = 0, yAxis = 0, y = 9, x = 6.5),\n               text = \"Within the grey zone shows&lt;br&gt;under-representation, while&lt;br&gt;outside the grey zone indicates&lt;br&gt;over-representation compared&lt;br&gt;to the population of England&lt;br&gt;and Wales.\",\n               x = 100, # offset in pixels\n               y = 50\n               )\n    )\n  ) |&gt;\n  hc_legend(\n    align = \"right\",\n    verticalAlign = \"bottom\",\n    layout = \"horizontal\"\n  ) |&gt;\n  hc_tooltip(\n    formatter = JS(\n      \"function() {\n         return '&lt;b&gt;' + this.series.name + '&lt;/b&gt;&lt;br/&gt;' +\n                this.point.category + ': ' +\n                Highcharts.numberFormat(Math.abs(this.point.y), 0) + '%';\n       }\"\n    )\n  ) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"ethnic_minority_bar\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    ) |&gt;\n  # Add a series of numeric values for the bars\n  hc_add_series(\n    name = \"Asian population\",\n    data = bar_data$asian_percent,\n    color = '#bd82d5'\n  ) |&gt;\n  hc_add_series(\n    name = \"Black population\",\n    data = bar_data$black_percent,\n    color = '#62b19c'\n  )"
  },
  {
    "objectID": "posts/04-highcharter_bar/index.html#fin",
    "href": "posts/04-highcharter_bar/index.html#fin",
    "title": "A Bidirectional Bar Chart using Highcharter",
    "section": "Fin",
    "text": "Fin\nI think the end result is pretty good."
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html",
    "href": "posts/09-mlr3_hyperparameter/index.html",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "I am attempting to learn how to use {mlr3} (Lang et al. 2019), by reading through the book Applied Machine Learning Using mlr3 in R (Bischl et al. 2024).\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com.\n\nBecker, Marc, Lennart Schneider, and Sebastian Fischer. 2024. “Hyperparameter Optimization.” In Applied Machine Learning Using mlr3 in R, edited by Bernd Bischl, Raphael Sonabend, Lars Kotthoff, and Michel Lang. CRC Press. https://mlr3book.mlr-org.com/hyperparameter_optimization.html.\nIn this post, I am working through the exercises given in Chapter 4 of the book (Becker, Schneider, and Fischer 2024), which covers hyperparameter optimisation (HPO). This includes the following:\n\nQ1: Tunes regr.ranger on mtcars using random search and 3-fold CV\nQ2: Evaluates tuned model with nested resampling (3-fold CV outer, holdout inner)\nQ3: Benchmarks tuned XGBoost vs logistic regression on spam using Brier score - Uses AutoTuner, predefined tuning spaces, and benchmark() for comparison\n\nMy previous posts cover:\n\nPart one:\n\nCreate a classification tree model to predict diabetes.\nLook at the confusion matrix and create measures without using {mlr3measures}.\nChange the thresholds in the model.\n\nPart two:\n\nRepeated cross-validation resampling.\nUsing a custom resampling strategy.\nCreating a function that produces a ROC curve.\n\n\n\n\n\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n\nLoading required package: paradox\n\nlibrary(mlr3tuningspaces)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n\nSuppress all messaging unless it’s a warning:1\n1 The packages in {mlr3} that make use of optimization, i.e., {mlr3tuning} or {mlr3select}, use the logger of their base package {bbotk}.\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html#prerequisites",
    "href": "posts/09-mlr3_hyperparameter/index.html#prerequisites",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "library(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n\nLoading required package: paradox\n\nlibrary(mlr3tuningspaces)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n\nSuppress all messaging unless it’s a warning:1\n1 The packages in {mlr3} that make use of optimization, i.e., {mlr3tuning} or {mlr3select}, use the logger of their base package {bbotk}.\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html#question-1",
    "href": "posts/09-mlr3_hyperparameter/index.html#question-1",
    "title": "Getting Started with {mlr3}",
    "section": "Question 1",
    "text": "Question 1\nTune the mtry, sample.fraction, and num.trees hyperparameters of lrn(\"regr.ranger\") on tsk(\"mtcars\"). Use a simple random search with 50 evaluations. Evaluate with a 3-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.\n\nAnswer\nLet’s load the task and look at the properties of the mtry, sample.fraction, and num.trees parameters.\n\ntsk_mtcars &lt;- tsk(\"mtcars\")\nlrn(\"regr.ranger\")$param_set$data[id %in% c(\"mtry\", \"sample.fraction\", \"num.trees\")]\n\n                id    class lower upper levels nlevels is_bounded special_vals\n            &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;list&gt;   &lt;num&gt;     &lt;lgcl&gt;       &lt;list&gt;\n1:            mtry ParamInt     1   Inf [NULL]     Inf      FALSE    &lt;list[1]&gt;\n2:       num.trees ParamInt     1   Inf [NULL]     Inf      FALSE    &lt;list[0]&gt;\n3: sample.fraction ParamDbl     0     1 [NULL]     Inf       TRUE    &lt;list[0]&gt;\n          default storage_type                   tags\n           &lt;list&gt;       &lt;char&gt;                 &lt;list&gt;\n1: &lt;NoDefault[0]&gt;      integer                  train\n2:            500      integer train,predict,hotstart\n3: &lt;NoDefault[0]&gt;      numeric                  train\n\n\nThe hyperparameters I’m looking at are:\n\nmtry: number of variables considered at each tree split.\nnum.trees: number of trees in the forest.\nsample.fraction: fraction of observations used to train each tree.\n\nNow I will set up the tuning of the mtry, sample.fraction, and num.trees hyperparameters.\n\nlearner &lt;- lrn(\"regr.ranger\",\n               mtry = to_tune(p_int(1, 10)),\n               num.trees = to_tune(20, 2000),\n               sample.fraction = to_tune(0.1, 1))\nlearner\n\n&lt;LearnerRegrRanger:regr.ranger&gt;: Random Forest\n* Model: -\n* Parameters: mtry=&lt;ObjectTuneToken&gt;, num.threads=1,\n  num.trees=&lt;RangeTuneToken&gt;, sample.fraction=&lt;RangeTuneToken&gt;\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], se, quantiles\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, missings, oob_error,\n  selected_features, weights\n\n\nSetting up an instance to terminate the tuner after 50 evaluations, and to use 3-fold CV.\n\ninstance &lt;- ti(task = tsk_mtcars,\n               learner = learner,\n               resampling = rsmp(\"cv\", folds = 3),\n               # rmse gives interpretability in the original units (MPG) rather than squared units\n               measures = msr(\"regr.rmse\"), \n               terminator = trm(\"evals\", n_evals = 50))\ninstance\n\n&lt;TuningInstanceBatchSingleCrit&gt;\n* State:  Not optimized\n* Objective: &lt;ObjectiveTuningBatch:regr.ranger_on_mtcars&gt;\n* Search Space:\n                id    class lower upper nlevels\n            &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n1:            mtry ParamInt   1.0    10      10\n2:       num.trees ParamInt  20.0  2000    1981\n3: sample.fraction ParamDbl   0.1     1     Inf\n* Terminator: &lt;TerminatorEvals&gt;\n\n\nThe tuning step uses 3-fold cross-validation:\n\nIn each evaluation, two-thirds of the data is used for training,\nOne-third is used for validation (i.e. to compute the RMSE).\nThis is repeated for 50 random configurations (as specified by the terminator).\n\nNow I can set up the tuning process (random search).\n\ntuner &lt;- tnr(\"random_search\")\n#tuner$param_set\ntuner\n\n&lt;TunerBatchRandomSearch&gt;: Random Search\n* Parameters: batch_size=1\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning, bbotk\n\n\nAnd trigger the tuning process.\n\nset.seed(333)\ntuner$optimize(instance)\n\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   &lt;int&gt;     &lt;int&gt;           &lt;num&gt;             &lt;list&gt;    &lt;list&gt;     &lt;num&gt;\n1:     5       187       0.9761367          &lt;list[4]&gt; &lt;list[3]&gt;  2.371042\n\ninstance$result\n\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   &lt;int&gt;     &lt;int&gt;           &lt;num&gt;             &lt;list&gt;    &lt;list&gt;     &lt;num&gt;\n1:     5       187       0.9761367          &lt;list[4]&gt; &lt;list[3]&gt;  2.371042\n\ninstance$result$learner_param_vals\n\n[[1]]\n[[1]]$num.threads\n[1] 1\n\n[[1]]$mtry\n[1] 5\n\n[[1]]$num.trees\n[1] 187\n\n[[1]]$sample.fraction\n[1] 0.9761367\n\n\nAll 50 of the random search evaluations are stored in the archive slot of the instance object.\n\nas.data.table(instance$archive)[, .(mtry, sample.fraction, num.trees, regr.rmse)]\n\n     mtry sample.fraction num.trees regr.rmse\n    &lt;int&gt;           &lt;num&gt;     &lt;int&gt;     &lt;num&gt;\n 1:     5       0.9761367       187  2.371042\n 2:     8       0.3760474      1227  3.165695\n 3:     4       0.9020999       959  2.387701\n 4:    10       0.8781863      1283  2.446111\n 5:     2       0.2718788       161  5.976847\n---                                          \n46:     9       0.2032898       720  5.989323\n47:     7       0.6087772        35  2.758711\n48:     4       0.1445110      1487  5.968638\n49:     6       0.7125770      1637  2.508655\n50:     3       0.1578512       302  5.936757\n\n\nNow let’s visualise the effect of each hyperparameter using marginal plots.\n\nautoplot(instance, type = \"marginal\", cols_x = c(\"mtry\", \"sample.fraction\", \"num.trees\"))\n\n\n\n\n\n\n\n\nAs during the HPO stage, I used 3-fold CV, the model has not seen the full data all at once. So, now I’ll train the model using the optimised hyperparameters.\n\nlrn_ranger_tuned &lt;- lrn(\"regr.ranger\")\nlrn_ranger_tuned$param_set$values = instance$result_learner_param_vals\nlrn_ranger_tuned$train(tsk_mtcars)$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, mtry = 5L, num.threads = 1L,      num.trees = 187L, sample.fraction = 0.976136745349504) \n\nType:                             Regression \nNumber of trees:                  187 \nSample size:                      32 \nNumber of independent variables:  10 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       5.910838 \nR squared (OOB):                  0.837275 \n\n\n\n\n\n\n\n\nSummary of question 1 (hyperparameter tuning with random search)\n\n\n\n\n\nI tuned the regr.ranger learner on the mtcars dataset, focusing on three hyperparameters:\n\nmtry: number of variables considered at each split,\nnum.trees: number of trees in the forest,\nsample.fraction: fraction of the dataset used for each tree.\n\nSteps I took:\n\nExploration I inspected the available parameters for the learner using $param_set.\nLearner setup I defined the tuning ranges with to_tune():\n\nmtry from 1 to 10,\nnum.trees from 1 to 100,000,\nsample.fraction from 0.1 to 1.\n\nTuning instance I created a TuningInstanceSingleCrit using:\n\nthe mtcars task,\n3-fold cross-validation for resampling,\nroot mean squared error (RMSE) as the evaluation metric,\na limit of 50 evaluations using a random search strategy.\n\nRunning the tuner I used tnr(\"random_search\") and called $optimize() to run the search. I then extracted the best hyperparameter combination and the archive of evaluated configurations.\nVisualising results I used marginal plots to visualise the effect of each hyperparameter on the cross-validated RMSE.\nTraining the final model I retrained the regr.ranger model on the full dataset using the best parameters found."
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html#question-2",
    "href": "posts/09-mlr3_hyperparameter/index.html#question-2",
    "title": "Getting Started with {mlr3}",
    "section": "Question 2",
    "text": "Question 2\nEvaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold CV for the outer resampling.\n\nAnswer\nOK, so here we need an outer and inner resampling strategy. The outer resampling strategy will be a 3-fold CV, and the inner resampling strategy will be a holdout validation.\n\n\n\n\n\n\nFigure 1: An illustration of nested resampling. The large blocks represent 3-fold CV for the outer resampling for model evaluation and the small blocks represent 4-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.\n\n\n\nFigure 1 shows an example of a nest resampling strategy (with 3-fold CV on the outer and 4-fold CV on the inner nest). Here, we need to do something slightly different as we are using the holdout resampling strategy on the inner nest.\n\nOuter resampling start\n\nPerform 3-fold cross-validation on the full dataset.\nFor each outer fold, split the data into:\n\nTraining set (light blue blocks)\nTest set (dark blue block)\n\n\nInner resampling\n\nWithin each outer training set, perform holdout validation (assuming 70/30 training-test split).\nThis inner split is used for tuning hyperparameters (not evaluation).\n\nHPO – Hyperparameter tuning\n\nEvaluate different hyperparameter combinations by fitting models on the inner training set and evaluating performance on the inner validation (holdout) set.\nSelect the best hyperparameter configuration based on performance on the inner holdout set.\n\nTraining\n\nFit the model on the entire outer training set using the tuned hyperparameters.\n\nEvaluation\n\nEvaluate the trained model on the outer test set (unseen during tuning).\n\nOuter resampling repeats\n\nRepeat steps 2–5 for each of the 3 outer folds.\n\nAggregation\n\nAverage the 3 outer test performance scores.\nThis gives an unbiased estimate of the model’s generalisation performance with tuning.\n\n\nI will use AutoTuner to do nested resampling, as that is what is done in Section 4.3.1 of the mlr3 tutorial.\n\n## create auto_tuner to resample a random forest \n# with 3-fold CV in outer resampling and \n# holdout validation in inner resampling\nat &lt;- auto_tuner(\n1         tuner = tnr(\"random_search\"),\n         learner = lrn(\"regr.ranger\",\n                       mtry = to_tune(1, 1e1),\n                       num.trees = to_tune(1, 1e5),\n                       sample.fraction = to_tune(0.1, 1)),\n         # inner resampling\n         resampling = rsmp(\"holdout\", ratio = 0.7),\n         measure = msr(\"regr.rmse\"),\n         terminator = trm(\"evals\", n_evals = 50)\n        )\n\n# resampling step\nrr &lt;- resample(tsk(\"mtcars\"),\n               at, \n               # outer resampling\n               rsmp(\"cv\", folds = 3), \n2               store_models = TRUE)\n\nrr\n\n\n1\n\nThe tuners and learners are the same as in the previous exercise, I’m just defining them again here for clarity.\n\n2\n\nSet store_models = TRUE so that the AutoTuner models (fitted on the outer training data) are stored,\n\n\n\n\n&lt;ResampleResult&gt; with 3 resampling iterations\n task_id        learner_id resampling_id iteration  prediction_test warnings\n  mtcars regr.ranger.tuned            cv         1 &lt;PredictionRegr&gt;        0\n  mtcars regr.ranger.tuned            cv         2 &lt;PredictionRegr&gt;        0\n  mtcars regr.ranger.tuned            cv         3 &lt;PredictionRegr&gt;        0\n errors\n      0\n      0\n      0\n\n\nNow I aggregate across the three outer folds to get the final performance.\n\nrr$aggregate()\n\nregr.mse \n13.13362 \n\n\nThe inner tuning results can also be accessed, returning the optimal configurations (across all outer folds) and the full tuning archives.\n\n# optimal configurations\nextract_inner_tuning_results(rr)\n\n   iteration  mtry num.trees sample.fraction regr.rmse learner_param_vals\n       &lt;int&gt; &lt;int&gt;     &lt;int&gt;           &lt;num&gt;     &lt;num&gt;             &lt;list&gt;\n1:         1     1     81784       0.6769030  2.884958          &lt;list[4]&gt;\n2:         2     5     73924       0.9194928  2.065022          &lt;list[4]&gt;\n3:         3     3     15719       0.7797092  1.807826          &lt;list[4]&gt;\n    x_domain task_id        learner_id resampling_id\n      &lt;list&gt;  &lt;char&gt;            &lt;char&gt;        &lt;char&gt;\n1: &lt;list[3]&gt;  mtcars regr.ranger.tuned            cv\n2: &lt;list[3]&gt;  mtcars regr.ranger.tuned            cv\n3: &lt;list[3]&gt;  mtcars regr.ranger.tuned            cv\n\n# full tuning archives\nextract_inner_tuning_archives(rr)\n\n     iteration  mtry num.trees sample.fraction regr.rmse x_domain_mtry\n         &lt;int&gt; &lt;int&gt;     &lt;int&gt;           &lt;num&gt;     &lt;num&gt;         &lt;int&gt;\n  1:         1     2     15547       0.3219244  5.115809             2\n  2:         1     8     23468       0.3020848  5.113502             8\n  3:         1     5     47564       0.3500007  5.115646             5\n  4:         1     9     14922       0.8105999  3.261807             9\n  5:         1     2     81447       0.1308460  5.124990             2\n ---                                                                  \n146:         3     5     20556       0.5608060  2.081965             5\n147:         3     2      9179       0.8461799  1.867438             2\n148:         3     4     93847       0.1486419  3.733662             4\n149:         3     3     15719       0.7797092  1.807826             3\n150:         3     1     31967       0.2513883  3.739324             1\n     x_domain_num.trees x_domain_sample.fraction runtime_learners\n                  &lt;int&gt;                    &lt;num&gt;            &lt;num&gt;\n  1:              15547                0.3219244            0.068\n  2:              23468                0.3020848            0.096\n  3:              47564                0.3500007            0.202\n  4:              14922                0.8105999            0.115\n  5:              81447                0.1308460            0.388\n ---                                                             \n146:              20556                0.5608060            0.126\n147:               9179                0.8461799            0.053\n148:              93847                0.1486419            0.376\n149:              15719                0.7797092            0.096\n150:              31967                0.2513883            0.119\n               timestamp warnings errors batch_nr  resample_result task_id\n                  &lt;POSc&gt;    &lt;int&gt;  &lt;int&gt;    &lt;int&gt;           &lt;list&gt;  &lt;char&gt;\n  1: 2025-04-05 13:18:36        0      0        1 &lt;ResampleResult&gt;  mtcars\n  2: 2025-04-05 13:18:36        0      0        2 &lt;ResampleResult&gt;  mtcars\n  3: 2025-04-05 13:18:36        0      0        3 &lt;ResampleResult&gt;  mtcars\n  4: 2025-04-05 13:18:36        0      0        4 &lt;ResampleResult&gt;  mtcars\n  5: 2025-04-05 13:18:37        0      0        5 &lt;ResampleResult&gt;  mtcars\n ---                                                                      \n146: 2025-04-05 13:19:17        0      0       46 &lt;ResampleResult&gt;  mtcars\n147: 2025-04-05 13:19:17        0      0       47 &lt;ResampleResult&gt;  mtcars\n148: 2025-04-05 13:19:17        0      0       48 &lt;ResampleResult&gt;  mtcars\n149: 2025-04-05 13:19:18        0      0       49 &lt;ResampleResult&gt;  mtcars\n150: 2025-04-05 13:19:18        0      0       50 &lt;ResampleResult&gt;  mtcars\n            learner_id resampling_id\n                &lt;char&gt;        &lt;char&gt;\n  1: regr.ranger.tuned            cv\n  2: regr.ranger.tuned            cv\n  3: regr.ranger.tuned            cv\n  4: regr.ranger.tuned            cv\n  5: regr.ranger.tuned            cv\n ---                                \n146: regr.ranger.tuned            cv\n147: regr.ranger.tuned            cv\n148: regr.ranger.tuned            cv\n149: regr.ranger.tuned            cv\n150: regr.ranger.tuned            cv\n\n\n\n\n\n\n\n\nSummary of question 2 (nested resampling)\n\n\n\n\n\nI evaluated the performance of the tuned model using nested resampling to obtain an unbiased estimate.\nSteps I took:\n\nResampling strategy I used:\n\nouter resampling: 3-fold cross-validation,\ninner resampling: holdout validation with a 70/30 split.\n\nAutoTuner setup I reused the same regr.ranger learner and parameter ranges as in Question 1, wrapped in an AutoTuner. The tuning again used 50 evaluations of random search and MSE as the measure.\nResample execution I called resample() with the outer CV and the AutoTuner, setting store_models = TRUE to keep the fitted models from each outer fold.\nAggregating performance I used $aggregate() to average the MSE across the outer test folds.\nInspecting inner results I extracted the best configurations and full tuning logs from each inner loop using extract_inner_tuning_results() and extract_inner_tuning_archives()."
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html#question-3",
    "href": "posts/09-mlr3_hyperparameter/index.html#question-3",
    "title": "Getting Started with {mlr3}",
    "section": "Question 3",
    "text": "Question 3\nTune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use mlr3tuningspaces and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n\nAnswer\nI’ll use the built-in spam task – since the outcome is categorical, this is a classification task.\n\ntsk_spam &lt;- tsk(\"spam\")\n\nFirst I’ll set up the logistic regression model (with no tuning).\n\n# requires probs to compute the brier score\nlrn_logreg &lt;- lrn(\"classif.log_reg\", predict_type = \"prob\")\n\n\n\nThe XGBoost model has lots of hyperparameters:\n\nlrn(\"classif.xgboost\")$param_set$ids()\n\n [1] \"alpha\"                       \"approxcontrib\"              \n [3] \"base_score\"                  \"booster\"                    \n [5] \"callbacks\"                   \"colsample_bylevel\"          \n [7] \"colsample_bynode\"            \"colsample_bytree\"           \n [9] \"device\"                      \"disable_default_eval_metric\"\n[11] \"early_stopping_rounds\"       \"eta\"                        \n[13] \"eval_metric\"                 \"feature_selector\"           \n[15] \"gamma\"                       \"grow_policy\"                \n[17] \"interaction_constraints\"     \"iterationrange\"             \n[19] \"lambda\"                      \"lambda_bias\"                \n[21] \"max_bin\"                     \"max_delta_step\"             \n[23] \"max_depth\"                   \"max_leaves\"                 \n[25] \"maximize\"                    \"min_child_weight\"           \n[27] \"missing\"                     \"monotone_constraints\"       \n[29] \"nrounds\"                     \"normalize_type\"             \n[31] \"nthread\"                     \"ntreelimit\"                 \n[33] \"num_parallel_tree\"           \"objective\"                  \n[35] \"one_drop\"                    \"outputmargin\"               \n[37] \"predcontrib\"                 \"predinteraction\"            \n[39] \"predleaf\"                    \"print_every_n\"              \n[41] \"process_type\"                \"rate_drop\"                  \n[43] \"refresh_leaf\"                \"reshape\"                    \n[45] \"seed_per_iteration\"          \"sampling_method\"            \n[47] \"sample_type\"                 \"save_name\"                  \n[49] \"save_period\"                 \"scale_pos_weight\"           \n[51] \"skip_drop\"                   \"strict_shape\"               \n[53] \"subsample\"                   \"top_k\"                      \n[55] \"training\"                    \"tree_method\"                \n[57] \"tweedie_variance_power\"      \"updater\"                    \n[59] \"verbose\"                     \"watchlist\"                  \n[61] \"xgb_model\"                  \n\n\nThe main ones are:\n\n\n\n\n\n\n\n\nHyperparameter\nDescription\nType\n\n\n\n\neta\nLearning rate (shrinkage)\nnumeric\n\n\nmax_depth\nMaximum depth of trees\ninteger\n\n\nnrounds\nNumber of boosting rounds (trees)\ninteger\n\n\ncolsample_bytree\nFraction of features randomly sampled per tree\nnumeric\n\n\nsubsample\nFraction of rows sampled per tree\nnumeric\n\n\nmin_child_weight\nMinimum sum of instance weights in a child node\nnumeric\n\n\ngamma\nMinimum loss reduction to make a split\nnumeric\n\n\nlambda\nL2 regularisation term on weights\nnumeric\n\n\nalpha\nL1 regularisation term on weights\nnumeric\n\n\n\nA typical tuning strategy for XGBoost might involve:\n\nStarting with basic tree shape and learning rate:\n\nmax_depth\neta\nnrounds\n\nAdding sampling and regularisation to control overfitting:\n\nsubsample\ncolsample_bytree\nmin_child_weight\ngamma\n\nFine-tuning regularisation terms if needed:\n\nlambda (L2)\nalpha (L1)\n\n\nFor the XGBoost learner, I’m going to use a predefined search space from {mlr3tuningspaces}. First, I’ll give a list of these predefined spaces.\n\nmlr_tuning_spaces$keys()[grepl(\"xgboost\", mlr_tuning_spaces$keys())]\n\n[1] \"classif.xgboost.default\" \"classif.xgboost.rbv1\"   \n[3] \"classif.xgboost.rbv2\"    \"regr.xgboost.default\"   \n[5] \"regr.xgboost.rbv1\"       \"regr.xgboost.rbv2\"      \n\n\nI will use the classif.xgboost.default space.\n\nspace = lts(\"classif.xgboost.default\")\nspace\n\n&lt;TuningSpace:classif.xgboost.default&gt;: Classification XGBoost with Default\n                  id lower upper levels logscale\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;list&gt;   &lt;lgcl&gt;\n1:               eta 1e-04     1 [NULL]     TRUE\n2:           nrounds 1e+00  5000 [NULL]    FALSE\n3:         max_depth 1e+00    20 [NULL]    FALSE\n4:  colsample_bytree 1e-01     1 [NULL]    FALSE\n5: colsample_bylevel 1e-01     1 [NULL]    FALSE\n6:            lambda 1e-03  1000 [NULL]     TRUE\n7:             alpha 1e-03  1000 [NULL]     TRUE\n8:         subsample 1e-01     1 [NULL]    FALSE\n\n\nPlugging this into auto_tuner() creates an AutoTuner object. I’m going to use 5-fold CV in the inner resampling and a terminator based on run time (of 60 seconds).\n\n# create terminator with time budget of 60 secs\ntrm_rt = trm(\"run_time\")\ntrm_rt$param_set$values$secs = 60\n\n# create xgboost learner with prob predict_type\n# 'prob' required for brier score\nlrn_xgb = lrn(\"classif.xgboost\", predict_type = \"prob\")\n\nat_xgb &lt;- auto_tuner(learner = lrn_xgb,\n                    resampling = rsmp(\"cv\", folds = 5),\n                    measure = msr(\"classif.bbrier\"),\n                    terminator = trm_rt,\n                    tuner = tnr(\"random_search\"),\n                    search_space = space)\nat_xgb\n\n&lt;AutoTuner:classif.xgboost.tuned&gt;\n* Model: list\n* Parameters: list()\n* Packages: mlr3, mlr3tuning, mlr3learners, xgboost\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, missings, multiclass,\n  offset, twoclass, weights\n* Search Space:\n                  id    class     lower       upper nlevels\n              &lt;char&gt;   &lt;char&gt;     &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1:             alpha ParamDbl -6.907755    6.907755     Inf\n2: colsample_bylevel ParamDbl  0.100000    1.000000     Inf\n3:  colsample_bytree ParamDbl  0.100000    1.000000     Inf\n4:               eta ParamDbl -9.210340    0.000000     Inf\n5:            lambda ParamDbl -6.907755    6.907755     Inf\n6:         max_depth ParamInt  1.000000   20.000000      20\n7:           nrounds ParamInt  1.000000 5000.000000    5000\n8:         subsample ParamDbl  0.100000    1.000000     Inf\n\n\nNow I can set up the outer resampling strategy (4-fold CV).\n\nouter_rsmp &lt;- rsmp(\"cv\", folds = 4)\n\nI can create a benchmark grid and run it for the task to compare the two learners.\n\n# Benchmark both learners\ndesign = benchmark_grid(\n  tasks = tsk_spam,\n  learners = list(lrn_logreg, at_xgb),\n  resamplings = outer_rsmp\n)\ndesign\n\n     task               learner resampling\n   &lt;char&gt;                &lt;char&gt;     &lt;char&gt;\n1:   spam       classif.log_reg         cv\n2:   spam classif.xgboost.tuned         cv\n\n# run the benchmark design\nset.seed(101)\nbmr = benchmark(design)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# the score for each of the 4-fold CV outer folds\nbmr$score(msr(\"classif.bbrier\"))[, \n         .(learner_id, resampling_id, iteration, classif.bbrier)\n         ]\n\n              learner_id resampling_id iteration classif.bbrier\n                  &lt;char&gt;        &lt;char&gt;     &lt;int&gt;          &lt;num&gt;\n1:       classif.log_reg            cv         1     0.06236199\n2:       classif.log_reg            cv         2     0.05802780\n3:       classif.log_reg            cv         3     0.05744753\n4:       classif.log_reg            cv         4     0.05651042\n5: classif.xgboost.tuned            cv         1     0.04560204\n6: classif.xgboost.tuned            cv         2     0.02618333\n7: classif.xgboost.tuned            cv         3     0.04326268\n8: classif.xgboost.tuned            cv         4     0.03606656\n\n# the aggregate score for each learner\nbmr$aggregate(msr(\"classif.bbrier\"))[,\n         .(learner_id, resampling_id, classif.bbrier)\n         ]\n\n              learner_id resampling_id classif.bbrier\n                  &lt;char&gt;        &lt;char&gt;          &lt;num&gt;\n1:       classif.log_reg            cv     0.05858693\n2: classif.xgboost.tuned            cv     0.03777865\n\n\nI can use autoplot to plot these results.\n\nautoplot(bmr, measure = msr(\"classif.bbrier\"))\n\n\n\n\n\n\n\n\nSo, XGBoost performs better than the logistic regression model on the spam task. But, the XGBoost model is much more computationally expensive, takes longer to train, and is less interpretable. So, the choice of model is a trade off between performance and interpretability.\n\n\n\n\n\n\nSummary of question 3 (XGBoost vs. logistic regression)\n\n\n\n\n\nI benchmarked a tuned XGBoost model against an untuned logistic regression model on the spam classification task using the Brier score.\nSteps I took:\n\nLoading the task\nI used the built-in tsk(\"spam\").\nLogistic regression setup\nI defined a classif.log_reg learner with predict_type = \"prob\" to enable Brier score calculation.\nXGBoost setup with tuning\nI used classif.xgboost with predict_type = \"prob\" and the predefined tuning space lts(\"classif.xgboost.default\") from {mlr3tuningspaces}.\nAutoTuner for XGBoost\nI created an AutoTuner with:\n\n5-fold CV for inner resampling,\n\n60-second time budget via trm(\"run_time\"),\n\nrandom search tuner,\n\nBrier score as the measure.\n\nOuter resampling\nI used 4-fold CV for the outer loop.\nBenchmark setup and execution\nI created a benchmark grid comparing both learners on the task, ran the benchmark, and scored the results using the Brier score.\nResults\nI looked at individual fold scores using bmr$score() and aggregate performance using bmr$aggregate(). I also visualised the comparison with autoplot()."
  },
  {
    "objectID": "posts/09-mlr3_hyperparameter/index.html#question4",
    "href": "posts/09-mlr3_hyperparameter/index.html#question4",
    "title": "Getting Started with {mlr3}",
    "section": "Question4",
    "text": "Question4\nWrite a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: task, learner, search_space, resampling, measure, random_search_stages, and random_search_size. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n    a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.\n    b. Identify the best configuration.\n    c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).\n    d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice."
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html",
    "href": "posts/03-highcharter_graphs/index.html",
    "title": "Some Highcharter Graphs",
    "section": "",
    "text": "In my previous post about {highcharter}, I considered changing some of the defaults for a scatter graph to make it look more appealing and be more accessible. This post will focus on plotting the following different types of graphs:\n\nBar charts, including grouped bar charts\nIcon plots\nLine graphs\nStream graphs\n\nFirst, we will load the packages we require in this post. The {highcharter} (Kunst 2022) and {tidyverse} (Wickham et al. 2019) packages are used throughout. The {medicaldata} package (Higgins 2021) is used to create the bar charts and (some of) the line graphs. The icon plots use data obtained via the {clmnis} package (Dempsey 2025).\n\nKunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. https://CRAN.R-project.org/package=highcharter.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\nHiggins, Peter. 2021. Medicaldata: Data Package for Medical Datasets. https://CRAN.R-project.org/package=medicaldata.\n\n\nCode\nlibrary(highcharter)\nlibrary(paletteer) # colour palettes\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# medical data package\n# use 'remotes::install_github(\"higgi13425/medicaldata\")' to access the 'thiomon' dataset\nlibrary(medicaldata)\n\n# obtaining MP information\n# remotes::install_github(\"houseofcommonslibrary/clmnis\")\nlibrary(clmnis)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#introduction",
    "href": "posts/03-highcharter_graphs/index.html#introduction",
    "title": "Some Highcharter Graphs",
    "section": "",
    "text": "In my previous post about {highcharter}, I considered changing some of the defaults for a scatter graph to make it look more appealing and be more accessible. This post will focus on plotting the following different types of graphs:\n\nBar charts, including grouped bar charts\nIcon plots\nLine graphs\nStream graphs\n\nFirst, we will load the packages we require in this post. The {highcharter} (Kunst 2022) and {tidyverse} (Wickham et al. 2019) packages are used throughout. The {medicaldata} package (Higgins 2021) is used to create the bar charts and (some of) the line graphs. The icon plots use data obtained via the {clmnis} package (Dempsey 2025).\n\nKunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. https://CRAN.R-project.org/package=highcharter.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\nHiggins, Peter. 2021. Medicaldata: Data Package for Medical Datasets. https://CRAN.R-project.org/package=medicaldata.\n\n\nCode\nlibrary(highcharter)\nlibrary(paletteer) # colour palettes\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# medical data package\n# use 'remotes::install_github(\"higgi13425/medicaldata\")' to access the 'thiomon' dataset\nlibrary(medicaldata)\n\n# obtaining MP information\n# remotes::install_github(\"houseofcommonslibrary/clmnis\")\nlibrary(clmnis)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#bar-charts",
    "href": "posts/03-highcharter_graphs/index.html#bar-charts",
    "title": "Some Highcharter Graphs",
    "section": "Bar charts",
    "text": "Bar charts\nFor the bar charts, I’m using data obtained from the {medicaldata} package. Loading the Covid data1, and ensuring factors are coded correctly.\n1 Description of the dataset can be found here\n\nCode\ncovid &lt;- tibble(medicaldata::covid_testing)\n\ncovid &lt;- covid |&gt;\n  mutate(across(c(gender, test_id, demo_group, drive_thru_ind, result, payor_group, patient_class), as_factor))\n\n# look at levels of the factors\n#sapply(covid[, c(\"gender\", \"test_id\", \"demo_group\", \"drive_thru_ind\", \"result\", \"payor_group\", \"patient_class\")], levels)\n\ncovid\n\n\n\n  \n\n\n\nLet’s start with a simple bar chart, showing the frequency of negative and positive Covid results.\nFirst we create counts of positive, negative and invalid results.\n\n\nCode\nresult_counts &lt;- covid |&gt;\n  count(result) |&gt;\n  # capitalise first letter\n  mutate(result = str_to_title(as.character(result))) |&gt;\n  arrange(desc(n))\n\n\n\n\nCode\n# Create the bar chart\nhchart(\n    result_counts,\n    type = \"bar\",\n    hcaes(x = result, y = n),\n    name = \"Results\"\n  ) |&gt;\n  hc_title(text = \"Results of Covid Tests\") |&gt;\n  hc_xAxis(title = list(text = \"Result\")) |&gt;\n  hc_yAxis(title = list(text = \"Count\")) |&gt;\n  hc_colors(\"#003087\") |&gt;\n  # a source\n  hc_credits(\n    text = \"Data obtained from the {medicaldata} package\",\n    href = \"https://higgi13425.github.io/medicaldata/\",\n    enabled = TRUE\n    ) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"covid_bar\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n\n\n\n\n\n\n\nGrouped bar chart\nGroup results by gender.\n\n\nCode\nresult_counts_gender &lt;- covid |&gt;\n  group_by(gender) |&gt;\n  count(result) |&gt;\n  # capitalise first letter\n  mutate(result = str_to_title(as.character(result)))\n\n\n\n\n\n\n\n\nThe hover box issues\n\n\n\nThe hard part here seems to be getting the hover box to output the correct things. Specifically, I don’t know how to get the names of the y-axis titles (“Positive”, etc.), without doing nested if statements. It must involve the formatter but I’m not sure how.\nFIXED: use this.key to get the names.\n\n\n\n\nCode\nhchart(\n  result_counts_gender,\n  type = \"bar\",\n  hcaes(x = result, y = n, group = gender) \n  ) |&gt;\n  hc_colors(c(\"#003087\", \"#006747\")) |&gt;\n  hc_title(text = \"Lots of people don't have Covid\",\n           align = \"left\") |&gt;\n  hc_subtitle(text = \"A bar chart showing Covid test results, split by gender.\",\n              align = \"left\") |&gt;\n  hc_xAxis(title = list(text = \"Result\")) |&gt;\n  hc_yAxis(title = list(text = \"Count\")) |&gt;\n  # a source\n  hc_credits(\n    text = \"Data obtained from the {medicaldata} package\",\n    href = \"https://higgi13425.github.io/medicaldata/\",\n    enabled = TRUE\n    ) |&gt;\n   hc_tooltip(\n      formatter = JS(\"function () {\n       if (this.series.name == 'male') {\n        return `&lt;b&gt;Male&lt;/b&gt;&lt;/br&gt;${this.y} ${this.key} results`\n      } else if (this.series.name == 'female') {\n        return `&lt;b&gt;Female&lt;/b&gt;&lt;/br&gt; ${this.y} ${this.key} results`\n      }}\")\n   ) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"covid_bar\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#icons-plot",
    "href": "posts/03-highcharter_graphs/index.html#icons-plot",
    "title": "Some Highcharter Graphs",
    "section": "Icons plot",
    "text": "Icons plot\nLet’s look at the gender split in parliament as of 31st December 2024. We can extract the data using {clmnis} (Dempsey 2025), which is an R package for downloading data from the UK Parliament’s Members Names Information Service (MNIS).\n\nDempsey, Noel. 2025. Clmnis: An r Package for Downloading Data from the Parliamentary Members Names Information Service. https://github.com/houseofcommonslibrary/clmnis.\nObtaining the data:\n\n\nCode\nmps &lt;- clmnis::fetch_mps(on_date = \"2024-12-31\")\n\nmps\n\n\n\n  \n\n\n\n\n\nCode\nmps_gender &lt;- mps |&gt;\n  count(gender) |&gt;\n  mutate(\n    gender = case_match(\n      gender,\n      \"M\" ~ \"Male MPs\",\n      \"F\" ~ \"Female MPs\"\n    )\n  ) |&gt;\n  add_column(col = c(\"#4477AA\", \"#EE6677\"))\n        \nmps_gender\n\n\n\n  \n\n\n\n\nA basic icon chart\nPlotting a simple icon chart. Choose between the “parliament view” and the “circular view” by selecting the relevant tab below.\n\nParliament viewCircle view\n\n\n\n\nCode\nhchart(\n  mps_gender,\n  \"item\",\n  hcaes(\n    name = gender,\n    y = n,\n    color = col\n  ),\n  name = \"Number of MPs\",\n  showInLegend = TRUE,\n  size = \"100%\",\n  center = list(\"50%\", \"75%\"),\n  startAngle = -100,\n  endAngle  = 100\n) %&gt;%\n  hc_title(\n    text = \"Male MPs make up a significant majority of the House of Commons\",\n    align = \"left\"\n    ) %&gt;%\n  hc_subtitle(\n    text = \"An item chart showing the proportion of male and femal MPs in the House of Commons, on 31st December 2024.\",\n    align = \"left\"\n    ) |&gt;\n  hc_legend(labelFormat = '{name} &lt;span style=\"opacity: 0.4\"&gt;{y}&lt;/span&gt;') |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"mp_icon_plot\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n\n\n\n\n\n\n\n\nTo change the plot to a circular layout, set startAngle = -180 and endAngle = 180 and change the center argument.\n\n\nCode\nhchart(\n  mps_gender,\n  \"item\",\n  hcaes(\n    name = gender,\n    y = n,\n    color = col\n  ),\n  name = \"Number of MPs\",\n  showInLegend = TRUE,\n  size = \"100%\",\n  center = list(\"50%\", \"50%\"),\n  startAngle = -180,\n  endAngle  = 180\n) %&gt;%\n  hc_title(\n    text = \"Male MPs make up a significant majority of the House of Commons\",\n    align = \"left\"\n    ) %&gt;%\n  hc_subtitle(\n    text = \"An item chart showing the proportion of male and femal MPs in the House of Commons, on 31st December 2024.\",\n    align = \"left\"\n    ) |&gt;\n  hc_legend(labelFormat = '{name} &lt;span style=\"opacity: 0.4\"&gt;{y}&lt;/span&gt;') |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"mp_icon_plot\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nAdding symbols to the icon chart\nWhat if we want female and male symbols instead of circles, and the icons arranges in a rectangle?\nFirst, lets add the icons to the tibble. We will use the {fontawesome} package (Iannone 2024), alongside the function given in the {highcharter} vignette to obtain the symbols.\n\nIannone, Richard. 2024. Fontawesome: Easily Work with ’Font Awesome’ Icons. https://github.com/rstudio/fontawesome.\n\n\nCode\nfa_to_png_to_datauri &lt;- function(name, ...) {\n  tmpfl &lt;- tempfile(fileext = \".png\")\n  fontawesome::fa_png(name, file = tmpfl, ...)\n  knitr::image_uri(tmpfl)\n\n}\n\n\nAdding the ‘person’ and ‘person-dress’ symbols to the tibble.\n\n\nCode\nmps_gender_icon &lt;- mps_gender |&gt;\n  add_column(faico = c(\"person-dress\", \"person\"))\n\nmps_gender_icon &lt;- mps_gender_icon |&gt;\n  mutate(\n    uri = map2_chr(faico, col, ~fa_to_png_to_datauri(.x, fill = .y)),\n    marker = map(uri, ~ list(symbol = str_glue(\"url({data_uri})\", data_uri = .x)))\n  )\n\n\nCreating the new icon plot.\n\n\nCode\nhchart(\n  mps_gender_icon,\n  \"item\",\n  hcaes(\n    name = gender,\n    y = n,\n    color = col\n  ),\n  name = \"Number of MPs\",\n  showInLegend = TRUE,\n  size = \"100%\"\n) |&gt;\n  hc_title(\n    text = \"Male MPs make up a significant majority of the House of Commons\",\n    align = \"left\"\n    ) |&gt;\n  hc_subtitle(\n    text = \"An item chart showing the proportion of male and femal MPs in the House of Commons, on 31st December 2024.\",\n    align = \"left\"\n    ) |&gt;\n  hc_legend(labelFormat = '{name} &lt;span style=\"opacity: 0.4\"&gt;{y}&lt;/span&gt;') |&gt;\n  hc_plotOptions(\n    item = list(\n      layout = \"vertical\",\n      rows = 18 # Specify the number of rows here\n    )\n  ) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"mp_icon_plot\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#line-graphs",
    "href": "posts/03-highcharter_graphs/index.html#line-graphs",
    "title": "Some Highcharter Graphs",
    "section": "Line graphs",
    "text": "Line graphs\nUsing data obtained from the {gapminder} package (Bryan 2023), we will produce a line graph. A few new things here:\n\nBryan, Jennifer. 2023. Gapminder: Data from Gapminder. https://CRAN.R-project.org/package=gapminder.\n\nWe have used the {paletteer} package (Hvitfeldt 2021) to obtain a colour palette.\nWe have added labels to the lines directly, using plotOptions.series.label.\nWe have set the linewidth directed, in plotOptions.\nWe have removed markers from the lines except when hovered over, in plotOptions.\n\n\nHvitfeldt, Emil. 2021. Paletteer: Comprehensive Collection of Color Palettes. https://github.com/EmilHvitfeldt/paletteer.\n\n\nCode\ngapminder_line &lt;- gapminder |&gt;\n  filter(country %in% c(\"United Kingdom\",\"France\",\"Germany\",\"Italy\",\"Netherlands\"))\n\nhchart(gapminder_line, \n       \"line\",\n        hcaes(x = year, y = pop, group = country)) |&gt;\n  hc_title(\n    text = \"The Netherlands has a much smaller population than Germany.\",\n    align = \"left\"\n    ) |&gt;\n  hc_subtitle(\n    text = \"A line chart showing changes in population between 1952 and 2007.\",\n    align = \"left\"\n    ) |&gt;\n  hc_xAxis(title = list(text = \"Year\")) |&gt;\n  hc_yAxis(title = list(text = \"Population\")) |&gt;\n  # a source\n  hc_credits(\n    text = \"Data obtained from the {gapminder} package\",\n    href = \"https://www.gapminder.org/\",\n    enabled = TRUE\n    ) |&gt;\n  hc_colors(colors = as.character(paletteer::paletteer_d(\"lisa::FridaKahlo\"))) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"population_line\"\n  ) |&gt;\n  hc_plotOptions(\n    series = list(\n   label = list(\n        enabled = TRUE, # add labels to lines\n        style = list(\n          fontWeight = \"bold\",\n          color = \"#333\"\n        ),\n        connectorAllowed = FALSE # include line connecting label to series?\n      ),\n      lineWidth = 2,\n      marker = list(\n        enabled = FALSE, # remove markers\n        symbol = \"circle\",\n        states = list(\n          hover = list(\n            enabled = TRUE # enable markers if hovered over\n          )\n        )\n      )),\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#stream-graphs",
    "href": "posts/03-highcharter_graphs/index.html#stream-graphs",
    "title": "Some Highcharter Graphs",
    "section": "Stream graphs",
    "text": "Stream graphs\nCopying the line graph we produced above.\n\n\nCode\nhchart(gapminder_line, \n       \"streamgraph\", zoomType = \"x\",\n        hcaes(x = year, y = pop, group = country)) |&gt;\n  hc_title(\n    text = \"The populations in these countries are relatively steady over time.\",\n    align = \"left\"\n    ) |&gt;\n  hc_subtitle(\n    text = \"A line chart showing changes in population between 1952 and 2007.\",\n    align = \"left\"\n    ) |&gt;\n  hc_xAxis(title = list(text = \"Year\")) |&gt;\n  hc_yAxis(visible = FALSE, \n           startOnTick = FALSE, endOnTick = FALSE, \n           title = list(text = \"Population\")) |&gt;\n  # a source\n  hc_credits(\n    text = \"Data obtained from the {gapminder} package\",\n    href = \"https://www.gapminder.org/\",\n    enabled = TRUE\n    ) |&gt;\n  hc_colors(colors = as.character(paletteer::paletteer_d(\"lisa::FridaKahlo\"))) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"population_line\"\n  ) |&gt;\n  hc_plotOptions(\n    series = list(\n   label = list(\n        enabled = TRUE, # add labels to lines\n        style = list(\n          fontWeight = \"bold\",\n          color = \"#555555\"\n        ),\n        connectorAllowed = FALSE # include line connecting label to series?\n      ),\n      lineWidth = 2,\n      marker = list(\n        enabled = FALSE, # remove markers\n        symbol = \"circle\",\n        states = list(\n          hover = list(\n            enabled = FALSE # enable markers if hovered over\n          )\n        )\n      )),\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n\n\n\n\n\n\nCopying the example given in the Highchart demos. First, we will create a list of data – medals won by countries in the Winter Olympics, which is taken from Olympedia.\n\n\nCode\ncustom_colors &lt;- as.character(paletteer_d(`\"awtools::bpalette\"`, n = 3))\n  \n# Categories\ncategories &lt;- c(\n  '',\n  '1924 Chamonix', '1928 St. Moritz', '1932 Lake Placid', \n  '1936 Garmisch-Partenkirchen', '1940 &lt;i&gt;Cancelled (Sapporo)&lt;/i&gt;', \n  '1944 &lt;i&gt;Cancelled (Cortina d\\'Ampezzo)&lt;/i&gt;', '1948 St. Moritz', \n  '1952 Oslo', '1956 Cortina d\\'Ampezzo', '1960 Squaw Valley', \n  '1964 Innsbruck', '1968 Grenoble', '1972 Sapporo', \n  '1976 Innsbruck', '1980 Lake Placid', '1984 Sarajevo', \n  '1988 Calgary', '1992 Albertville', '1994 Lillehammer', \n  '1998 Nagano', '2002 Salt Lake City', '2006 Turin', \n  '2010 Vancouver', '2014 Sochi', '2018 PyeongChang', \n  '2022 Beijing'\n)\n\n# medal data\nmedal_data &lt;- list(\n  list(name = \"Finland\", \n       data = c(0, 11, 4, 3, 6, 0, 0, 6, 9, 7, 8, 10, 5, 5, 7, 9, 13, 7, 7, 6, 12, 7, 9, 5, 5, 6, 8)),\n  list(name = \"Austria\", \n       data = c(0, 3, 4, 2, 4, 0, 0, 8, 8, 11, 6, 12, 11, 5, 6, 7, 1, 10, 21, 9, 17, 17, 23, 16, 17, 14, 18)),\n  list(name = \"Sweden\", \n       data = c(0, 2, 5, 3, 7, 0, 0, 10, 4, 10, 7, 7, 8, 4, 2, 4, 8, 6, 4, 3, 3, 7, 14, 11, 15, 14, 18))\n)\n\n\nNow we will create the stream graph. A few novel things here:\n\nI built the graph before adding the data. Note that the data is only added in the very last line, using `hc_add_series_list()\nAnnotations were added using hc_annotations().\n\n\n\nCode\n# Create the chart\nhighchart() |&gt;\n  hc_chart(type = \"streamgraph\", zoomType = \"x\", marginBottom = 30) |&gt;\n  hc_colors(colors = custom_colors) |&gt;\n  hc_title(text = \"Winter Olympic Medal Wins\", align = \"left\", floating = TRUE) |&gt;\n  hc_subtitle(\n    text = 'Source: &lt;a href=\"https://www.olympedia.org/statistics\"&gt;olympedia.org&lt;/a&gt;', \n    align = \"left\", y = 30, floating = TRUE\n  ) |&gt;\n  hc_xAxis(\n    categories = categories, \n    crosshair = TRUE,\n    labels = list(\n      align = \"left\", \n      rotation = 270, \n      reserveSpace = FALSE),\n    lineWidth = 0, # remove x-axis line\n    tickWidth = 0 # remove x-axis tick\n  ) |&gt;\n  hc_yAxis(visible = FALSE, \n           startOnTick = FALSE, endOnTick = FALSE, \n           minPadding = 0.1, maxPadding = 0.15) |&gt;\n  hc_legend(enabled = FALSE) |&gt;\n  hc_annotations(\n    list(\n      labels = list(\n        list(point = list(x = 5.5, xAxis = 0, y = 0, yAxis = 0), \n             text = \"Cancelled&lt;br&gt;during&lt;br&gt;World War II\"),\n        list(point = list(x = 18, xAxis = 0, y = 15, yAxis = 0), \n             text = \"Soviet Union fell,&lt;br&gt;Germany united\"),\n        list(point = list(x = 24.25, xAxis = 0, y = 20, yAxis = 0), \n             text = \"Russia banned from&lt;br&gt;the Olympic Games&lt;br&gt; in 2017\")\n      )\n    )\n  ) |&gt;\n  hc_plotOptions(\n    series = list(\n      label = list(\n        minFontSize = 5, \n        maxFontSize = 15, \n        style = list(color = \"rgba(255,255,255,0.75)\")),\n      accessibility = list(exposeAsGroupOnly = TRUE)\n    )\n  ) |&gt;\n  hc_add_series_list(medal_data)"
  },
  {
    "objectID": "posts/03-highcharter_graphs/index.html#other-charts",
    "href": "posts/03-highcharter_graphs/index.html#other-charts",
    "title": "Some Highcharter Graphs",
    "section": "Other charts",
    "text": "Other charts\nI have shamelessly stolen this next chart from Joshua Kunst who created it in a {highcharter} article, and in turn stole the idea from the Wall Street Journal. But, it is such a nice chart that I can’t not reproduce it here.\n\n\nCode\ndata(vaccines)\n\nfntltp &lt;- JS(\"function(){\n  return this.point.x + ' ' +  this.series.yAxis.categories[this.point.y] + ': ' +\n  Highcharts.numberFormat(this.point.value, 2);\n}\")\n\nplotline &lt;- list(\n  color = \"#fde725\", value = 1963, width = 2, zIndex = 5,\n  label = list(\n    text = \"Vaccine Intoduced\", verticalAlign = \"top\",\n    style = list(color = \"#606060\"), textAlign = \"left\",\n    rotation = 0, y = -5\n  )\n)\n\nhchart(\n  vaccines, \n  \"heatmap\", \n  hcaes(\n    x = year,\n    y = state, \n    value = count\n    )\n  ) |&gt;\n  hc_colorAxis(\n    stops = color_stops(10, viridisLite::inferno(10, direction = -1)),\n    type = \"logarithmic\"\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"\"),\n    reversed = TRUE, \n    offset = -20,\n    tickLength = 0,\n    gridLineWidth = 0, \n    minorGridLineWidth = 0,\n    labels = list(style = list(fontSize = \"9px\"))\n  ) |&gt;\n  hc_tooltip(\n    formatter = fntltp\n    ) |&gt;\n  hc_xAxis(\n    plotLines = list(plotline)) |&gt;\n  hc_title(\n    text = \"Infectious Diseases and Vaccines\"\n    ) |&gt;\n  hc_subtitle(\n    text = \"Number of cases per 100,000 people\"\n  ) |&gt; \n  hc_legend(\n    layout = \"horizontal\",\n    verticalAlign = \"top\",\n    align = \"left\",\n    valueDecimals = 0\n  ) |&gt;\n  hc_size(height = 900)  |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"vaccines\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )"
  },
  {
    "objectID": "posts/02-mandelian_randomisation/index.html",
    "href": "posts/02-mandelian_randomisation/index.html",
    "title": "A First Foray into Genetics, GWAS, and Mendelian Randomisation",
    "section": "",
    "text": "In the previous few weeks I have been trying to learn about Mendelian Randomisation, which has resulted in me doing a whistle-stop tour around genetics, codons, and genetic-wide association studies (GWAS). This post is a collection of my notes and thoughts.1\n1 Please let me know if there are any mistakes or misconceptions"
  },
  {
    "objectID": "posts/02-mandelian_randomisation/index.html#introduction",
    "href": "posts/02-mandelian_randomisation/index.html#introduction",
    "title": "A First Foray into Genetics, GWAS, and Mendelian Randomisation",
    "section": "",
    "text": "In the previous few weeks I have been trying to learn about Mendelian Randomisation, which has resulted in me doing a whistle-stop tour around genetics, codons, and genetic-wide association studies (GWAS). This post is a collection of my notes and thoughts.1\n1 Please let me know if there are any mistakes or misconceptions"
  },
  {
    "objectID": "posts/02-mandelian_randomisation/index.html#genetics-for-dummys",
    "href": "posts/02-mandelian_randomisation/index.html#genetics-for-dummys",
    "title": "A First Foray into Genetics, GWAS, and Mendelian Randomisation",
    "section": "Genetics for dummys",
    "text": "Genetics for dummys\n\n\n\n\n\n\nNote\n\n\n\nBefore diving in, it’s worth remarking that the field of genetics was created before anyone had any idea about the structure of a cell, what DNA looked like, and what chromosomes were. This means the language around this topic can be somewhat confusing for a newcomer, as old terms are still floating around, and overlapping with newer more precise terms.\n\n\nLet’s start by describing the basic mechanics of genetic material and cells. In humans, most cell contains DNA, which is packaged in 23 chromosomes2. DNA is a long code created from four nucleotides – adenine (A), guanine (G), cytosine (C), and thymine (T) – which are paired with each other (AT and CG) to create base pairs. Parts of the DNA do a specific job, and are known as genes. These genes either code for proteins, or help control other genes. Genes are small sections of DNA that have the ability to be copied into an RNA sequence, which can then encode for proteins (or perform other tasks).\n2 by being tightly coiled around proteins known as histones3 The full table showing these combinations can be seen hereA single-nucleotide polymorphism (SNP) is when there is a substitution in a single nucleotide. A SNP in a gene can create differences within populations, for example in the ability to metabolise alcohol. Most of the time, however, they do not create any differences, as when the RNA from genes are read, they are done in codons. A codon is a set of three base pairs, and therefore there are \\(4^3 = 64\\) different combinations of the four nucleotides (\\(61\\) of which specify amino acids). However, as there are only \\(20\\) amino acids, there are many codons with different base pairs that produce the same amino acids. For example, UCU, UCC, UCA, UCG, AGU, AGC all produce the Serine amino acid.3\n\nA short history\nLets start with Gregor Mendel – the “father of modern genetics” and well known for his experiments into the inheritance of pea plants. I won’t go into detail on him, as Wikipedia can do that for me. However, two quick points to make:\n\nThe Laws of Inheritance are important pre-requisite conditions for Mendelian Randomisation to work.\nThe genius of Mendel was his ability to understand that the underlying process was random, and he was only observing a realisation of this random process.\n\nThe double-helix structure of DNA was discovered by Rosalind Franklin, James Watson and Francis Crick in 1953. This knowledge of the structure allowed the further discoveries of how proteins are translated from RNA, which is transcribed by DNA.\n\nBiobanks\nIn the last twenty years or so, large biobanks have been created. These store the genetic and health information of individuals, either at a single point in time or with a temporal aspect. Two examples of large biobanks are:\n\nThe UK Biobank has information on around half-a-million individuals aged between 40 and 69.\nThe Avon Longitudinal Study of Parents and Children. Here, more than 14,000 pregnant women were recruited into the study in 1991/92, with their children, and grandchildren, being followed up in detail.\n\nOne important caveat for these biobanks when using them for research is that they are not representative of the population of the UK. For example, it is known that the individuals in the UK Biobank are more likely to be in a higher social-economic bracket, with fewer lung cancers and other health issues than the general population. This makes performing analysis and obtaining generalisable result (using GWAS4 and Mendelian Randomisation) more difficult.\n4 genome-wise association studies\n\nGenome-wise associated studies\nGenome-wide association studies (GWAS) attempt to find associations between genetic variants and phenotypes. The genetic variants considered are usually SNPs, as opposed to indels5. As biobank sequencing is normally short-read (i.e. only a small section of DNA is read at a time) and the location of these short-reads are random,6 it is trickier to notice indels compared to SNPs. It is also worth noting here that when an individual’s genome is sequenced, the result isn’t the sequence of DNA in a specific cell, but instead is an average. This means that the SNPs in the sequences obtained were present in most of the cells within the individual so most likely have been there since the very early cell division stages. Therefore, if certain SNPs are associated with phenotypes, then the relationship can be thought of a “lifetime” association.\n5 insertions and deletions6 I am not sure if they are truly random along the whole of the DNA - it might just be that there is some uncertainty in the exact location"
  },
  {
    "objectID": "posts/02-mandelian_randomisation/index.html#mendelian-randomisation",
    "href": "posts/02-mandelian_randomisation/index.html#mendelian-randomisation",
    "title": "A First Foray into Genetics, GWAS, and Mendelian Randomisation",
    "section": "Mendelian Randomisation",
    "text": "Mendelian Randomisation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pblog",
    "section": "",
    "text": "Getting Started with {mlr3}\n\n\n03 Hyperparameter Optimisation\n\n\n\ncode\n\n\nr\n\n\nmachine learning\n\n\nmlr3\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with {mlr3}\n\n\n02 Evaluation and Benchmarking\n\n\n\ncode\n\n\nr\n\n\nmachine learning\n\n\nmlr3\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fitting and Validation\n\n\nSimulating the effect of data-splitting and stepwise selection on the lung dataset\n\n\n\ncode\n\n\nr\n\n\nstatistics\n\n\nmodel fitting\n\n\nmodel validation\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fitting and Validation\n\n\nSome critiques of data-splitting and the stepwise procedure\n\n\n\ncode\n\n\nr\n\n\nstatistics\n\n\nmodel fitting\n\n\nmodel validation\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with {mlr3}\n\n\n01 Data and Basic Modeling\n\n\n\ncode\n\n\nr\n\n\nmachine learning\n\n\nmlr3\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nA Bidirectional Bar Chart using Highcharter\n\n\n\n\n\n\ncode\n\n\ngraphics\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSome Highcharter Graphs\n\n\n\n\n\n\ncode\n\n\ngraphics\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nA First Foray into Genetics, GWAS, and Mendelian Randomisation\n\n\n\n\n\n\ngenetics\n\n\ngwas\n\n\nmendelian randomisation\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\nPaul Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Accessible Highcharter\n\n\n\n\n\n\ncode\n\n\naccessibility\n\n\ngraphics\n\n\nr\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nPaul Smith\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Pblog},\n  url = {https://pws3141.github.io/blog/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Pblog.” n.d. https://pws3141.github.io/blog/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a little space on the internet where probably nobody visits. Mainly about maths and R, sometimes both, very occasionally neither. I hope some of these posts are useful to someone."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Leeds | PhD in Statistics | September 2016 - August 2020\nUniversity of Bristol | MMath | September 2011 - June 2015\nUniversity of Hull | PGCE | September 2020 - June 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nNHS Blood and Transplant, Bristol | Statistician | November 2023 - present\nEquality and Human Rights Commission, Cardiff | Statistician | April 2023 - October 2023 (Maternity Cover)\nSecondary School, Bath | Maths Teacher | September 2020 - August 2022\nKPMG, London | Investment Advisor | September 2015 - August 2016"
  },
  {
    "objectID": "posts/01-highcharter/index.html",
    "href": "posts/01-highcharter/index.html",
    "title": "Getting Started with Accessible Highcharter",
    "section": "",
    "text": "Inspired by the quarto and me blog, I am looking into using {highcharter} (Kunst 2022). This package is a wrapper for Highcharts – an interactive charting library1.\n\nKunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. https://CRAN.R-project.org/package=highcharter.\n1 this needs a license for commercial and governmental useIn this post I will only be considering a scatter graph. Different plots – including survival curves – will come later.\n\n\n\nMy main requirements are mostly subjective:\n\nLooks nice\nIs interactive in a nice and obvious way\nIs accessible, following advice given by the Government Analysis Function"
  },
  {
    "objectID": "posts/01-highcharter/index.html#introduction",
    "href": "posts/01-highcharter/index.html#introduction",
    "title": "Getting Started with Accessible Highcharter",
    "section": "",
    "text": "Inspired by the quarto and me blog, I am looking into using {highcharter} (Kunst 2022). This package is a wrapper for Highcharts – an interactive charting library1.\n\nKunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. https://CRAN.R-project.org/package=highcharter.\n1 this needs a license for commercial and governmental useIn this post I will only be considering a scatter graph. Different plots – including survival curves – will come later.\n\n\n\nMy main requirements are mostly subjective:\n\nLooks nice\nIs interactive in a nice and obvious way\nIs accessible, following advice given by the Government Analysis Function"
  },
  {
    "objectID": "posts/01-highcharter/index.html#getting-started-with-highcharter",
    "href": "posts/01-highcharter/index.html#getting-started-with-highcharter",
    "title": "Getting Started with Accessible Highcharter",
    "section": "Getting started with {highcharter}",
    "text": "Getting started with {highcharter}\nLets use the {palmerpenguins} data2 (Horst, Hill, and Gorman 2020).\n2 because penguins are nicer than eugenicists\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\nThis is also the first time I have used the base R pipe |&gt;, after a life-time (well, 5 years) of using %&gt;%. The differences between the two are explain in this tidyverse blog. This means that the below code will not work on R versions prior to 4.1.0.\n\n\nCode\nlibrary(highcharter)\nlibrary(palmerpenguins)\n\n#data(package = 'palmerpenguins')\n\npenguins\n\n\n\n  \n\n\n\nA basic scatter graph, using the hchart function. Here, hcaes is similar in spirit to ggplot’s aes.\n\n\nCode\nhchart(penguins,\"scatter\", \n       hcaes(x = flipper_length_mm, y = bill_length_mm, group = species))\n\n\n\n\n\n\n\n\n\n\nSimple changes\nLets change a few things about the plot:\n\nAdd \\(x\\) and \\(y\\) axis labels;\nAdd a title and subtitle;\nAdd a source;\nChange the colours to the Government Analysis Function categorical data colour palette;\nMake the hover box specify ‘flipper length’ and ‘bill length’.\n\n\n\n\n\n\nGovernment analysis function colour palette\n\n\n\n\nCode\nhc_penguins &lt;- hchart(penguins,\"scatter\", \n       hcaes(x = flipper_length_mm, y = bill_length_mm, group = species)) |&gt;\n  # x axis label\n  hc_xAxis(title = list(text = \"Flipper Length (mm)\")) |&gt;\n  # y axis label\n  hc_yAxis(title = list(text = \"Bill Length (mm)\")) |&gt;\n  # title and subtitle\n  hc_title(text = \"Gentoo's have &lt;i&gt;big&lt;/i&gt; flippers!\",\n           margin = 20, # space between title (or subtitle) and plot [default = 15]\n           align = \"left\",\n           stlyle = list(useHTML = TRUE)) |&gt;\n  hc_subtitle(text = \"A scatter graph showing the relationship between flipper length \n              and bill length, for Adelie, Chinstrap and Gentoo penguins\",\n              align = \"left\") |&gt;\n  # a source\n hc_credits(\n    text = \"Chart created using R and highcharter\",\n    href = \"http://jkunst.com/highcharter\",\n    enabled = TRUE\n    ) |&gt;\n  # hover box options\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br&gt;\",\n    pointFormat = \"Flipper Length: {point.x} mm&lt;br&gt;Bill Length: {point.y} mm\"\n    #&gt; valueSuffix applies globally but only when values are displayed individually\n    #&gt; here, displayed twice so hard-coded into 'pointFormat'\n    #&gt;valueSuffix = \" mm\"\n  ) |&gt;\n  hc_colors(c(\"#12436D\", \"#28A197\", \"#801650\"))\n  \nhc_penguins"
  },
  {
    "objectID": "posts/01-highcharter/index.html#adding-accessibility",
    "href": "posts/01-highcharter/index.html#adding-accessibility",
    "title": "Getting Started with Accessible Highcharter",
    "section": "Adding accessibility",
    "text": "Adding accessibility\nHere we assume the visually aspects of the graph are accessible.3 In this section I will add the following capabilities to the graph.\n3 This is probably a big assumption. I am assuming the following information given by the Government Analysis Function (which apply to static charts) has been applied:\n\nGuidance on designing charts.\nGuidance on the use of colour. For alternative colour palettes, consider Paul Tol’s notes\n\n\nThe ability to download the data;\nKeyboard navigation;\nAlt text, following guidance given by Amy Cesal in her blog post, “Writing Alt Text for Data Visualization”.\n\n\nExporting the data\nFirst, lets try and include a menu to export the data and the plot as an image – this requires using a module. Examples of using modules and plug-ins4 in {highcharter} are given in the modules vignette.\n4 I’m not sure what the difference is between a ‘module’ and a ‘plug-in’, except that the ‘.js’ files seem to live in different folders.\n\nCode\nhc_penguins2 &lt;- hc_penguins |&gt;\n  #hc_add_dependency(name = \"modules/exporting.js\") |&gt; \n  #hc_add_dependency(name = \"modules/export-data.js\") |&gt; \n  hc_exporting(\n    enabled = TRUE,\n    filename = \"palmer_penguins\"\n  )\n\nhc_penguins2\n\n\n\n\n\n\n\n\n\n\n\n\nComment on hc_add_dependency\n\n\n\nIn the quartoandme blog, the following lines are included in the ‘working example’:\n  hc_add_dependency(name = \"modules/accessibility.js\") |&gt; \n  hc_add_dependency(name = \"modules/exporting.js\") |&gt; \n  hc_exporting(\n    enabled = TRUE\n  )\nBut, (I think) the hc_exporting() function automatically includes the exporting.js and export-data.js modules when enabled = TRUE, so the two hc_add_dependency calls are unnecessary. I’m willing to be proved wrong here.\n\n\n\n\nKeyboard navigation\nTo get keyboard navigation working, we need to use the accessibility Highchart module.\n\n\n\n\n\n\nRequired changes to {highcharter} v0.9.4\n\n\n\nIf using v0.9.4 of {highcharter}, then copying the code below will result in no plot being output. This is a known issue, and is discussed in the GitHub repo issue 755.\nThere are two ways to fix this issue:\n\nUncomment the accessibility module in the ‘highcharts.yaml’ file.5 On my Mac, this is found at /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/highcharter/htmlwidgets (for your computer, use .libPaths() to find the default path for packages). The line\n\n    # - modules/accessibility.js\n    needs to be edited to be\n    - modules/accessibility.js\n    before loading the package into R.\n\nInstall an older version of {highcharter}, for example,6\n\nremotes::install_github(\"jbkunst/highcharter@8ff41366c8c411b497b5378d27be48617360f81f\")\n\n\n6 taken from mfherman’s reply to GitHub issue 755.5 Discussed by batpigandme in their reply to GitHub issue 755.\n\nCode\nhc_penguins3 &lt;- hc_penguins |&gt;\n  #hc_add_dependency(name = \"modules/exporting.js\") |&gt; \n  #hc_add_dependency(name = \"modules/export-data.js\") |&gt; \n  hc_add_dependency(name = \"modules/accessibility.js\") |&gt; \n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"palmer_penguins\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n\nhc_penguins3\n\n\n\n\n\n\n\n\nAlt text\n\n\n\n\n\nExample alt-text format for data viz, from Amy Cesal’s Medium post\n\n\nIn this section we add alt-text to the plot, to allow those using screen readers to understand the plot. )\nFirst, lets add accessible descriptions to the plot, by enabling the accessibility options in hc_xAxis and hc_yAxis.\n\n\nCode\nhc_penguins4 &lt;- hchart(penguins,\"scatter\", \n       hcaes(x = flipper_length_mm, y = bill_length_mm, group = species)) |&gt;\n  hc_add_dependency(name = \"modules/accessibility.js\") |&gt; \n  # x axis label\n  hc_xAxis(title = list(text = \"Flipper Length (mm)\"),\n           accessibility = list(\n                   enabled = TRUE,\n                   description = \"flipper length in millimeters\"\n           )) |&gt;\n  # y axis label\n  hc_yAxis(title = list(text = \"Bill Length (mm)\"),\n           accessibility = list(\n                   enabled = TRUE,\n                   description = \"bill length in millimeters\"\n           )) |&gt;\n  # title and subtitle\n  hc_title(text = \"Gentoo's have &lt;i&gt;big&lt;/i&gt; flippers!\",\n           margin = 20, # space between title (or subtitle) and plot [default = 15]\n           align = \"left\",\n           stlyle = list(useHTML = TRUE)) |&gt;\n  hc_subtitle(text = \"A scatter graph showing the relationship between flipper length \n              and bill length, for Adelie, Chinstrap and Gentoo penguins\",\n              align = \"left\") |&gt;\n  # a source\n hc_credits(\n    text = \"Chart created using R and highcharter\",\n    href = \"http://jkunst.com/highcharter\",\n    enabled = TRUE\n    ) |&gt;\n  # hover box options\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br&gt;\",\n    pointFormat = \"Flipper Length: {point.x} mm&lt;br&gt;Bill Length: {point.y} mm\"\n    #&gt; valueSuffix applies globally but only when values are displayed individually\n    #&gt; here, displayed twice so hard-coded into 'pointFormat'\n    #&gt;valueSuffix = \" mm\"\n  ) |&gt;\n  hc_colors(c(\"#12436D\", \"#28A197\", \"#801650\")) |&gt;\n  hc_exporting(\n    accessibility = list(\n      enabled = TRUE # default value is TRUE\n      ),\n    enabled = TRUE,\n    filename = \"palmer_penguins\"\n  ) |&gt;\n  hc_plotOptions(\n    accessibility = list(\n      enabled = TRUE,\n      keyboardNavigation = list(enabled = TRUE)\n      )\n    )\n  \nhc_penguins4\n\n\n\n\n\n\nNote that the desciption in hc_xAxis and hc_yAxis does not start with a capital letter. The reason why is clear from looking at the html output below. Here, aria-hidden=\"false\" refers to Accessible Rich Internet Applications, and is telling screen readers not to ignore this section.\n&lt;div id=\"highcharts-screen-reader-region-before-4\"\naria-label=\"Chart screen reader information, Gentoo's have big flippers!.\"\nstyle=\"position: relative;\" role=\"region\" aria-hidden=\"false\"&gt;\n...\n&lt;h4&gt;Gentoo's have big flippers!&lt;/h4&gt;\n&lt;div&gt;Scatter chart with 3 data series.&lt;/div&gt;\n...\n&lt;div&gt;The chart has 1 X axis displaying flipper length in millimeters. Range: 171.41 to 231.59.&lt;/div&gt;\n&lt;div&gt;The chart has 1 Y axis displaying bill length in millimeters. Range: 30 to 65.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\nThe alt-text is saved as a string to the alt_text_penguins object.\n\n\nCode\nalt_text_penguins &lt;- \"A scatter plot displays the relationship between bill\n        length (mm) on the y-axis and flipper length (mm) on the x-axis for\n        three penguin species: Adelie, Chinstrap, and Gentoo. Each species is\n        represented by a different colour: dark blue for Adelie, teal for Chinstrap,\n        and burgundy for Gentoo. Gentoo penguins have the largest flipper and bill\n        lengths, forming a distinct cluster towards the upper right of the graph.\n        Adelie penguins have smaller flipper and bill lengths, clustering at the lower\n        left, while Chinstrap penguins are positioned between the other two species.\n        The chart highlights that Gentoo penguins have notably large flippers.\"\n\n\n\nUsing Highchart accessibility description\nThe simple way to include this alt-text in the plot would be to use the hc_chart() function with the accessibility.description option set to equal alt_text_penguins. But, as discussed in the Highcharts accessibility documentation,\n\nNote: Since Highcharts now supports captions and linked descriptions, it is preferred to define the description using those methods, as a visible caption/description benefits all users. If the accessibility.description option is defined, the linked description is ignored, and the caption is hidden from screen reader users.\n\n\n\nCode\nhc_penguins4 |&gt;\n  hc_chart(\n    accessibility = list(\n      description = alt_text_penguins\n    )\n  )\n\n\n\n\n\n\n\n\nUsing linkedDescription\nHere, we first define an external HTML element, where the &lt;div&gt; with an ID (chart-description) contains the description of the chart. Then, then the linkedDescription option in hc_chart(accessibility = ...) connects the chart to the &lt;div&gt; by its ID.\n\n\nIf accessibility.description is also defined in the chart, it will override the linked description, as mentioned in the documentation.\n\n\nCode\n# Add an external description for the chart\ndescription_id &lt;- \"chart-description\"\n\ncat(sprintf(\n  '&lt;div id=\"%s\"&gt;\n    A scatter plot displays the relationship between bill length (mm) on the y-axis \n    and flipper length (mm) on the x-axis for three penguin species: Adelie, Chinstrap, \n    and Gentoo. Each species is represented by a different colour: dark blue for Adelie, \n    teal for Chinstrap, and burgundy for Gentoo. Gentoo penguins have the largest flipper \n    and bill lengths, forming a distinct cluster towards the upper right of the graph. \n    Adelie penguins have smaller flipper and bill lengths, clustering at the lower left, \n    while Chinstrap penguins are positioned between the other two species. \n    The chart highlights that Gentoo penguins have notably large flippers.\n  &lt;/div&gt;',\n  description_id\n))\n\n\n&lt;div id=\"chart-description\"&gt;\n    A scatter plot displays the relationship between bill length (mm) on the y-axis \n    and flipper length (mm) on the x-axis for three penguin species: Adelie, Chinstrap, \n    and Gentoo. Each species is represented by a different colour: dark blue for Adelie, \n    teal for Chinstrap, and burgundy for Gentoo. Gentoo penguins have the largest flipper \n    and bill lengths, forming a distinct cluster towards the upper right of the graph. \n    Adelie penguins have smaller flipper and bill lengths, clustering at the lower left, \n    while Chinstrap penguins are positioned between the other two species. \n    The chart highlights that Gentoo penguins have notably large flippers.\n  &lt;/div&gt;\n\n\n\n\nCode\nhc_penguins4 |&gt;\n  hc_chart(\n    accessibility = list(\n      linkedDescription = description_id\n    )\n  )"
  },
  {
    "objectID": "posts/01-highcharter/index.html#finished",
    "href": "posts/01-highcharter/index.html#finished",
    "title": "Getting Started with Accessible Highcharter",
    "section": "Finished",
    "text": "Finished\nWhat have we achieved here? I think we have some good looking graphs, which contain some accessibility features to increase integration with screen-readers. The {highcharter} package seems relatively easy to use, even though the syntax is a little different to what I’m used to (from base R and {ggplot2}).\nWe have:\n\nAdded \\(x\\) and \\(y\\) axis labels, and used these labels in the hover box text.\nAdded a title, subtitle, and a source.\nChanged the colours of the points.\nAllowed for exporting of the data, via hc_exporting().\nAllowed for keyboard navigation, including in the drop-down menu, using hc_add_dependency().\nAdded alt-text, via both description and linkedDescription options.\n\nThe next time I look at Highcharts and {highcharter}, I will be creating different graphs to see what capabilities Highcharts has, and whether it could be useful in my work."
  },
  {
    "objectID": "posts/05-mlr3_basic_modelling/index.html",
    "href": "posts/05-mlr3_basic_modelling/index.html",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "I am attempting to learn how to use {mlr3} (Lang et al. 2019), by reading through the book Applied Machine Learning Using mlr3 in R (Bischl et al. 2024).\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com.\n\nFoss, Natalie, and Lars Kotthoff. 2024. “Data and Basic Modeling.” In Applied Machine Learning Using mlr3 in R, edited by Bernd Bischl, Raphael Sonabend, Lars Kotthoff, and Michel Lang. CRC Press. https://mlr3book.mlr-org.com/data_and_basic_modeling.html.\nIn this first blog post, I am going through the exercises given in Section 2 (Foss and Kotthoff 2024). This involves creating a classification tree model, on the PimaIndiansDiabetes2 (from the {mlbench} package), to predict whether a person has diabetes or not. No (proper) evaluation or validation is done here – that’ll be for a later post.\n\n\n\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)"
  },
  {
    "objectID": "posts/05-mlr3_basic_modelling/index.html#prerequisites",
    "href": "posts/05-mlr3_basic_modelling/index.html#prerequisites",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "library(mlr3)\nlibrary(mlr3viz)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)"
  },
  {
    "objectID": "posts/05-mlr3_basic_modelling/index.html#sec-question-one",
    "href": "posts/05-mlr3_basic_modelling/index.html#sec-question-one",
    "title": "Getting Started with {mlr3}",
    "section": "Question 1",
    "text": "Question 1\nTrain a classification model with the \"classif.rpart\" learner on the Pima Indians Diabetes dataset. Do this without using tsk(\"pima\"), and instead by constructing a task from the dataset in the mlbench package: data(PimaIndiansDiabetes2, package = \"mlbench\").\n\n\n\n\n\n\nMissing data\n\n\n\n\n\nNote: The dataset has NAs in its features. You can either rely on rpart’s capability to handle them internally (surrogate splits) or remove them from the initial data.frame using na.omit().\nThe rpart algorithm has a built-in method called surrogate splits, which allows it to handle missing values without removing data. If a feature value is missing at a particular split, rpart:\n\nTries to use an alternative feature (a surrogate variable) that closely mimics the main splitting feature.\nIf no good surrogate is found, it assigns the most common class (for classification) or the mean value (for regression) within that split.\n\n\n\n\n\nMake sure to define the pos outcome as the positive class.\nTrain the model on a random 80% subset of the given data and evaluate its performance with the classification error measure on the remaining data.\n\n\nAnswer\nLoading the data:\n\ndata(PimaIndiansDiabetes2, package = \"mlbench\")\npima &lt;- as.data.table(PimaIndiansDiabetes2)\npima\n\n     pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n        &lt;num&gt;   &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt; &lt;num&gt;   &lt;fctr&gt;\n  1:        6     148       72      35      NA  33.6    0.627    50      pos\n  2:        1      85       66      29      NA  26.6    0.351    31      neg\n  3:        8     183       64      NA      NA  23.3    0.672    32      pos\n  4:        1      89       66      23      94  28.1    0.167    21      neg\n  5:        0     137       40      35     168  43.1    2.288    33      pos\n ---                                                                        \n764:       10     101       76      48     180  32.9    0.171    63      neg\n765:        2     122       70      27      NA  36.8    0.340    27      neg\n766:        5     121       72      23     112  26.2    0.245    30      neg\n767:        1     126       60      NA      NA  30.1    0.349    47      pos\n768:        1      93       70      31      NA  30.4    0.315    23      neg\n\n\nI want to predict whether each person has diabetes, using a CART (‘classification and regression tree’).\n\nCreating a task\nFirst, I create the task. I am defining pos to be the positive class in this step. It can also be done later by setting tsk_pima$positive = \"pos\".\n\ntsk_pima &lt;- as_task_classif(pima, target = \"diabetes\", positive = \"pos\")\ntsk_pima\n\n&lt;TaskClassif:pima&gt; (768 x 9)\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n    triceps\n\n\n\n#autoplot(tsk_pima, type = \"duo\") +\n  #theme(strip.text.y = element_text(angle = -0, size = 8))\n\nautoplot(tsk_pima, type = \"pairs\")\n\n\n\n\n\n\n\nFigure 1: A pairs plot of the pima dataset. Note that it is unbalanced, as there are more negative diabetes outcomes than positive.\n\n\n\n\n\nLet’s see how unbalanced the data is…\n\npima[, .N, by = \"diabetes\"]\n\n   diabetes     N\n     &lt;fctr&gt; &lt;int&gt;\n1:      pos   268\n2:      neg   500\n\n\n\n\nSplitting the data\nCreate a split of \\(80\\%\\) training and \\(20\\%\\) test data.\n\n\n\n\n\n\nImportant\n\n\n\nI know this is bad practice. Most of the time (see below for caveats), all the data should be used to fit the model, and then internal validation done via resampling (e.g. using bootstrap or cross-validation).\nFrom Frank Harrell’s blog,\n\ndata splitting is an unstable method for validating models or classifiers, especially when the number of subjects is less than about 20,000 (fewer if signal:noise ratio is high). This is because were you to split the data again, develop a new model on the training sample, and test it on the holdout sample, the results are likely to vary significantly. Data splitting requires a significantly larger sample size than resampling to work acceptably well\n\nAlso see Steyerberg (2018).\nTo chose whether to do internal or external validation, see the Biostatistics for Biomedical Research summary.\n\n\n\nSteyerberg, Ewout W. 2018. “Validation in Prediction Research: The Waste by Data Splitting.” Journal of Clinical Epidemiology 103: 131–33.\n\nset.seed(52)\nsplits &lt;- partition(tsk_pima, ratio = 0.8)\nsplits\n\n$train\n  [1]   1   2   3   4   5   8   9  10  11  12  14  18  19  20  21  22  23  24\n [19]  25  27  28  29  30  31  32  34  35  36  37  38  39  40  41  42  43  44\n [37]  46  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64\n [55]  65  66  69  70  71  72  77  78  79  80  81  82  83  85  86  88  89  90\n [73]  91  92  93  94  97  98  99 100 101 103 104 105 106 107 108 109 110 111\n [91] 112 113 114 116 117 118 119 120 121 122 123 124 125 128 129 130 132 133\n[109] 135 136 137 138 139 140 142 143 144 145 146 148 149 151 152 153 157 158\n[127] 159 160 161 162 163 164 165 166 167 169 170 171 173 174 175 177 178 179\n[145] 180 181 182 183 184 185 186 187 188 189 190 191 194 195 196 197 198 199\n[163] 200 201 202 203 204 205 206 207 209 210 212 213 215 216 217 218 220 221\n[181] 222 224 225 226 228 229 231 233 234 235 236 237 238 243 244 245 246 247\n[199] 248 251 252 255 256 257 258 259 261 262 263 264 266 267 268 269 270 271\n[217] 273 275 276 277 279 280 281 282 283 284 285 286 287 290 291 292 293 294\n[235] 295 297 298 299 301 302 305 306 307 309 310 311 312 313 314 315 316 318\n[253] 319 320 321 322 324 326 327 328 329 330 331 332 333 334 335 336 337 339\n[271] 340 341 343 344 346 347 349 350 351 352 353 354 355 356 357 358 359 360\n[289] 361 364 365 366 367 368 369 370 371 372 373 374 375 376 378 379 380 381\n[307] 382 384 386 387 389 390 391 392 393 394 395 396 397 398 399 401 402 403\n[325] 404 405 406 407 408 409 410 411 412 414 415 416 417 418 420 421 422 423\n[343] 424 425 427 429 430 433 436 437 439 440 441 442 443 444 445 446 447 448\n[361] 449 450 451 452 453 454 455 456 457 459 461 463 464 465 466 469 470 471\n[379] 472 473 474 476 477 478 482 483 484 485 486 487 488 489 490 491 493 494\n[397] 495 497 498 499 500 501 503 504 505 506 508 509 510 511 512 513 514 515\n[415] 516 517 518 520 521 522 523 524 526 527 528 529 530 532 533 534 535 536\n[433] 537 538 539 542 543 544 545 548 549 550 551 552 553 554 555 556 557 560\n[451] 562 563 564 565 566 568 569 570 572 573 574 575 576 578 579 580 581 582\n[469] 583 584 585 586 588 589 590 591 592 594 595 596 597 598 600 601 602 603\n[487] 604 605 606 607 608 609 610 611 612 615 616 617 618 619 620 621 623 625\n[505] 627 628 629 630 632 634 635 636 637 639 640 641 642 643 644 645 646 647\n[523] 649 650 651 654 656 658 659 660 661 662 663 665 666 667 669 670 672 674\n[541] 676 677 678 681 683 684 686 687 688 689 690 692 693 695 697 698 700 701\n[559] 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 722 723\n[577] 724 725 726 727 728 729 730 732 735 736 737 738 739 740 741 742 743 744\n[595] 746 748 749 750 751 752 753 754 756 757 758 759 760 761 762 763 764 765\n[613] 766 767\n\n$test\n  [1]   6   7  13  15  16  17  26  33  45  47  67  68  73  74  75  76  84  87\n [19]  95  96 102 115 126 127 131 134 141 147 150 154 155 156 168 172 176 192\n [37] 193 208 211 214 219 223 227 230 232 239 240 241 242 249 250 253 254 260\n [55] 265 272 274 278 288 289 296 300 303 304 308 317 323 325 338 342 345 348\n [73] 362 363 377 383 385 388 400 413 419 426 428 431 432 434 435 438 458 460\n [91] 462 467 468 475 479 480 481 492 496 502 507 519 525 531 540 541 546 547\n[109] 558 559 561 567 571 577 587 593 599 613 614 622 624 626 631 633 638 648\n[127] 652 653 655 657 664 668 671 673 675 679 680 682 685 691 694 696 699 718\n[145] 719 720 721 731 733 734 745 747 755 768\n\n$validation\ninteger(0)\n\n\n\n\nTraining the model\nNow, I will train the classification tree on the training data.\n\n# loading the learners\nlrn_featureless &lt;- lrn(\"classif.featureless\", predict_type = \"prob\")\nlrn_rpart &lt;- lrn(\"classif.rpart\", predict_type = \"prob\") # 'prob' is the default prediction type\nlrn_rpart\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n# training the learners\nlrn_featureless$train(tsk_pima, splits$train)\nlrn_rpart$train(tsk_pima, splits$train)\nlrn_rpart\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: rpart\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\n\n\nEvaluating the model\nHere, I’m evaluating the model on the test data (and comparing against the featureless learner).\nI will consider the Brier, log-loss and accuracy measures. The Brier score lies between \\([0,\n1]\\), where \\(0\\) is best. The log-loss is the negative logarithm of the predicted probability for the true class, and the accuracy is the number of correct predictions divided by total number of predictions.\n\n# load accuracy measures\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\n\n# predicting using the featureless learner\nprediction_featureless &lt;- lrn_featureless$predict(tsk_pima, splits$test)\nprediction_featureless \n\n&lt;PredictionClassif&gt; for 154 observations:\n row_ids truth response  prob.pos  prob.neg\n       6   neg      neg 0.3485342 0.6514658\n       7   pos      neg 0.3485342 0.6514658\n      13   neg      neg 0.3485342 0.6514658\n     ---   ---      ---       ---       ---\n     747   pos      neg 0.3485342 0.6514658\n     755   pos      neg 0.3485342 0.6514658\n     768   neg      neg 0.3485342 0.6514658\n\n\n\n# obtaining score of featureless learner\nprediction_featureless$score(measures)\n\n classif.mbrier classif.logloss     classif.acc \n      0.4553977       0.6478575       0.6493506 \n\n\n\n# predicting using the classification tree\nprediction_rpart &lt;- lrn_rpart$predict(tsk_pima, splits$test)\nprediction_rpart \n\n&lt;PredictionClassif&gt; for 154 observations:\n row_ids truth response   prob.pos  prob.neg\n       6   neg      neg 0.00000000 1.0000000\n       7   pos      neg 0.08675799 0.9132420\n      13   neg      neg 0.24390244 0.7560976\n     ---   ---      ---        ---       ---\n     747   pos      pos 0.77777778 0.2222222\n     755   pos      pos 0.76744186 0.2325581\n     768   neg      neg 0.08675799 0.9132420\n\n\n\n# obtaining score of the classification tree\nprediction_rpart$score(measures) \n\n classif.mbrier classif.logloss     classif.acc \n      0.2763229       0.8516860       0.8246753 \n\n\n\n# confusion matrix\n1prediction_rpart$confusion\n\n\n1\n\nAll off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified.\n\n\n\n\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n\n\n\nprediction_plot &lt;- autoplot(prediction_rpart) + ggtitle(\"Default\")\nprediction_plot"
  },
  {
    "objectID": "posts/05-mlr3_basic_modelling/index.html#question-2",
    "href": "posts/05-mlr3_basic_modelling/index.html#question-2",
    "title": "Getting Started with {mlr3}",
    "section": "Question 2",
    "text": "Question 2\nCalculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in Exercise 1.\n\nTry to solve this in two ways:\n\nUsing mlr3measures-predefined measure objects.\nWithout using mlr3 tools by directly working on the ground truth and prediction vectors.\n\nCompare the results.\n\n\nAnswer\nI’ve already started this in Question 1 (Section 2.1.1.4), but I will reiterate here. The confusion matrix gives the number of predictions that are correct (true positives or negatives) on the diagonal, and those that are incorrect (false positives and negatives) on the top right and bottom left, respectively\n\n# confusion matrix\nconf_matrix &lt;- prediction_rpart$confusion\nconf_matrix\n\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n\n\nI want to obtain the rates, both using the mlr3measures objects, and without.\n\n\n\nSensitivity\n\n(true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\n\nSpecificity\n\n(true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.\n\n\n\nUsing mlr3measures\nFirst, let’s figure out the measures we need…\n\nas.data.table(mlr_measures)[task_type == \"classif\" & predict_type == \"response\"]\n\nKey: &lt;key&gt;\n                    key                         label task_type\n                 &lt;char&gt;                        &lt;char&gt;    &lt;char&gt;\n 1:         classif.acc       Classification Accuracy   classif\n 2:        classif.bacc             Balanced Accuracy   classif\n 3:          classif.ce          Classification Error   classif\n 4:       classif.costs Cost-sensitive Classification   classif\n 5:         classif.dor         Diagnostic Odds Ratio   classif\n---                                                            \n19: classif.specificity                   Specificity   classif\n20:          classif.tn                True Negatives   classif\n21:         classif.tnr            True Negative Rate   classif\n22:          classif.tp                True Positives   classif\n23:         classif.tpr            True Positive Rate   classif\n             packages predict_type properties task_properties\n               &lt;list&gt;       &lt;char&gt;     &lt;list&gt;          &lt;list&gt;\n 1: mlr3,mlr3measures     response                           \n 2: mlr3,mlr3measures     response                           \n 3: mlr3,mlr3measures     response                           \n 4:              mlr3     response                           \n 5: mlr3,mlr3measures     response                   twoclass\n---                                                          \n19: mlr3,mlr3measures     response                   twoclass\n20: mlr3,mlr3measures     response                   twoclass\n21: mlr3,mlr3measures     response                   twoclass\n22: mlr3,mlr3measures     response                   twoclass\n23: mlr3,mlr3measures     response                   twoclass\n\n\nOK, so we need to use the measures classif.tpr classif.fpr classif.tnr and classif.fnr, for the true positive, false positive, true negative and false negative rates, respectively.\n\nmeasures &lt;- msrs(c(\"classif.tpr\", \"classif.fpr\", \"classif.tnr\", \"classif.fnr\"))\nprediction_rpart$score(measures)\n\nclassif.tpr classif.fpr classif.tnr classif.fnr \n  0.7407407   0.1300000   0.8700000   0.2592593 \n\n\n\n\nWithout using mlr3measures\nI can obtain these rates directly from the confusion matrix.\n\nstr(conf_matrix)\n\n 'table' int [1:2, 1:2] 40 14 13 87\n - attr(*, \"dimnames\")=List of 2\n  ..$ response: chr [1:2] \"pos\" \"neg\"\n  ..$ truth   : chr [1:2] \"pos\" \"neg\"\n\n# true positive rate / sensitivity\ntpr &lt;- conf_matrix[1, 1]/ sum(conf_matrix[, 1])\n# false positive rate\nfpr &lt;- conf_matrix[1, 2]/ sum(conf_matrix[, 2])\n\n# true negative rate / specificity\ntnr &lt;- conf_matrix[2, 2]/ sum(conf_matrix[, 2])\n# false negative rate\nfnr &lt;- conf_matrix[2, 1]/ sum(conf_matrix[, 1])\n\ndata.table(\n  classif.tpr = tpr,\n  classif.fpr = fpr,\n  classif.tnr = tnr,\n  classif.fnr = fnr\n)\n\n   classif.tpr classif.fpr classif.tnr classif.fnr\n         &lt;num&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1:   0.7407407        0.13        0.87   0.2592593"
  },
  {
    "objectID": "posts/05-mlr3_basic_modelling/index.html#question-3",
    "href": "posts/05-mlr3_basic_modelling/index.html#question-3",
    "title": "Getting Started with {mlr3}",
    "section": "Question 3",
    "text": "Question 3\nChange the threshold of the model from Question 1 such that the false positive rate is lower than the false negative rate.\n\nWhat is one reason you might do this in practice?\n\n\nAnswer\nOne reason I might want a lower false positive rate than false negative rate is it the damage done by a false positive is higher than that done by a false negative. That if, if classifying the outcome as positive when it is actually negative is more damaging than the other way round. For example, if I am building a model to predict fraud for a bank, and a false positive would result in a customer transaction being wrongly declined. Lots of false positives could result in annoyed customers and a loss of trust.\n\nInverse weights\nLet’s first change the thresholds such that they account for the inbalanced data. I’m not considering false positives here.\nFrom Figure 1, it’s clear that the data is unbalanced (more people with negative diabetes than positive). I can account for this by changing the thresholds using inverse weightings.\nFirst, let’s use the training data to obtain new thresholds.\n\nnew_thresh = proportions(table(tsk_pima$truth(splits$train)))\nnew_thresh\n\n\n      pos       neg \n0.3485342 0.6514658 \n\n\nAnd then I’ll use these thresholds to reweight the model.\n\nprediction_rpart$set_threshold(new_thresh)\nprediction_rpart$confusion\n\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n\nprediction_plot_newt &lt;- autoplot(prediction_rpart) +\n                                ggtitle(\"Inverse weighting thresholds\")\nprediction_plot + prediction_plot_newt +\n        plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nOh, it doesn’t make a difference!\n\n\nReducing false positive rate\nThis can be achieved by making it more difficult for the model to predict a positive result.\nSo, let’s create thresholds where the pos result is penalised.\n\nnew_thresh &lt;- c(\"pos\" = 0.7, \"neg\" = 0.3)\n\n\nprediction_rpart$set_threshold(new_thresh)\nprediction_rpart$confusion\n\n        truth\nresponse pos neg\n     pos  35  10\n     neg  19  90\n\nmeasures &lt;- msrs(c(\"classif.tpr\", \"classif.fpr\", \"classif.tnr\", \"classif.fnr\"))\nprediction_rpart$score(measures)\n\nclassif.tpr classif.fpr classif.tnr classif.fnr \n  0.6481481   0.1000000   0.9000000   0.3518519 \n\nprediction_plot_newt &lt;- autoplot(prediction_rpart) +\n                                ggtitle(\"New thresholds\")\nprediction_plot + prediction_plot_newt +\n        plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nHere, the false positive rate has decreased, but the false negative has increased (as expected)."
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "I am attempting to learn how to use {mlr3} (Lang et al. 2019), by reading through the book Applied Machine Learning Using mlr3 in R (Bischl et al. 2024).\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com.\nMy previous posts include:\n\nPart one:\n\nCreate a classification tree model to predict diabetes.\nLook at the confusion matrix and create measures without using {mlr3measures}.\nChange the thresholds in the model.\n\n\nIn this second blog post, I am going through the exercises given in Section 3 (Casalicchio and Burk 2024). This involves using repeated cross-validation resampling, using a custom resampling strategy, and creating a function that produces a ROC.\n\nCasalicchio, Giuseppe, and Lukas Burk. 2024. “Evaluation and Benchmarking.” In Applied Machine Learning Using mlr3 in R, edited by Bernd Bischl, Raphael Sonabend, Lars Kotthoff, and Michel Lang. CRC Press. https://mlr3book.mlr-org.com/evaluation_and_benchmarking.html.\n\n\n\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3data)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)"
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html#prerequisites",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html#prerequisites",
    "title": "Getting Started with {mlr3}",
    "section": "",
    "text": "library(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3data)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)"
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html#question-1",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html#question-1",
    "title": "Getting Started with {mlr3}",
    "section": "Question 1",
    "text": "Question 1\nApply a repeated cross-validation resampling strategy on tsk(\"mtcars\") and evaluate the performance of lrn(\"regr.rpart\").\n\nUse five repeats of three folds each.\nCalculate the MSE for each iteration and visualize the result.\nFinally, calculate the aggregated performance score.\n\n\nAnswer\nFirst, I’ll load the Task, Learner, and create the rsmp() object.\n\ntsk_mtcars &lt;- tsk(\"mtcars\")\ntsk_mtcars\n\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n# load learner\nlrn_rpart &lt;- lrn(\"regr.rpart\")\nlrn_rpart\n\n&lt;LearnerRegrRpart:regr.rpart&gt;: Regression Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, selected_features, weights\n\n# load resampling method: 5 lots of three-fold CV\nrcv53 = rsmp(\"repeated_cv\", repeats = 5, folds = 3)\nrcv53\n\n&lt;ResamplingRepeatedCV&gt;: Repeated Cross-Validation\n* Iterations: 15\n* Instantiated: FALSE\n* Parameters: folds=3, repeats=5\n\n\nNow, I’ll use the resample() function to run the resampling strategy.\n\nrr &lt;- resample(tsk_mtcars, lrn_rpart, rcv53)\nrr\n\n&lt;ResampleResult&gt; with 15 resampling iterations\n task_id learner_id resampling_id iteration  prediction_test warnings errors\n  mtcars regr.rpart   repeated_cv         1 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         2 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         3 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         4 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         5 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         6 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         7 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         8 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv         9 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        10 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        11 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        12 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        13 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        14 &lt;PredictionRegr&gt;        0      0\n  mtcars regr.rpart   repeated_cv        15 &lt;PredictionRegr&gt;        0      0\n\n\nCalculating the MSE for each iteration requires running $score().\n\nrr_mse &lt;- rr$score(msr(\"regr.mse\"))\nrr_mse\n\n    task_id learner_id resampling_id iteration  regr.mse\n     &lt;char&gt;     &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;\n 1:  mtcars regr.rpart   repeated_cv         1 18.862093\n 2:  mtcars regr.rpart   repeated_cv         2 16.995342\n 3:  mtcars regr.rpart   repeated_cv         3 22.749711\n 4:  mtcars regr.rpart   repeated_cv         4 21.232898\n 5:  mtcars regr.rpart   repeated_cv         5 14.973906\n 6:  mtcars regr.rpart   repeated_cv         6 27.803989\n 7:  mtcars regr.rpart   repeated_cv         7 31.085264\n 8:  mtcars regr.rpart   repeated_cv         8 28.174360\n 9:  mtcars regr.rpart   repeated_cv         9 13.289618\n10:  mtcars regr.rpart   repeated_cv        10 22.672955\n11:  mtcars regr.rpart   repeated_cv        11 28.082449\n12:  mtcars regr.rpart   repeated_cv        12  7.416607\n13:  mtcars regr.rpart   repeated_cv        13 21.555455\n14:  mtcars regr.rpart   repeated_cv        14 11.597811\n15:  mtcars regr.rpart   repeated_cv        15 21.823650\nHidden columns: task, learner, resampling, prediction_test\n\n\nLet’s plot this.\n\nautoplot(rr, measure = msr(\"regr.mse\"), type = \"boxplot\") +\nautoplot(rr, measure = msr(\"regr.mse\"), type = \"histogram\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAggregating the MSE scores (using macro aggregation) gives:\n\nrr$aggregate(msr(\"regr.mse\"))\n\nregr.mse \n20.55441"
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html#question-2",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html#question-2",
    "title": "Getting Started with {mlr3}",
    "section": "Question 2",
    "text": "Question 2\nUse tsk(\"spam\") and five-fold CV to benchmark lrn(\"classif.ranger\"), lrn(\"classif.log_reg\"), and lrn(\"classif.xgboost\", nrounds = 100) with respect to AUC.\n\nWhich learner appears to perform best?\nHow confident are you in your conclusion?\nThink about the stability of results and investigate this by re-running the experiment with different seeds.\nWhat can be done to improve this?\n\n\nAnswer\nLet’s load the task, learners and resampling method.\n\ntsk_spam &lt;- tsk(\"spam\")\ntsk_spam\n\n&lt;TaskClassif:spam&gt; (4601 x 58): HP Spam Detection\n* Target: type\n* Properties: twoclass\n* Features (57):\n  - dbl (57): address, addresses, all, business, capitalAve,\n    capitalLong, capitalTotal, charDollar, charExclamation, charHash,\n    charRoundbracket, charSemicolon, charSquarebracket, conference,\n    credit, cs, data, direct, edu, email, font, free, george, hp, hpl,\n    internet, lab, labs, mail, make, meeting, money, num000, num1999,\n    num3d, num415, num650, num85, num857, order, original, our, over,\n    parts, people, pm, project, re, receive, remove, report, table,\n    technology, telnet, will, you, your\n\n# set up leaners\n# first set up the 'lrns()' then modify the xgboost 'nrounds' argument\nlearners &lt;- lrns(c(\"classif.ranger\", \"classif.log_reg\", \"classif.xgboost\"), \n                 predict_type = \"prob\")\n# adjust 'nrounds' argument for xgboost\nlearners$classif.xgboost$param_set$values$nrounds &lt;- 100\nlearners\n\n$classif.ranger\n&lt;LearnerClassifRanger:classif.ranger&gt;: Random Forest\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, missings, multiclass,\n  oob_error, selected_features, twoclass, weights\n\n$classif.log_reg\n&lt;LearnerClassifLogReg:classif.log_reg&gt;: Logistic Regression\n* Model: -\n* Parameters: use_pred_offset=TRUE\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: offset, twoclass, weights\n\n$classif.xgboost\n&lt;LearnerClassifXgboost:classif.xgboost&gt;: Extreme Gradient Boosting\n* Model: -\n* Parameters: nrounds=100, nthread=1, verbose=0\n* Validate: NULL\n* Packages: mlr3, mlr3learners, xgboost\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, internal_tuning, missings,\n  multiclass, offset, twoclass, validation, weights\n\n# set up resampling\ncv5 &lt;- rsmp(\"cv\", folds = 5)\ncv5\n\n&lt;ResamplingCV&gt;: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n\n\nNow we can set up the benchmark grid.\n\nset.seed(1)\n\ndesign &lt;- benchmark_grid(tsk_spam, learners, cv5)\ndesign\n\n     task         learner resampling\n   &lt;char&gt;          &lt;char&gt;     &lt;char&gt;\n1:   spam  classif.ranger         cv\n2:   spam classif.log_reg         cv\n3:   spam classif.xgboost         cv\n\n\nNow, see how well these perform in terms of AUC.2\n2 Recall: AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance\nbmr &lt;- benchmark(design)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nbmr$score(msr(\"classif.auc\"))[, .(learner_id, iteration, classif.auc)]\n\n         learner_id iteration classif.auc\n             &lt;char&gt;     &lt;int&gt;       &lt;num&gt;\n 1:  classif.ranger         1   0.9905328\n 2:  classif.ranger         2   0.9807452\n 3:  classif.ranger         3   0.9861848\n 4:  classif.ranger         4   0.9818771\n 5:  classif.ranger         5   0.9914047\n 6: classif.log_reg         1   0.9729649\n 7: classif.log_reg         2   0.9604636\n 8: classif.log_reg         3   0.9831002\n 9: classif.log_reg         4   0.9646964\n10: classif.log_reg         5   0.9757900\n11: classif.xgboost         1   0.9896168\n12: classif.xgboost         2   0.9844034\n13: classif.xgboost         3   0.9909272\n14: classif.xgboost         4   0.9853751\n15: classif.xgboost         5   0.9919197\n\n\nAnd let’s aggregate by Learner.\n\nbmr$aggregate(msr(\"classif.auc\"))\n\n      nr task_id      learner_id resampling_id iters classif.auc\n   &lt;int&gt;  &lt;char&gt;          &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1    spam  classif.ranger            cv     5   0.9861489\n2:     2    spam classif.log_reg            cv     5   0.9714030\n3:     3    spam classif.xgboost            cv     5   0.9884484\nHidden columns: resample_result\n\n\n\nautoplot(bmr, measure = msr(\"classif.auc\"))\n\n\n\n\n\n\n\n\nSo, from a naive look at this, it appears that the XGBoost model performs the best (highest AUC). However, the results from all three of these models appear very similar, and I would maybe prefer “simpler” models over more flexible ones in this case (here, the logistic regression model).\nIf we run this 5 times with different seeds, let’s see how the AUC varies.\n\nbmr_auc &lt;- rbindlist(lapply(seq_len(5), function(i) {\n                         tmp_seed &lt;- i * 100\n                         set.seed(tmp_seed)\n                         design &lt;- benchmark_grid(tsk_spam, learners, cv5)\n                         bmr &lt;- benchmark(design)\n                         data.table(\n                                       seed = tmp_seed,\n                                       auc = bmr$aggregate(msr(\"classif.auc\"))\n                                       )\n                    })\n                )\n\n\nbmr_auc[, .(seed, auc.learner_id, auc.classif.auc)]\n\n     seed  auc.learner_id auc.classif.auc\n    &lt;num&gt;          &lt;char&gt;           &lt;num&gt;\n 1:   100  classif.ranger       0.9866242\n 2:   100 classif.log_reg       0.9708618\n 3:   100 classif.xgboost       0.9883961\n 4:   200  classif.ranger       0.9864776\n 5:   200 classif.log_reg       0.9705954\n 6:   200 classif.xgboost       0.9879994\n 7:   300  classif.ranger       0.9855379\n 8:   300 classif.log_reg       0.9710461\n 9:   300 classif.xgboost       0.9878387\n10:   400  classif.ranger       0.9866107\n11:   400 classif.log_reg       0.9697749\n12:   400 classif.xgboost       0.9888800\n13:   500  classif.ranger       0.9862768\n14:   500 classif.log_reg       0.9702488\n15:   500 classif.xgboost       0.9879613\n\n# some summary stats\nbmr_auc[, as.list(summary(auc.classif.auc)), by = auc.learner_id]\n\n    auc.learner_id      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n            &lt;char&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:  classif.ranger 0.9855379 0.9862768 0.9864776 0.9863055 0.9866107 0.9866242\n2: classif.log_reg 0.9697749 0.9702488 0.9705954 0.9705054 0.9708618 0.9710461\n3: classif.xgboost 0.9878387 0.9879613 0.9879994 0.9882151 0.9883961 0.9888800\n\n\nAlthough XGBoost achieved the highest AUC on average, the difference compared to ranger was minimal across repeated runs (although XGBoost always very slighly outperforms the random forest model, after aggregation). Confidence intervals or additional repeats could provide better insight into whether the observed difference is meaningful. The choice of model will depend on how important that small difference is in the AUC compared to model complexity."
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html#question-3",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html#question-3",
    "title": "Getting Started with {mlr3}",
    "section": "Question 3",
    "text": "Question 3\nA colleague reports a \\(93.1\\%\\) classification accuracy using lrn(\"classif.rpart\") on tsk(\"penguins_simple\").\n\nYou want to reproduce their results and ask them about their resampling strategy.\nThey said they used a custom three-fold CV with folds assigned as factor(task$row_ids %% 3).\nSee if you can reproduce their results.\n\n\nAnswer\nLet’s have a look at the Task. This task doesn’t seem to be in included in the default {mlr3} package, but is referenced in the {mlr3data} (Becker 2024) docs.\n\nBecker, Marc. 2024. Mlr3data: Collection of Machine Learning Data Sets for ’Mlr3’. https://github.com/mlr-org/mlr3data.\n\ntsk_penguins &lt;- tsk(\"penguins_simple\")\ntsk_penguins\n\n&lt;TaskClassif:penguins&gt; (333 x 11): Simplified Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (10):\n  - dbl (7): bill_depth, bill_length, island.Biscoe, island.Dream,\n    island.Torgersen, sex.female, sex.male\n  - int (3): body_mass, flipper_length, year\n\n\nOK, so this is a multi-class classification task, using 10 features to predict the species of the penguin.\nThey said they used a custom three-fold CV, so let’s try and reproduce this. By looking at factor(tsk_penguins$row_ids %% 3), we can see that the CV is putting every third observation into the same fold. This feels weird and wrong, but fine.3\n3  This folding strategy does not ensure class balance within each fold and may lead to biased performance estimates, particularly in smaller datasets.\n# load learner\nlrn_rpart &lt;- lrn(\"classif.rpart\")\nlrn_rpart\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n# create custom resampling strategy\nrsmp_custom = rsmp(\"custom_cv\")\nfolds &lt;- factor(tsk_penguins$row_ids %% 3)\nrsmp_custom$instantiate(tsk_penguins, f = folds)\nrr &lt;- resample(tsk_penguins, lrn_rpart, rsmp_custom)\nrr$predictions()\n\n[[1]]\n&lt;PredictionClassif&gt; for 111 observations:\n row_ids     truth  response\n       3    Adelie    Adelie\n       6    Adelie    Adelie\n       9    Adelie    Adelie\n     ---       ---       ---\n     327 Chinstrap Chinstrap\n     330 Chinstrap    Adelie\n     333 Chinstrap Chinstrap\n\n[[2]]\n&lt;PredictionClassif&gt; for 111 observations:\n row_ids     truth  response\n       1    Adelie    Adelie\n       4    Adelie    Adelie\n       7    Adelie    Adelie\n     ---       ---       ---\n     325 Chinstrap Chinstrap\n     328 Chinstrap Chinstrap\n     331 Chinstrap Chinstrap\n\n[[3]]\n&lt;PredictionClassif&gt; for 111 observations:\n row_ids     truth response\n       2    Adelie   Adelie\n       5    Adelie   Adelie\n       8    Adelie   Adelie\n     ---       ---      ---\n     326 Chinstrap   Gentoo\n     329 Chinstrap   Gentoo\n     332 Chinstrap   Gentoo\n\nrr$score(msr(\"classif.acc\"))\n\n    task_id    learner_id resampling_id iteration classif.acc\n     &lt;char&gt;        &lt;char&gt;        &lt;char&gt;     &lt;int&gt;       &lt;num&gt;\n1: penguins classif.rpart     custom_cv         1   0.9369369\n2: penguins classif.rpart     custom_cv         2   0.9189189\n3: penguins classif.rpart     custom_cv         3   0.9369369\nHidden columns: task, learner, resampling, prediction_test\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9309309 \n\n\nSo, we get a model with \\(93.1\\%\\) accuracy, as required.4\n4 Would the results change much with, for example, grouped resampling? I should look at this at some point."
  },
  {
    "objectID": "posts/08-mlr3_evaluation_benchmarking/index.html#question-4",
    "href": "posts/08-mlr3_evaluation_benchmarking/index.html#question-4",
    "title": "Getting Started with {mlr3}",
    "section": "Question 4",
    "text": "Question 4\n(*) Program your own ROC plotting function without using mlr3’s autoplot() function.\n\nThe signature of your function should be my_roc_plot(task, learner, train_indices, test_indices).\nYour function should use the $set_threshold() method of Prediction, as well as mlr3measures.\n\n\nAnswer\nLet’s first have a look at the output from using autoplot(). I’ll use the german_credit task.\n\ntsk_german = tsk(\"german_credit\")\ntsk_german\n\n&lt;TaskClassif:german_credit&gt; (1000 x 21): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker,\n    housing, job, other_debtors, other_installment_plans,\n    people_liable, personal_status_sex, property, purpose, savings,\n    status, telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n\nlrn_ranger = lrn(\"classif.ranger\", predict_type = \"prob\")\nsplits = partition(tsk_german, ratio = 0.8)\n\nlrn_ranger$train(tsk_german, splits$train)\nprediction = lrn_ranger$predict(tsk_german, splits$test)\n\n\nautoplot(prediction, type = \"roc\")\n\n\n\n\n\n\n\n\nFirst, I’ll do all the steps to create the ROC, then I’ll wrap this in a function, my_roc_plot().\n\nCreating the ROC\nOK – so I need to use $set_threshold() to obtain predictions over the range of thresholds. Then, I need to use {mlr3measures} (Lang, Becker, and Koers 2024) to compute the TPR (Sensitivity) and FPR (\\(1 -\\) Specificity) and plot these all on a lovely graph.\n\nLang, Michel, Marc Becker, and Lona Koers. 2024. Mlr3measures: Performance Measures for ’Mlr3’. https://CRAN.R-project.org/package=mlr3measures.\n\n\nSensitivity\n\n(true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\n\nSpecificity\n\n(true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.\n\n\n\nI’ll first check to see which is the positive outcome in the Task.\n\ntsk_german$positive\n\n[1] \"good\"\n\n# also, by looking at the help file 'prediction$help()'\n# can see that the positive class is the first level of '$truth', i.e.\nlevels(prediction$truth)[1]\n\n[1] \"good\"\n\n\nSo having good credit is the positive outcome here.\nNow, I’ll create a vector of thresholds5 and then obtain predictions and calculate the measures.\n5 Thresholds were discussed in Section 2.5.4 of the mlr3 tutorial, and I looked at them in Question 3 of my previous post\npositive_class &lt;- levels(prediction$truth)[1]\nthresholds &lt;- seq(0, 1, length = 101)\ntsk_german_measures &lt;- rbindlist(\n         lapply(thresholds, function(j) {\n                prediction$set_threshold(j)\n                tpr_tmp &lt;- mlr3measures::tpr(truth = prediction$truth,\n                                             response = prediction$response,\n                                             positive = positive_class)\n                fpr_tmp &lt;- mlr3measures::fpr(truth = prediction$truth,\n                                             response = prediction$response,\n                                             positive = positive_class)\n                data.table(threshold = j,\n                           tpr = tpr_tmp,\n                           fpr = fpr_tmp)\n                    }\n                 )\n         )\n\n# order by increasing fpr, and tpr\n# s.t. the step function avoids spikes\n# spikes are happening as seed not set in $set_threshold(),\n# so possible to get non-monotonic tpr/ fpr\n# also put them in descending threshold order, just to make the data look nicer.\ntsk_german_measures &lt;- tsk_german_measures[order(fpr, tpr, -threshold)]\ntsk_german_measures\n\n     threshold         tpr   fpr\n         &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n  1:      1.00 0.000000000     0\n  2:      0.99 0.000000000     0\n  3:      0.98 0.006896552     0\n  4:      0.97 0.013793103     0\n  5:      0.96 0.013793103     0\n ---                            \n 97:      0.04 1.000000000     1\n 98:      0.03 1.000000000     1\n 99:      0.02 1.000000000     1\n100:      0.01 1.000000000     1\n101:      0.00 1.000000000     1\n\n\nOK, I think I’ve got everything required to plot the ROC.\n\nggplot(tsk_german_measures, aes(x = fpr, y = tpr)) +\n        geom_step() +\n        geom_abline(intercept = 0, slope = 1,\n                    linetype = \"dotted\", colour = \"grey\") +\n        labs(x = \"1 - Specificity\",\n             y = \"Sensitivity\") +\n        theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMaking the function my_roc_plot()\n\nmy_roc_plot &lt;- function(task, learner, train_indices, test_indices) {\n        # task: a 'Task' object\n        # learner: a 'Learner' object\n\n        # train the learner on the task\n        learner$train(task, row_ids = train_indices)\n        # create the prediction object\n        prediction &lt;- learner$predict(task, row_ids = test_indices)\n\n        # find TPR and FPR over a seq of thresholds\n        positive_class &lt;- levels(prediction$truth)[1]\n        thresholds &lt;- seq(0, 1, length = 101)\n        tpr_fpr_thresholds &lt;- rbindlist(\n                 lapply(thresholds, function(j) {\n                        prediction$set_threshold(j)\n                        tpr_tmp &lt;- mlr3measures::tpr(truth = prediction$truth,\n                                                     response = prediction$response,\n                                                     positive = positive_class)\n                        fpr_tmp &lt;- mlr3measures::fpr(truth = prediction$truth,\n                                                     response = prediction$response,\n                                                     positive = positive_class)\n                        data.table(threshold = j,\n                                   tpr = tpr_tmp,\n                                   fpr = fpr_tmp)\n                            }\n                         )\n                 )\n\n        tpr_fpr_thresholds &lt;- tpr_fpr_thresholds[order(fpr, tpr, -threshold)]\n\n        # and plot\n        ggplot(tpr_fpr_thresholds, aes(x = fpr, y = tpr)) +\n                geom_step() +\n                geom_abline(intercept = 0, slope = 1,\n                            linetype = \"dotted\", colour = \"grey\") +\n                labs(x = \"1 - Specificity\",\n                     y = \"Sensitivity\") +\n                theme_minimal()\n\n}\n\nLet’s test it:\n\nmy_roc_plot(task = tsk_german,\n            learner = lrn(\"classif.ranger\", predict_type = \"prob\"),\n            train_indices = splits$train,\n            test_indices = splits$test)\n\n\n\n\n\n\n\n\nCool, looks good!"
  },
  {
    "objectID": "posts/06-stepwise_datasplitting/index.html",
    "href": "posts/06-stepwise_datasplitting/index.html",
    "title": "Model Fitting and Validation",
    "section": "",
    "text": "This post is partly a expansion of a comment I made about training/test sets in a previous post. At work, we often need to fit a model and validate it (using internal validation), in the presence of missing data. This short article ignores the missing data issue (I will look at this later), but instead focuses on fitting and validating a model.\nThere are two main areas that I am looking at here, to try and improve current practice:\n\nFitting a model\nValidating the chosen model\n\nThis short article collates some of the critiques in the way we currently do model building and validation.\n\n\nCurrently, at work we use a stepwise procedure to chose explanatory variables in the model. This stepwise procedure is sometimes a mix of clinical judgement with some form of automated selection, and sometimes it is fully automated. This is often done on a training set which consists of around \\(70\\%\\) of the full data. The remaining \\(30\\%\\) of the data is the test set used for model validation.1\n1 For an example of a fully automated stepwise procedure on a \\(70\\%\\) training set, with validation on the remaining \\(30\\%\\) test set, see Collett, Friend, and Watson (2017).\nCollett, David, Peter J Friend, and Christopher JE Watson. 2017. “Factors Associated with Short-and Long-Term Liver Graft Survival in the United Kingdom: Development of a UK Donor Liver Index.” Transplantation 101 (4): 786–92.\nThere are lots of issues that result from this process: from the use of the stepwise procedure, and from not using all the data to fit the model (data-splitting)."
  },
  {
    "objectID": "posts/06-stepwise_datasplitting/index.html#current-practice",
    "href": "posts/06-stepwise_datasplitting/index.html#current-practice",
    "title": "Model Fitting and Validation",
    "section": "",
    "text": "Currently, at work we use a stepwise procedure to chose explanatory variables in the model. This stepwise procedure is sometimes a mix of clinical judgement with some form of automated selection, and sometimes it is fully automated. This is often done on a training set which consists of around \\(70\\%\\) of the full data. The remaining \\(30\\%\\) of the data is the test set used for model validation.1\n1 For an example of a fully automated stepwise procedure on a \\(70\\%\\) training set, with validation on the remaining \\(30\\%\\) test set, see Collett, Friend, and Watson (2017).\nCollett, David, Peter J Friend, and Christopher JE Watson. 2017. “Factors Associated with Short-and Long-Term Liver Graft Survival in the United Kingdom: Development of a UK Donor Liver Index.” Transplantation 101 (4): 786–92.\nThere are lots of issues that result from this process: from the use of the stepwise procedure, and from not using all the data to fit the model (data-splitting)."
  }
]