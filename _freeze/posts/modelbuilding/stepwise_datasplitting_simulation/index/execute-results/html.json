{
  "hash": "9e1909a1cb4909c6de02d4d69c58887b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Fitting and Validation\"\nsubtitle: \"Simulating the effect of data-splitting and stepwise selection on the `lung` dataset\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-03-15\"\ncategories: [code, r, statistics, model fitting, model validation]\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\n\nThis is *Part Two* of an four part series on model fitting and validation. Part One can be found [here](../stepwise_datasplitting/index.qmd).\n\n# Introduction\n\nThe aim of this article is to consider the effect of using data-splitting (to obtain a single training and test set) and stepwise selection (to obtain 'significant' factors) on the `lung` R dataset.\n\nIn general -- assuming we have data $X \\in \\mathbb{R}^{n \\times p}$ -- the process consists of the following steps:\n\n1. Randomly allocate the data to a *training* set, $X_\\text{train} \\in \\mathbb{R}^{n_1 \\times p}$, and a *test* set, $X_\\text{test} \\in \\mathbb{R}^{n_2 \\times p}$.\n    - Here, $n = n_1 + n_2$, where $n_1 \\approx 0.7 n$ and $n_2 \\approx 0.3 n$.\n2. Perform model building on the test set, using stepwise selection starting with a model that includes all factors, to obtain model $\\mathcal{M}$.\n    - Stepwise is performed by adding and removing a single factor at each step such that AIC is minimised. Here, $\\text{AIC} = 2k - 2 \\log(\\hat L)$, and $\\hat L$ is the maximised (partial) likelihood.\n3. Perform model validation on the test set, to obtain [Harrell’s C-index](https://statisticaloddsandends.wordpress.com/2019/10/26/what-is-harrells-c-index/), assessing the predictive performance of the model over all observed times.\n    - We are answering the question: *Among all pairs of individuals, what fraction had predicted risks that were correctly ranked in the same order as their actual observed outcomes (event times)?*\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nThere are many issues with this procedure, which I have discussed in\na [previous post](../stepwise_datasplitting/index.qmd). In this article\nI will perform the above steps 100 times and obtain the following:\n\n1. The amount of variability in the factors chosen in the model selection stage.\n    - In addition, the variability in the coefficients estimated for each factor.\n2. The amount of variability in the c-statistic obtained from validation on the test set.\n\n## Prerequisites and data cleaning\n\nThe `lung` dataset is from the `{survival}` package [@therneau2024survival], and I will use `{data.table}` [@barrett2024datatable] as I like the syntax and it's fast.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nlibrary(broom) # for 'tidy'\nlibrary(data.table)\noptions(datatable.print.nrows = 20) # set max # rows before truncating (default = 100)\nlibrary(ggplot2)\nlung <- as.data.table(lung)\nlung\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      inst  time status   age   sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n     <num> <num>  <num> <num> <num>   <num>    <num>     <num>    <num>   <num>\n  1:     3   306      2    74     1       1       90       100     1175      NA\n  2:     3   455      2    68     1       0       90        90     1225      15\n  3:     3  1010      1    56     1       0       90        90       NA      15\n  4:     5   210      2    57     1       1       90        60     1150      11\n  5:     1   883      2    60     1       0      100        90       NA       0\n ---                                                                           \n224:     1   188      1    77     1       1       80        60       NA       3\n225:    13   191      1    39     1       0       90        90     2350      -5\n226:    32   105      1    75     2       2       60        70     1025       5\n227:     6   174      1    66     1       1       90       100     1075       1\n228:    22   177      1    58     2       1       80        90     1060       0\n```\n\n\n:::\n:::\n\n\n\n\nThe `lung` dataset has status encoded in a non-standard way (as `1` for censored and `2` for dead). I will change this to `0` and `1` respectively. I will also ensure the data contains no missingness by using `na.omit()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#recode 'status' as 1 if event and 0 if censored\nlung_recode <- copy(lung)[, status := as.integer(status == 2)]\n\n# remove NA's (st 'ampute' works)\nlung_complete <- na.omit(lung_recode)\n\nlung_complete[, `:=`(sex = as.factor(sex), ph.ecog = as.factor(ph.ecog))]\n\nlung_complete\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      inst  time status   age    sex ph.ecog ph.karno pat.karno meal.cal\n     <num> <num>  <int> <num> <fctr>  <fctr>    <num>     <num>    <num>\n  1:     3   455      1    68      1       0       90        90     1225\n  2:     5   210      1    57      1       1       90        60     1150\n  3:    12  1022      0    74      1       1       50        80      513\n  4:     7   310      1    68      2       2       70        60      384\n  5:    11   361      1    71      2       2       60        80      538\n ---                                                                    \n163:    11   203      0    71      2       1       80        90     1025\n164:    13   191      0    39      1       0       90        90     2350\n165:    32   105      0    75      2       2       60        70     1025\n166:     6   174      0    66      1       1       90       100     1075\n167:    22   177      0    58      2       1       80        90     1060\n     wt.loss\n       <num>\n  1:      15\n  2:      11\n  3:       0\n  4:      10\n  5:       1\n ---        \n163:       0\n164:      -5\n165:       5\n166:       1\n167:       0\n```\n\n\n:::\n:::\n\n\n\n\n# The code\n\nHere, I'll do the following:\n\n- Randomly allocate $\\approx 70\\%$ of the data to the training set, and the remainder to the test set.\n- Use stepwise selection to choose 'significant' factors and estimate coefficients.\n- Obtain the C-index on the test set.\n\nFor reproducibility, I'll create a `data.table` where the rows consist of an\n`.id` variable -- relating to the simulation number -- and the row indices\nwhich will be chosen from `lung_complete` to make a training set for that\nsimulation. This is shown in @lst-training-set-indices.\n\n\n\n\n::: {.cell}\n\n```{#lst-training-set-indices .r .cell-code  lst-cap=\"Generating training set indices for data-splitting simulations\"}\nset.seed(1000)\n\nn_x <- nrow(lung_complete)\nn1 <- 0.7 # size of training set\n\ntraining_idx <- rbindlist(\n                  lapply(seq_len(n_sim), function(i) {\n                                idx <- sample(1:n_x, floor(n1 * n_x))\n                                data.frame(.id = i, as.list(idx))\n                                }\n                        ),\n                          use.names = FALSE\n                  )\n\n# change column names except '.id'\nold_names <- names(training_idx)\nnew_names <- c(\".id\", paste0(\"idx_\", seq_len(ncol(training_idx) - 1))) \nsetnames(training_idx, old = old_names, new = new_names)\n\ntraining_idx[1:5, 1:12]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     .id idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 idx_11\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>  <int>  <int>\n1:     1    68    43    51    88    29    99    61   146   150    102     45\n2:     2   163    99   121    62     5   151   166    41     2    116    146\n3:     3    40    52    26   101   133   102    71   119    13     32    166\n4:     4   130   155    51   142    48    76    72    38    42     85     91\n5:     5    37   126    87   166    59    93    14    84    74     49    110\n```\n\n\n:::\n:::\n\n\n\n\nNow, I'll build 100 Cox proportional hazard models, using a stepwise\nprocedure, on each training sample, and obtain the C-index on the respective test sample.\n\n\n\n\n\n::: {.cell}\n\n```{#lst-model-building-validation .r .cell-code  lst-cap=\"Fitting Cox PH models on the training samples, and obtaining the C-index on the respective test samples\"}\n# split data into training and test sets\n# and obtain cox coefs\nmodels <- training_idx[, {\n        tmp_train <- lung_complete[unlist(.SD)]\n        tmp_test <- lung_complete[-unlist(.SD)]\n        # fit the full model to the training data\n        cox_full_tmp <- coxph(Surv(time, status) ~ .,\n                              data = tmp_train)\n        # use stepwise selection on training data\n        cox_step_tmp <- step(cox_full_tmp,\n                             direction = \"both\",\n                             trace = 0)\n        # get linear predictor on test data\n        lp_tmp <- predict(cox_step_tmp, \n                          newdata = tmp_test, type = \"lp\")\n        # obtain c-statistic using linear predictor\n        # see callout note below for why `reverse = TRUE` is required\n        c_stat <- concordance(Surv(tmp_test$time, tmp_test$status) ~ lp_tmp, \n                              reverse = TRUE) \n        data.table(as.data.table(tidy(cox_step_tmp)),\n                         c = c_stat$concordance)\n        }, by = .id] \nmodels \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       .id     term    estimate  std.error statistic      p.value         c\n     <int>   <char>       <num>      <num>     <num>        <num>     <num>\n  1:     1     inst -0.02516878 0.01555328 -1.618230 1.056130e-01 0.5320829\n  2:     1     sex2 -0.74073698 0.24700260 -2.998904 2.709530e-03 0.5320829\n  3:     1 ph.ecog1  0.65422440 0.34366145  1.903689 5.695074e-02 0.5320829\n  4:     1 ph.ecog2  1.94984289 0.52954389  3.682118 2.313046e-04 0.5320829\n  5:     1 ph.ecog3  2.91211017 1.15977949  2.510917 1.204180e-02 0.5320829\n ---                                                                       \n629:   100     sex2 -0.49918121 0.24386054 -2.046995 4.065862e-02 0.5729730\n630:   100 ph.ecog1  0.90183824 0.31795471  2.836373 4.562908e-03 0.5729730\n631:   100 ph.ecog2  2.66165163 0.57943576  4.593523 4.358245e-06 0.5729730\n632:   100 ph.ecog3  3.61852274 1.17636760  3.076014 2.097883e-03 0.5729730\n633:   100 ph.karno  0.04106388 0.01400488  2.932112 3.366650e-03 0.5729730\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n# Calculating the C-statistic using `concordance()`\n\nThe `reverse = TRUE` argument in `concordance()` is important in\n@lst-model-building-validation: if TRUE then assume that larger x values\npredict smaller response values y; a proportional hazards model is the\ncommon example of this, larger hazard = shorter survival.\n\n:::\n\nThe `models` output from @lst-model-building-validation has 7 columns. The\n`.id` variable refers back to the training sample obtained from `training_idx`\nin @lst-training-set-indices, and the `term` variable gives the factors that\nhave been selected in the model relating to the `.id`.^[The `term` estimates\ncan contain `NA`'s if there is complete separation or zero events in the\nsplit.]\nThe `c` variable is the C-index obtained from using the `.id`$^{\\text{th}}$\nmodel to predict the test set events, so is identical for rows with the same\n`.id`'s.\n\n:::{.column-margin}\nAnnoyingly, it appears to not be possible to both label / caption listings\n(using `#| lst-label` and `#| lst-cap`) and use code annotations (via `# <1>`).\nI'm not sure why, but worth remember for the future.\n\nFor example, if the code block contains the lines,\n\n```r\n#| lst-cap: \"An example of when the hover annotation doesn't work\"\n#| lst-label: lst-no-hover\n```\n\nthen the output looks like this:\n\n\n\n\n::: {.cell}\n\n```{#lst-no-hover .r .cell-code  lst-cap=\"An example of when the hover annotation doesn't work\"}\nx <- rnorm(100) # <1>\n```\n:::\n\n\n\n1. This hovering code annotation doesn't work\n\n:::\n\n:::{.callout-note collapse = true}\n# Using the linear predictor to access predictive ability\n\nFitting a Cox proportional hazards model:\n\n$$\n\\lambda(t \\mid X) = \\lambda_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p),\n$$\n\n- $\\beta_1, \\beta_2, \\dots, \\beta_p$ are the estimated coefficients.  \n- Linear Predictor:\n  $$\n  \\text{LP}(X) = \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p.\n  $$\n- The hazard ratio between two individuals is determined by the difference\n  in their linear predictors. A higher linear predictor implies a higher hazard\n  (i.e., worse prognosis, or shorter survival on average).\n\nWhy the LP Is Enough to Rank Subjects\n\n- Harrell’s c-index is fundamentally about whether the model places the higher risk on the patient who actually experiences the event earlier.  \n- In a Cox model, risk is monotonic in $\\exp(\\text{LP})$. That is, if $\\text{LP}_A > \\text{LP}_B$, then $\\exp(\\text{LP}_A) > \\exp(\\text{LP}_B)$.  \n- So for pairwise comparisons, we only need to compare the linear predictor $\\text{LP}_A\\) vs. \\(\\text{LP}_B$.  \n- If $\\text{LP}_A > \\text{LP}_B$ but patient A actually survived longer, that pair is discordant. If A also died sooner, that pair is concordant.\n\n:::\n\n# Analysing the results\n\nUsing the models and coefficient estimates obtained in `models` (from @lst-model-building-validation), I'll look at a few things:\n\n1. How often each factor is chosen -- using the stepwise procedure -- to be in the 100 models.\n2. How many different models are chosen from the 100 stepwise selection procedures, and the proportion of times each model is selected.\n3. The variability in the estimated factor coefficients (if there are chosen in the model).\n4. The variability in the C-index obtained on the test set.\n\n## The model factors \n\nFirst, let's look at the proportion that each factor has been chosen to be in the 100 models.\n\n\n\n\n::: {.cell}\n\n```{#lst-factor-proportion .r .cell-code  lst-cap=\"Obtaining the proportion that each factor is chosen to be in the models.\"}\nwhich_variables <- models[, unique(term)]\n\nproportion_variables <- rbindlist(\n      lapply(1:length(which_variables), \n             function(i) {\n              prop_tmp <- nrow(models[term == which_variables[i]]) / n_sim\n              data.table(which_variables[i], prop_tmp)\n                    }\n             )\n      )\n\n# categorical factors in each model are present in the `models` dataset multiple times\n# e.g. 'ph.ecog1', 'ph.ecog2', etc.\n# create a \"expl_factor\" group by removing trailing digits\nproportion_variables[\n  , expl_factor := sub(\"[0-9]+$\", \"\", V1)\n]\n\n# collapse by the explanatory factor\ngrouped_proportions <- proportion_variables[\n  , .(prop = unique(prop_tmp)),\n  by = expl_factor\n]\n\ngrouped_proportions[order(prop, decreasing = TRUE)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   expl_factor  prop\n        <char> <num>\n1:     ph.ecog  0.94\n2:         sex  0.91\n3:        inst  0.72\n4:     wt.loss  0.68\n5:    ph.karno  0.64\n6:   pat.karno  0.30\n7:         age  0.22\n8:    meal.cal  0.04\n```\n\n\n:::\n:::\n\n\n\nNow I'll consider each model and the factors obtained using the stepwise\nprocedure. First -- in @lst-model-terms -- I'll create a `terms_string`\nvariable that gives a string containing all the terms in each of the 100\nmodels.\nThen -- in @lst-model-terms-groups -- I'll group these models together to see\nhow many different combination of factors have been chosen from the 100\nstepwise selection procedures.\n\n\n\n\n::: {.cell}\n\n```{#lst-model-terms .r .cell-code  lst-cap=\"Creating string of all the factors in each of the models.\"}\nmodel_variables <- models[, .(\n  terms_string = paste(\n    unique(sub(\"[0-9]+$\", \"\", term)),\n    collapse = \", \"\n  )\n), by = .id]\n\nmodel_variables\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       .id                                     terms_string\n     <int>                                           <char>\n  1:     1                     inst, sex, ph.ecog, ph.karno\n  2:     2            inst, sex, ph.ecog, ph.karno, wt.loss\n  3:     3                 sex, ph.ecog, pat.karno, wt.loss\n  4:     4       sex, ph.ecog, ph.karno, pat.karno, wt.loss\n  5:     5                inst, age, sex, ph.ecog, ph.karno\n ---                                                       \n 96:    96             age, sex, ph.ecog, ph.karno, wt.loss\n 97:    97            inst, sex, ph.ecog, ph.karno, wt.loss\n 98:    98 inst, sex, ph.ecog, ph.karno, pat.karno, wt.loss\n 99:    99                      age, sex, ph.ecog, ph.karno\n100:   100                           sex, ph.ecog, ph.karno\n```\n\n\n:::\n:::\n\n\n\n\nGrouping these models together gives the follow:\n\n\n\n\n::: {.cell df_print='paged'}\n\n```{#lst-model-terms-groups .r .cell-code  lst-cap=\"Grouping models together to show the number of different models obtained from using stepwise selection.\"}\nproportion_model_variables <- model_variables[\n      , .(prop = nrow(.SD) / nrow(model_variables))\n      , by = terms_string]\n\n#proportion_model_variables\nproportion_model_variables[order(prop, decreasing = TRUE)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                        terms_string  prop\n                                              <char> <num>\n 1:            inst, sex, ph.ecog, ph.karno, wt.loss  0.14\n 2:                      inst, sex, ph.ecog, wt.loss  0.13\n 3: inst, sex, ph.ecog, ph.karno, pat.karno, wt.loss  0.11\n 4:                     inst, sex, ph.ecog, ph.karno  0.06\n 5:                inst, age, sex, ph.ecog, ph.karno  0.04\n---                                                       \n30:           inst, sex, ph.ecog, pat.karno, wt.loss  0.01\n31:                                        pat.karno  0.01\n32:                     inst, sex, ph.ecog, meal.cal  0.01\n33:  inst, sex, ph.ecog, ph.karno, meal.cal, wt.loss  0.01\n34:             age, sex, ph.ecog, ph.karno, wt.loss  0.01\n```\n\n\n:::\n:::\n\n\n\n\nThat is, from 100 simulations of splitting the data into a training and\ntest set, I've obtained 34 different models!\n\n### The factor coefficients\n\nOK -- let's look at the variability of the coefficient estimates for each\nfactor. First, I'll get summary statistics for each factor coefficient.\nA box-plot showing this variability is given in @fig-coef-box, and the kernel\ndensities of the estimates are shown in @fig-coef-density.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove any rows that have NA in the estimate\nmodels_clean <- subset(models, !is.na(estimate))\n\nmodels_clean[, as.list(summary(estimate)), by = term]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term          Min.       1st Qu.        Median          Mean\n       <char>         <num>         <num>         <num>         <num>\n 1:      inst -0.0509354446 -0.0365704124 -0.0302755030 -0.0320112247\n 2:      sex2 -1.1315746564 -0.7029760419 -0.5881394025 -0.6093326104\n 3:  ph.ecog1  0.1420228124  0.5092835045  0.7024669149  0.7242113000\n 4:  ph.ecog2  0.8063634936  1.3599992736  1.7791183312  1.7739151854\n 5:  ph.ecog3  1.8659038585  2.5201583000  3.0779479537  3.0501194170\n 6:  ph.karno -0.0211069194  0.0252318873  0.0304998545  0.0305489979\n 7:   wt.loss -0.0292142915 -0.0210200718 -0.0186741004 -0.0187607396\n 8: pat.karno -0.0280318096 -0.0223547207 -0.0183404498 -0.0193037902\n 9:       age  0.0208960667  0.0220780244  0.0227820171  0.0242739341\n10:  meal.cal -0.0009556685 -0.0007011441 -0.0006010875 -0.0006590115\n          3rd Qu.          Max.\n            <num>         <num>\n 1: -0.0262214394 -0.0216371920\n 2: -0.5037040983 -0.3564017818\n 3:  0.9017954598  1.4072391610\n 4:  2.1107234044  2.8765227030\n 5:  3.5501307812  4.2490415152\n 6:  0.0372071507  0.0516899027\n 7: -0.0160832211 -0.0122667120\n 8: -0.0169197955 -0.0142537345\n 9:  0.0248300821  0.0391007125\n10: -0.0005589549 -0.0004782023\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(models_clean, aes(x=term, y=estimate)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(\n     axis.title = element_blank(),\n     # remove the grid lines\n     panel.grid.major = element_blank() ,\n     panel.grid.minor = element_blank() ,\n     # explicitly set the horizontal lines \n     panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n        )\n```\n\n::: {.cell-output-display}\n![Box plots showing the variability in the factor coefficient estimates.](index_files/figure-html/fig-coef-box-1.png){#fig-coef-box width=672}\n:::\n:::\n\n\n\n\nWe can see from @fig-coef-box (and by referring back to @lst-factor-proportion)\nthat the factors that are chosen the most times have the largest varibility in\ntheir coefficient estimates.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(models_clean, aes(x = estimate)) +\n  geom_density(fill = \"grey\", alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\") +\n  labs(\n    title = \"Density Plot of Factor Coefficients\",\n    x = \"Coefficient Estimate\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n     axis.text.y = element_blank(),\n     axis.title.y=element_blank(),\n     # remove the grid lines\n     panel.grid.major = element_blank() ,\n     panel.grid.minor = element_blank() ,\n     # explicitly set the horizontal lines \n     panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n  )\n```\n\n::: {.cell-output-display}\n![Density plots of the estimates for each factor coefficient.](index_files/figure-html/fig-coef-density-1.png){#fig-coef-density width=768}\n:::\n:::\n\n\n\n\n## The C-Index on the test sets\n\nFinally, let's look at how the C-index varies for each training/test split and\nrespective model. The code in @lst-model-c gives the C-index values obtained\nfrom the 100 models, and the histogram is given in @fig-hist-c.\n\n\n\n\n::: {.cell}\n\n```{#lst-model-c .r .cell-code  lst-cap=\"Code to output the C-index, in descending order.\"}\nc_stats <- models[, .(c_stats = unique(c)), by = .id]\nc_stats[order(c_stats, decreasing = TRUE)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       .id   c_stats\n     <int>     <num>\n  1:    52 0.7121514\n  2:     6 0.6835700\n  3:    25 0.6673729\n  4:    58 0.6621475\n  5:    50 0.6596509\n ---                \n 96:    53 0.5090000\n 97:     7 0.5019802\n 98:    18 0.4983819\n 99:    66 0.4790356\n100:     2 0.4610318\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(c_stats, aes(x = c_stats)) +\n        stat_bin(bins = 30) +\n        xlab(\"C-Index\") + ylab(\"\") +\n        theme_minimal() +\n        theme(\n           # remove the grid lines\n           panel.grid.major = element_blank() ,\n           panel.grid.minor = element_blank() ,\n           # explicitly set the horizontal lines \n           panel.grid.major.y = element_line(linewidth = .1, color = \"grey\" )\n        )\n```\n\n::: {.cell-output-display}\n![Histogram of the C-index values obtained on the test sets of the models.](index_files/figure-html/fig-hist-c-1.png){#fig-hist-c width=672}\n:::\n:::\n\n\n\n\nThese C-statistics varies quite a bit, from worse-than-random (a minimum of\n0.46) to *OK* (a maximum of 0.71). Some\nof this variability could be accounted for by:\n\n1.\tSome predictors might violate proportional hazards, hurting model performance.\n2.\tSmall test set size reduces the reliability of C-index estimates.\n\n# Conclusion\n\nTo sum up, I have shown by simulation some not-that-surprising results. These are:\n\n- Building a model on a single training-test split and then performing stepwise\n  selection creates a huge amount of variability, both in the model factors and\n  in their coefficient estimates.\n- The model validation step (performed on the test set) is not stable, and\n  results in very different results depending on the data-split, the model and\n  the coefficient estimates.\n\n**Next steps:** I will examine better methods for model building and validation in some future\nposts, but as a spoiler, they will include the use of the lasso, bootstrapping\nand cross-validation.  \n\n\n# Fin\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}