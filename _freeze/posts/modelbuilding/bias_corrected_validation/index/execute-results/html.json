{
  "hash": "048d953b9bdea69f50401e17c83e7637",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Validation using the Bootstrap\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-04-26\"\ncategories: [code, r, statistics, model fitting, model validation]\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\nThis is *Part Three* of a four part series on model fitting and validation.\n\n- [Part One](../stepwise_datasplitting/index.qmd) considers some theoretical issues with data-splitting and using stepwise selection\n- [Part Two](../stepwise_datasplitting_simulation/index.qmd) looks at simulating the effect of data-splitting and stepwise selection in model building and validation on the `lung` dataset, including:\n    - The variability of model terms chosen and the coefficient estimates from data-splitting and using stepwise selection;\n    - The variability in the C-statistic obtained from the test set.\n\n# Introduction\n\nIn this part, I look at using the bootstrap to validate a model. Much of this\nwork is inspired by the excellent book by @harrell2001regression. Using the\nbootstrap to validate a model means that data-splitting is not required, and\ninstead both model building and validation can be performed on the full\ndataset. As discussed by Harrell in\na [post](https://www.fharrell.com/post/split-val/),\n\n> Data are too precious to not be used in model development/parameter\nestimation. Resampling methods allow the data to be used for both development\nand validation, and they do a good job in estimating the likely future\nperformance of a model.\n\n:::{.callout-note collapse=\"true\"}\n# The hierarchy of validation methods\n\nIn @harrell2001regression[Chapter 5.2], a hierarchy of validation methods is\ngiven, from worst to best.^[Specifically, \"one suggested hierarchy\", so I'm\nunsure whether this is Harrell's suggested one or not...]\n\n\n1. Attempting several validations (internal or external) and reporting only the one that “worked”\n2. Reporting apparent performance on the training dataset (no validation)\n3. Reporting predictive accuracy on an undersized independent test sample\n4. Internal validation using data-splitting where at least one of the training\nand test samples is not huge and the investigator is not aware of the\narbitrariness of variable selection done on a single sample\n5. Strong internal validation using 100 repeats of 10-fold cross-validation or\n   several hundred bootstrap resamples, repeating all analysis steps involving\n   $Y$ afresh at each re-sample and the arbitrariness of selected “important\nvariables” is reported (if variable selection is used)\n6. External validation on a large test sample, done by the original research\nteam\n7. Re-analysis by an independent research team using strong internal validation of the original dataset\n8. External validation using new test data, done by an independent research\nteam\n9. External validation using new test data generated using different\n   instruments/technology, done by an independent research team\n\nIn this post, I am looking at option (5). Currently at work we often rely on (3) or (4).\n\n:::\n\n\n## The method\n\nI will only consider complete-case data for now, and will consider two different scenarios.\n\n1. Model fitting where we already \"know\" the factors in the model.\n2. Model building and fitting, where we use stepwise to select the factors\n   in the model.\n\n:::{.callout-note collapse=\"true\"}\n# Why stepwise?\n\nI am using stepwise here for simplicity. The lasso would be better (although it still [shares many of stepwise's flaws](../stepwise_datasplitting/index.qmd)). Using expert judgement to choose variables is much better, and means it is much harder to hide behind the computer.\n\nI'm waiting with baited breath for guidance from the [*STRengthening Analytical\nThinking for Observational Studies*\n(STRATOS)](https://stratos-initiative.org/group_2) initiative on variable\nselection techniques for multivariate analysis, but I'm not sure it's coming\nanytime soon [@sauerbrei2020state]. \n\n:::\n\nThe method for model validation in this article is based on @harrell2001regression[Chapter 5.3.5], which describes the process of calculating an optimism-corrected accuracy metric using bootstrap resampling as:\n\n> From the original $X$ and $Y$ in the sample of size $n$, draw a sample with replacement also of size $n$. Derive a model in the bootstrap sample and apply it without change to the original sample. The accuracy index from the bootstrap sample minus the index computed on the original sample is an estimate of optimism. This process is repeated for $100$ or so bootstrap replications to obtain an average optimism, which is subtracted from the final model fit’s apparent accuracy to obtain the overfitting-corrected estimate.\n\nHe goes on...\n\n> Note that bootstrapping validates the *process* that was used to fit the original model (as does cross-validation). It provides an estimate of the expected value of the optimism, which when subtracted from the original index, provides an estimate of the expected bias-corrected index. If stepwise variable selection is part of the bootstrap process (*as it must be if the final model is developed that way*), and not all resamples (samples with replacement or training samples in cross-validation) resulted in the same model (which is almost always the case), this internal validation process actually provides an unbiased estimate of the future performance of the process used to identify markers and scoring systems; it does not validate a single final model. But resampling does tend to provide good estimates of the future performance of the final model that was selected using the same procedure repeated in the resamples.\n\nA good introduction to model building and internal validation is given in @collins2024evaluation. See *Box 2* for a simple explanation of the bootstrapping internal validation method.\n\nI have clarified this procedure where appropriate in @sec-analysis.\n\n## The data\n\nThe examples will be performed on the complete-case of\n[`pbc`](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/pbc.html)\ndata from the `{survival}` package [@therneau2024survival].^[In my next post\nI'll be considering the full dataset and use multiple imputation to handle the\nmissing data.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\noptions(datatable.print.nrows = 20) # max # rows before truncating (default = 100)\nlibrary(survival)\nlibrary(rms) # for 'validate'\nlibrary(mice)\n\n# load pbc dataset\ndata(pbc)\npbc <- as.data.table(pbc)\npbc[, status := fifelse(status == 2, 1, 0)]\n\n## used in 'step()' below\n# creating upper and lower model for stepwise selection\n# create upper scope (everything except 'id' and outcome variables)\nupper_names <- paste(setdiff(names(pbc), c(\"time\", \"status\", \"id\")), \n                             collapse = \" + \")\nscope <- list(\n          upper = reformulate(upper_names),\n          lower = ~1\n        )\n\n# complete case\npbc_complete <- na.omit(pbc)\n```\n:::\n\n\n\nThis is a dataset of patients with primary biliary cirrhosis,\nand has 276 observations (compared to 418\nobservations in the full `pbc` dataset).\nThroughout this post I will use $B = 100$ bootstrap iterations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of bootstrap iterations\nB <- 100\n```\n:::\n\n\n\n# Analysis {#sec-analysis}\n\n\n| Notation | Definition |\n|----------|------------|\n| $X \\in \\mathbb{R}^{n \\times p}$ | dataset containing missingness (`pbc`) |\n| $\\tilde{X} \\in \\mathbb{R}^{n \\times \\tilde{p}}$ | complete-case dataset (`pbc_complete`) |\n| $X^{(i)}$ (or $\\tilde{X}^{(i)}$) | $i$-th bootstrap dataset (or complete-case dataset), $i = 1,\\dots,B$ |\n\n## Choosing factors manually\n\nIn this section, I consider building a model and validating it, when the factors are chosen in the model *a-priori*.\n\n### Model building {#sec-manual-building}\n\n$$\n\\begin{align}\n  \\text{Data:}\\quad&\\tilde{X}\\\\\n  &\\\\\n  &\\downarrow \\quad \\color{gray}{\\text{Factors chosen a-priori}}\\\\\n  &\\\\\n  \\text{Model:}\\quad&\\mathcal{M}\n\\end{align}\n$$\n\n#### The example\n\nI will build a model using the following factors: `age`, `ascites`, `edema`, `albumin`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a Cox PH model to pbc\ncox1 <- coxph(Surv(time, status) ~ age + ascites + edema + albumin, \n                  data = pbc_complete)\nsummary(cox1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\ncoxph(formula = Surv(time, status) ~ age + ascites + edema + \n    albumin, data = pbc_complete)\n\n  n= 276, number of events= 111 \n\n             coef exp(coef)  se(coef)      z Pr(>|z|)    \nage      0.029756  1.030203  0.009434  3.154 0.001610 ** \nascites  0.795809  2.216234  0.337631  2.357 0.018421 *  \nedema    1.603254  4.969174  0.323922  4.950 7.44e-07 ***\nalbumin -1.041877  0.352792  0.274523 -3.795 0.000148 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage        1.0302     0.9707     1.011    1.0494\nascites    2.2162     0.4512     1.143    4.2954\nedema      4.9692     0.2012     2.634    9.3758\nalbumin    0.3528     2.8345     0.206    0.6042\n\nConcordance= 0.757  (se = 0.027 )\nLikelihood ratio test= 95.51  on 4 df,   p=<2e-16\nWald test            = 116.5  on 4 df,   p=<2e-16\nScore (logrank) test = 172.7  on 4 df,   p=<2e-16\n```\n\n\n:::\n:::\n\n\n\n### Model validation\n\nFirst, $B$ bootstrap datasets are created, and models fitted to each of these:\n\n$$\n\\begin{align}\n  \\text{Data:}\\quad\\quad\\quad&\\tilde{X}&&&&\\\\\n  \\\\\n  &\\downarrow\\\\\n  \\\\\n  \\text{Bootstrapped data:}\\quad\\quad&\\tilde{X}^{(1)} \\quad &&\\tilde{X}^{(2)} \\quad &&\\ldots \\quad &&\\tilde{X}^{(B)}\\\\\n  \\\\\n  &\\downarrow && \\downarrow  &&\\ldots &&\\downarrow \\quad\\color{gray}{\\text{Same factors as original model building (chosen a-priori)}}\\\\\n  \\\\\n  \\text{Models:}\\quad\\quad &\\mathcal{M}^{(1)} \\quad &&\\mathcal{M}^{(2)} \\quad &&\\ldots \\quad &&\\mathcal{M}^{(B)}\\\\\n\\end{align}\n$$\n\nThen, calculating the optimism-corrected C-statistic involves:\n\n1. Calculate the C-statistic obtained from the original model $\\mathcal{M}$ (from @sec-manual-building), fitted to the complete-case dataset $\\tilde{X}$, denoted $c$.\n2. For $b = 1, 2, \\ldots, B$:\n    i. Calculate the C-statistic from the $b^\\text{th}$ bootstrap model $\\mathcal{M}^{(b)}$, fitted to the $b^\\text{th}$ bootstrap dataset $\\tilde{X}^{(b)}$, denoted $c^{(b)}$.\n    ii. Calculate the C-statistic from the $b^\\text{th}$ bootstrap model $\\mathcal{M}^{(b)}$, fitted to the original dataset, $\\tilde{X}$, denoted $c^{(b)}_X$.\n    iii. Calculate the optimism for the $b^\\text{th}$ bootstrap model $o^{(b)} = c^{(b)} - c^{(b)}_X$.\n3. Calculate the mean optimism, $\\bar{o} = \\frac{1}{B} \\sum_{b=1}^{B} o^{(b)}$.\n4. Calculate the optimism-corrected C-statistic,\n$$\n  c_\\text{opt} = c - \\bar{o}.\n$$\n\n#### The example\n\nFirst, I obtain the (optimistic) C-statistic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc1 <- concordance(cox1)$concordance\n```\n:::\n\n\n\nThen, we follow Steps (2) - (4) above to obtain the optimism-corrected C-statistic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of bootstrap samples\nB <- 100\n\n\nset.seed(10)\n\nc_boots1 <- rbindlist(\n  lapply(seq_len(B), function(b) {\n      \n      # 2a. create bootstrap sample\n      idx <- sample(nrow(pbc_complete), replace = TRUE)\n      pbc_boot <- pbc_complete[idx]\n      \n      # 2b. fit model\n      cox_boot <- coxph(Surv(time, status) ~ age + ascites + edema + albumin, \n                        data = pbc_boot)\n\n      # 2c. \n      # calculate concordance on bootstrap sample\n      c_stat_boot <- concordance(cox_boot)\n      \n      # 2d.\n      # predict on original data\n      lp_X <- predict(cox_boot, newdata = pbc_complete, type = \"lp\")\n      # calculate concordance on original data\n      c_stat_X <- concordance(Surv(time, status) ~ lp_X, \n                              data = pbc_complete, reverse = TRUE)\n      c_boot <- c_stat_boot$concordance\n      c_x <- c_stat_X$concordance\n      \n      # save c stats and calculate optimism\n      data.table(c_boot = c_boot, c_x = c_x, optimism = c_boot - c_x)\n    }\n  )\n)\n\nprint(c_boots1, topn = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        c_boot       c_x     optimism\n         <num>     <num>        <num>\n  1: 0.7414303 0.7518621 -0.010431756\n  2: 0.7249516 0.7552477 -0.030296102\n  3: 0.7277843 0.7552477 -0.027463354\n  4: 0.7299828 0.7534767 -0.023493912\n  5: 0.7489701 0.7543622 -0.005392114\n ---                                 \n 96: 0.7580732 0.7539455  0.004127666\n 97: 0.7296153 0.7548831 -0.025267743\n 98: 0.8220811 0.7535809  0.068500184\n 99: 0.7695147 0.7469660  0.022548752\n100: 0.7220383 0.7516537 -0.029615478\n```\n\n\n:::\n:::\n\n\n\nNow, we can compute the mean optimism, $\\bar{o}$, and the overfitting-corrected estimate of accuracy.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3. mean difference\noptimism_mean1 <- mean(c_boots1$optimism)\n\n# 4. overfitting-corrected estimate of accuracy\nc_opt1 <- c1 - optimism_mean1\n```\n:::\n\n\nSo, the original C-statistic was 0.7565, compared to the bias-corrected estimate of 0.7547.\n\n:::{.callout-note}\n# The quick code\n\nThis can be done in a few lines lines, using the `{rms}` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# validate\nset.seed(10)\ncox1_cph <- rms::cph(Surv(time, status) ~ age + ascites + edema + albumin, \n                         data = pbc_complete, x = TRUE, y = TRUE) # <1>\nbias_corrected_metrics1 <- rms::validate(cox1_cph, B = 100)\nbias_corrected_metrics1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      index.orig training   test optimism index.corrected   n\nDxy       0.5131   0.5072 0.5035   0.0037          0.5094 100\nR2        0.2981   0.3038 0.2865   0.0173          0.2807 100\nSlope     1.0000   1.0000 0.9359   0.0641          0.9359 100\nD         0.0859   0.0892 0.0819   0.0073          0.0786 100\nU        -0.0018  -0.0018 0.0025  -0.0044          0.0025 100\nQ         0.0877   0.0910 0.0794   0.0116          0.0761 100\ng         0.9916   1.0087 0.9469   0.0619          0.9298 100\n```\n\n\n:::\n:::\n\n\n1. `rms::validate()` requires the model to be fit using `rms::cph()` instead of `survival::coxph()`.\n\nFor a detailed explanation of the `validate` output, see John Haman's\n[website](https://randomeffect.net/post/2021/05/02/the-rms-validate-function/).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc1_rms <- (bias_corrected_metrics1[\"Dxy\", \"index.corrected\"] + 1) / 2\nc1_rms\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7546952\n```\n\n\n:::\n:::\n\n\n:::\n\n## Analysis Using Stepwise Selection\n\nThe model building stage (@sec-complete-building) is done using stepwise selection, and the model validation stage (@sec-complete-validation) is done using bootstrapping. The model building and validation stages are shown below.\n\n\n### Model building {#sec-complete-building}\n\nHere, I perform stepwise selection on the complete-case dataset, starting from the null model.\n\n$$\n\\begin{align}\n  \\text{Data:}\\quad&\\tilde{X}\\\\\n  &\\\\\n  &\\downarrow \\quad \\color{gray}{\\text{Stepwise selection}}\\\\\n  &\\\\\n  \\text{Model:}\\quad&\\mathcal{M}\n\\end{align}\n$$\n\n#### The example {#sec-analysis-stepwise-building-example}\n\n:::{.panel-tabset}\n\n## The code\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# fit a Cox PH model to pbc: start with the null model\ncox_null <- coxph(Surv(time, status) ~ 1, data = pbc_complete)\n\n# use stepwise selection (minimising AIC)\n# creating upper and lower model for stepwise selection\n# create upper scope (everything except 'id' and outcome variables)\nupper_names <- paste(setdiff(names(pbc), c(\"time\", \"status\", \"id\")), \n                             collapse = \" + \")\nscope <- list(\n          upper = reformulate(upper_names),\n          lower = ~1\n        )\n\ncox2 <- step(cox_null, scope = scope, trace = 0)\n```\n:::\n\n\n\n## The output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# look at the model\nsummary(cox2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\ncoxph(formula = Surv(time, status) ~ bili + stage + copper + \n    albumin + protime + age + edema + ast, data = pbc_complete)\n\n  n= 276, number of events= 111 \n\n              coef  exp(coef)   se(coef)      z Pr(>|z|)    \nbili     0.0851214  1.0888492  0.0193352  4.402 1.07e-05 ***\nstage    0.4327939  1.5415584  0.1456307  2.972  0.00296 ** \ncopper   0.0028535  1.0028576  0.0009832  2.902  0.00370 ** \nalbumin -0.7185954  0.4874364  0.2724486 -2.638  0.00835 ** \nprotime  0.2275175  1.2554794  0.1013729  2.244  0.02481 *  \nage      0.0313836  1.0318812  0.0102036  3.076  0.00210 ** \nedema    0.8217952  2.2745795  0.3471465  2.367  0.01792 *  \nast      0.0043769  1.0043865  0.0018067  2.423  0.01541 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nbili       1.0888     0.9184    1.0484    1.1309\nstage      1.5416     0.6487    1.1588    2.0508\ncopper     1.0029     0.9972    1.0009    1.0048\nalbumin    0.4874     2.0515    0.2858    0.8314\nprotime    1.2555     0.7965    1.0292    1.5314\nage        1.0319     0.9691    1.0114    1.0527\nedema      2.2746     0.4396    1.1519    4.4915\nast        1.0044     0.9956    1.0008    1.0079\n\nConcordance= 0.845  (se = 0.019 )\nLikelihood ratio test= 163.8  on 8 df,   p=<2e-16\nWald test            = 176.1  on 8 df,   p=<2e-16\nScore (logrank) test = 257.5  on 8 df,   p=<2e-16\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Model validation {#sec-complete-validation}\n\nThe model validation stage is done using bootstrapping. The bootstrap procedure -- starting from the complete-case dataset $\\tilde{X}$ (which is the same as used in the model building stage) -- is as follows.\n\nFirst, $B$ bootstrap datasets are created, and models fitted to each of these (using the same stepwise procedure as in @sec-complete-building):\n\n$$\n\\begin{align}\n  \\text{Data:}\\quad\\quad\\quad&\\tilde{X}&&&&\\\\\n  \\\\\n  &\\downarrow\\\\\n  \\\\\n  \\text{Bootstrapped data:}\\quad\\quad&\\tilde{X}^{(1)} \\quad &&\\tilde{X}^{(2)} \\quad &&\\ldots \\quad &&\\tilde{X}^{(B)}\\\\\n  \\\\\n  &\\downarrow && \\downarrow  &&\\ldots &&\\downarrow \\quad\\color{gray}{\\text{Stepwise selection}}\\\\\n  \\\\\n  \\text{Models:}\\quad\\quad &\\mathcal{M}^{(1)} \\quad &&\\mathcal{M}^{(2)} \\quad &&\\ldots \\quad &&\\mathcal{M}^{(B)}\\\\\n\\end{align}\n$$\n\nThen, calculating the optimism-corrected C-statistic involves:\n\n1. Calculate the C-statistic obtained from the original model $\\mathcal{M}$ (from @sec-complete-building), fitted to the complete-case dataset $\\tilde{X}$, denoted $c$.\n2. For $b = 1, 2, \\ldots, B$:\n    i. Calculate the C-statistic from the $b^\\text{th}$ bootstrap model $\\mathcal{M}^{(b)}$, fitted to the $b^\\text{th}$ bootstrap dataset $\\tilde{X}^{(b)}$, denoted $c^{(b)}$.\n    ii. Calculate the C-statistic from the $b^\\text{th}$ bootstrap model $\\mathcal{M}^{(b)}$, fitted to the original dataset, $\\tilde{X}$, denoted $c^{(b)}_X$.\n    iii. Calculate the optimism for the $b^\\text{th}$ bootstrap model $o^{(b)} = c^{(b)} - c^{(b)}_X$.\n3. Calculate the mean optimism, $\\bar{o} = \\frac{1}{B} \\sum_{b=1}^{B} o^{(b)}$.\n4. Calculate the optimism-corrected C-statistic,\n$$\n  c_\\text{opt} = c - \\bar{o}.\n$$\n\n#### The example\n\nFirst, I obtain the (optimistic) C-statistic^[The C-statistic obtained from the original model $\\mathcal{M}$ (from @sec-complete-building), fitted to the complete-case dataset $\\tilde{X}$.], as described in Step (1).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc <- concordance(cox2)$concordance\n```\n:::\n\n\n\nThen, follow Step (2) to obtain the C-statistic from the $b^\\text{th}$ bootstrap model $\\mathcal{M}^{(b)}$, fitted to both the bootstrap dataset,$\\tilde{X}^{(b)}$, and the original dataset, $\\tilde{X}$. From these the $b^\\text{th}$ bootstrap model optimism can be calculated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\n\n# start time of execution\ntotal_time_start <- Sys.time()\n      \nc_boots <- rbindlist(\n  lapply(seq_len(B), function(b) {\n      \n      # start time of inner execution\n      start_time <- Sys.time()\n\n      # 2a. create bootstrap sample\n      idx <- sample(nrow(pbc_complete), replace = TRUE)\n      pbc_boot <- pbc_complete[idx]\n      \n      # 2b. fit null model\n      cox_boot_null <- coxph(Surv(time, status) ~ 1, data = pbc_boot)\n      # use stepwise selection (minimising AIC)\n      cox_boot <- step(cox_boot_null, scope = scope, trace = 0)\n\n      # 2c. \n      c_boot <- concordance(cox_boot)$concordance\n      \n      # 2d.\n      # predict on original data\n      lp_X <- predict(cox_boot, newdata = pbc_complete, type = \"lp\")\n      # calculate concordance on original data\n      c_x <- concordance(Surv(time, status) ~ lp_X, \n                              data = pbc_complete, reverse = TRUE)$concordance\n      \n      # end time of inner execution\n      end_time <- Sys.time()\n      time_taken <- end_time - start_time\n      \n      # save c stats and calculate optimism\n      data.table(c_boot = c_boot, c_x = c_x, optimism = c_boot - c_x,\n                 time = time_taken)\n    }\n  )\n)\n\n# end time of inner execution\ntotal_time_end <- Sys.time()\ntotal_time_taken <- total_time_end - total_time_start\n\nprint(c_boots, topn = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        c_boot       c_x     optimism           time\n         <num>     <num>        <num>     <difftime>\n  1: 0.8538053 0.8443148  0.009490538 0.1987031 secs\n  2: 0.8293180 0.8343143 -0.004996320 0.2695990 secs\n  3: 0.8455517 0.8346268  0.010924944 0.2656732 secs\n  4: 0.8362208 0.8348872  0.001333566 0.3547299 secs\n  5: 0.8589161 0.8320225  0.026893555 0.1707869 secs\n ---                                                \n 96: 0.8501148 0.8251992  0.024915579 0.1238530 secs\n 97: 0.8402561 0.8210323  0.019223753 0.3700459 secs\n 98: 0.8800687 0.8248346  0.055234084 0.2577620 secs\n 99: 0.8755016 0.8236366  0.051864931 0.2591660 secs\n100: 0.8329462 0.8116048  0.021341406 0.2755342 secs\n```\n\n\n:::\n:::\n\n\n\nThis process took 23.74 secs in total.^[On my Macbook Air M3.]\n\nNow, I compute the mean optimism, $\\bar{o}$, and the overfitting-corrected estimate of accuracy.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# (3) mean optimism\noptimism_mean <- mean(c_boots$optimism)\n\n# (4) optimism-corrected estimate of accuracy\nc_opt <- c - optimism_mean\n```\n:::\n\n\n\nSo, the original C-statistic was $c =$ 0.8454, compared to the\nbias-corrected estimate of $c_\\text{opt}=$ 0.8235. The mean of the\noptimism was $\\bar{o} =$ 0.0218.\n\n:::{.callout-important collapse=\"true\"}\n# Using `rms::validate()`\n\nThe `validate()` function cannot be used here; it doesn't validate the *whole\nprocess*, as the stepwise procedure is not repeated. That is, `validate` assumes\nthe model is fixed for every bootstrap iteration. This means that the mean\noptimism will be smaller than the one obtained above, and therefore the\n'optimism-adjusted' C-statistic will be higher than it should be.\n\nThe model found from the stepwise procedure in @sec-analysis-stepwise-building-example is,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula(cox2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSurv(time, status) ~ bili + stage + copper + albumin + protime + \n    age + edema + ast\n<environment: 0x12251e7e0>\n```\n\n\n:::\n:::\n\n\n\nSo, assuming this model form is constant throughout: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# validate\nset.seed(10)\ncox2_cph <- rms::cph(formula(cox2),\n                         data = pbc_complete, x = TRUE, y = TRUE) # <1>\nbias_corrected_metrics2 <- rms::validate(cox2_cph, B = 100)\nbias_corrected_metrics2\nc2_rms <- (bias_corrected_metrics2[\"Dxy\", \"index.corrected\"] + 1) / 2\nc2_rms\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      index.orig training   test optimism index.corrected   n\nDxy       0.6907   0.6950 0.6791   0.0159          0.6748 100\nR2        0.4561   0.4700 0.4379   0.0321          0.4240 100\nSlope     1.0000   1.0000 0.8994   0.1006          0.8994 100\nD         0.1479   0.1561 0.1400   0.0161          0.1319 100\nU        -0.0018  -0.0018 0.0049  -0.0068          0.0049 100\nQ         0.1498   0.1579 0.1351   0.0228          0.1270 100\ng         1.4660   1.5596 1.3985   0.1611          1.3049 100\n[1] 0.8373867\n```\n\n\n:::\n:::\n\n\n:::\n\n# Summary\n\n**What I've done:** for both a predefined model and using a stepwise model building\nprocedure, I have calculated the apparent performance C-statistic, and then\nadjusted for optimism. For the predefined model, it is possible to calculate\nthe optimism easily using `rms::validate()`. However, when using a model building\nprocedure it is required to do this manually as the whole process needs to be\nincluded in each bootstrap iteration.\n\n**Next steps:** model building and validation in the presence of missing data,\nusing multiple imputation.\n\n\n\n\n# Fin\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}