{
  "hash": "d50defc45c7e9c40f5c015e77c30141f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with {mlr3}\"\nsubtitle: \"03 Hyperparameter Optimisation\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-03-21\"\ncategories: [code, r, machine learning, mlr3]\nimage: \"./fig/mlr3_logo.svg\"\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\n# Introduction\n\nI am attempting to learn how to use `{mlr3}` [@lang2019mlr3], by reading through the book\n[Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)\n[@bischl2024usingmlr3].\n\nIn this post, I am working through the exercises given in [Chapter\n4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html)\nof the book [@becker2024hyperparameter], which covers hyperparameter optimisation (HPO).\nThis includes the following:\n\n- **Q1:** Tunes `regr.ranger` on `mtcars` using random search and 3-fold CV\n- **Q2:** Evaluates tuned model with nested resampling (3-fold CV outer, holdout inner)\n- **Q3:** Benchmarks tuned XGBoost vs logistic regression on `spam` using Brier score\n        - Uses `AutoTuner`, predefined tuning spaces, and `benchmark()` for comparison\n\nMy previous posts cover:\n\n- [Part one](../05-mlr3_basic_modelling/index.qmd):\n    - Create a classification tree model to predict diabetes.\n    - Look at the confusion matrix and create measures without using {mlr3measures}.\n    - Change the thresholds in the model.\n- [Part two](../08-mlr3_evaluation_benchmarking/index.qmd):\n    - Repeated cross-validation resampling.\n    - Using a custom resampling strategy.\n    - Creating a function that produces a ROC curve.\n\n\n## Prerequisites\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: paradox\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(mlr3tuningspaces)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n```\n:::\n\n\n\nSuppress all messaging unless it's a warning:^[The packages in `{mlr3}` that\nmake use of optimization, i.e., `{mlr3tuning}` or `{mlr3select}`, use the\nlogger of their base package `{bbotk}`.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n\n# Exercises\n\n1. Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of `lrn(\"regr.ranger\")` on `tsk(\"mtcars\")`. Use a simple random search with 50 evaluations. Evaluate with a 3-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.\n2. Evaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold CV for the outer resampling.\n3. Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n4. (\\*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\n        a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.\n        b. Identify the best configuration.\n        c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).\n        d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.\n\n## Question 1\n\nTune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of\n`lrn(\"regr.ranger\")` on `tsk(\"mtcars\")`. Use a simple random search with 50\nevaluations. Evaluate with a 3-fold CV and the root mean squared error.\nVisualize the effects that each hyperparameter has on the performance via\nsimple marginal plots, which plot a single hyperparameter versus the\ncross-validated MSE.\n\n### Answer\n\nLet's load the task and look at the properties of the `mtry`,\n`sample.fraction`, and `num.trees` parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_mtcars <- tsk(\"mtcars\")\nlrn(\"regr.ranger\")$param_set$data[id %in% c(\"mtry\", \"sample.fraction\", \"num.trees\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                id    class lower upper levels nlevels is_bounded special_vals\n            <char>   <char> <num> <num> <list>   <num>     <lgcl>       <list>\n1:            mtry ParamInt     1   Inf [NULL]     Inf      FALSE    <list[1]>\n2:       num.trees ParamInt     1   Inf [NULL]     Inf      FALSE    <list[0]>\n3: sample.fraction ParamDbl     0     1 [NULL]     Inf       TRUE    <list[0]>\n          default storage_type                   tags\n           <list>       <char>                 <list>\n1: <NoDefault[0]>      integer                  train\n2:            500      integer train,predict,hotstart\n3: <NoDefault[0]>      numeric                  train\n```\n\n\n:::\n:::\n\n\n\nThe hyperparameters I'm looking at are:\n\n- `mtry`: number of variables considered at each tree split.\n- `num.trees`: number of trees in the forest.\n- `sample.fraction`: fraction of observations used to train each tree.\n\nNow I will set up the tuning of the `mtry`, `sample.fraction`, and `num.trees` hyperparameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner <- lrn(\"regr.ranger\",\n               mtry = to_tune(p_int(1, 10)),\n               num.trees = to_tune(20, 2000),\n               sample.fraction = to_tune(0.1, 1))\nlearner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerRegrRanger:regr.ranger>: Random Forest\n* Model: -\n* Parameters: mtry=<ObjectTuneToken>, num.threads=1,\n  num.trees=<RangeTuneToken>, sample.fraction=<RangeTuneToken>\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], se, quantiles\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, missings, oob_error,\n  selected_features, weights\n```\n\n\n:::\n:::\n\n\n\nSetting up an instance to terminate the tuner after 50 evaluations, and to use 3-fold CV.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstance <- ti(task = tsk_mtcars,\n               learner = learner,\n               resampling = rsmp(\"cv\", folds = 3),\n               # rmse gives interpretability in the original units (MPG) rather than squared units\n               measures = msr(\"regr.rmse\"), \n               terminator = trm(\"evals\", n_evals = 50))\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningInstanceBatchSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuningBatch:regr.ranger_on_mtcars>\n* Search Space:\n                id    class lower upper nlevels\n            <char>   <char> <num> <num>   <num>\n1:            mtry ParamInt   1.0    10      10\n2:       num.trees ParamInt  20.0  2000    1981\n3: sample.fraction ParamDbl   0.1     1     Inf\n* Terminator: <TerminatorEvals>\n```\n\n\n:::\n:::\n\n\n\nThe tuning step uses 3-fold cross-validation:\n\n- In each evaluation, two-thirds of the data is used for training,\n- One-third is used for validation (i.e. to compute the RMSE).\n- This is repeated for 50 random configurations (as specified by the terminator).\n\n\nNow I can set up the tuning process (random search).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuner <- tnr(\"random_search\")\n#tuner$param_set\ntuner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TunerBatchRandomSearch>: Random Search\n* Parameters: batch_size=1\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning, bbotk\n```\n\n\n:::\n:::\n\n\n\nAnd trigger the tuning process.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(333)\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   <int>     <int>           <num>             <list>    <list>     <num>\n1:     5       213       0.6735753          <list[4]> <list[3]>  2.960166\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   <int>     <int>           <num>             <list>    <list>     <num>\n1:     5       213       0.6735753          <list[4]> <list[3]>  2.960166\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$result$learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$num.threads\n[1] 1\n\n[[1]]$mtry\n[1] 5\n\n[[1]]$num.trees\n[1] 213\n\n[[1]]$sample.fraction\n[1] 0.6735753\n```\n\n\n:::\n:::\n\n\n\nAll 50 of the random search evaluations are stored in the `archive` slot of the\n`instance` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(mtry, sample.fraction, num.trees, regr.rmse)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mtry sample.fraction num.trees regr.rmse\n    <int>           <num>     <int>     <num>\n 1:     5       0.9761367       187  3.089494\n 2:     8       0.3760474      1227  3.486089\n 3:     4       0.9020999       959  3.073204\n 4:    10       0.8781863      1283  3.098450\n 5:     2       0.2718788       161  6.148185\n---                                          \n46:     9       0.2032898       720  6.219048\n47:     7       0.6087772        35  3.160166\n48:     4       0.1445110      1487  6.236493\n49:     6       0.7125770      1637  3.062291\n50:     3       0.1578512       302  6.245020\n```\n\n\n:::\n:::\n\n\n\nNow let's visualise the effect of each hyperparameter using marginal plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(instance, type = \"marginal\", cols_x = c(\"mtry\", \"sample.fraction\", \"num.trees\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n\nAs during the HPO stage, I used 3-fold CV, the model has not seen the full\ndata all at once. So, now I'll train the model using the optimised\nhyperparameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_ranger_tuned <- lrn(\"regr.ranger\")\nlrn_ranger_tuned$param_set$values = instance$result_learner_param_vals\nlrn_ranger_tuned$train(tsk_mtcars)$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, mtry = 5L, num.threads = 1L,      num.trees = 213L, sample.fraction = 0.673575255507603) \n\nType:                             Regression \nNumber of trees:                  213 \nSample size:                      32 \nNumber of independent variables:  10 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       5.829941 \nR squared (OOB):                  0.8395021 \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 1 (hyperparameter tuning with random search)\n\nI tuned the `regr.ranger` learner on the `mtcars` dataset, focusing on three hyperparameters:\n\n- `mtry`: number of variables considered at each split,\n- `num.trees`: number of trees in the forest,\n- `sample.fraction`: fraction of the dataset used for each tree.\n\n**Steps I took:**\n\n1. **Exploration**\n   I inspected the available parameters for the learner using `$param_set`.\n\n2. **Learner setup**\n   I defined the tuning ranges with `to_tune()`:\n   - `mtry` from 1 to 10,\n   - `num.trees` from 1 to 100,000,\n   - `sample.fraction` from 0.1 to 1.\n\n3. **Tuning instance**\n   I created a `TuningInstanceSingleCrit` using:\n   - the `mtcars` task,\n   - 3-fold cross-validation for resampling,\n   - root mean squared error (RMSE) as the evaluation metric,\n   - a limit of 50 evaluations using a random search strategy.\n\n4. **Running the tuner**\n   I used `tnr(\"random_search\")` and called `$optimize()` to run the search. I then extracted the best hyperparameter combination and the archive of evaluated configurations.\n\n5. **Visualising results**\n   I used marginal plots to visualise the effect of each hyperparameter on the cross-validated RMSE.\n\n6. **Training the final model**\n   I retrained the `regr.ranger` model on the full dataset using the best parameters found.\n\n:::\n\n## Question 2\n\nEvaluate the performance of the model created in Exercise 1 with nested\nresampling. Use a holdout validation for the inner resampling and a 3-fold\nCV for the outer resampling.\n\n### Answer\n\nOK, so here we need an outer and inner resampling strategy.\nThe outer resampling strategy will be a 3-fold CV, and the inner resampling\nstrategy will be a holdout validation.\n\n![An illustration of nested resampling. The large blocks represent 3-fold CV for the outer resampling for model evaluation and the small blocks represent 4-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}\n\n@fig-nested-resampling-example shows an example of a nest resampling strategy\n(with 3-fold CV on the outer and 4-fold CV on the inner nest). Here, we\nneed to do something slightly different as we are using the holdout resampling\nstrategy on the inner nest.\n\n1. Outer resampling start\n   - Perform 3-fold cross-validation on the full dataset.\n   - For each outer fold, split the data into:\n     - Training set (light blue blocks)\n     - Test set (dark blue block)\n\n2. Inner resampling\n   - Within each outer training set, perform holdout validation (assuming 70/30 training-test split).\n   - This inner split is used for tuning hyperparameters (not evaluation).\n\n3. HPO – Hyperparameter tuning\n   - Evaluate different hyperparameter combinations by fitting models on the inner training set and evaluating performance on the inner validation (holdout) set.\n   - Select the best hyperparameter configuration based on performance on the inner holdout set. \n\n4. Training\n   - Fit the model on the entire outer training set using the tuned hyperparameters.\n\n5. Evaluation\n   - Evaluate the trained model on the outer test set (unseen during tuning).\n\n6. Outer resampling repeats\n   - Repeat steps 2–5 for each of the 3 outer folds.\n\n7. Aggregation\n   - Average the 3 outer test performance scores.\n   - This gives an unbiased estimate of the model’s generalisation performance with tuning.\n\nI will use `AutoTuner` to do nested resampling, as that is what is done in\n[Section\n4.3.1](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)\nof the mlr3 tutorial.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create auto_tuner to resample a random forest \n# with 3-fold CV in outer resampling and \n# holdout validation in inner resampling\nat <- auto_tuner(\n         tuner = tnr(\"random_search\"), #<1>\n         learner = lrn(\"regr.ranger\",\n                       mtry = to_tune(1, 1e1),\n                       num.trees = to_tune(1, 1e5),\n                       sample.fraction = to_tune(0.1, 1)),\n         # inner resampling\n         resampling = rsmp(\"holdout\", ratio = 0.7),\n         measure = msr(\"regr.rmse\"),\n         terminator = trm(\"evals\", n_evals = 50)\n        )\n\n# resampling step\nrr <- resample(tsk(\"mtcars\"),\n               at, \n               # outer resampling\n               rsmp(\"cv\", folds = 3), \n               store_models = TRUE) #<2>\n\nrr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 3 resampling iterations\n task_id        learner_id resampling_id iteration  prediction_test warnings\n  mtcars regr.ranger.tuned            cv         1 <PredictionRegr>        0\n  mtcars regr.ranger.tuned            cv         2 <PredictionRegr>        0\n  mtcars regr.ranger.tuned            cv         3 <PredictionRegr>        0\n errors\n      0\n      0\n      0\n```\n\n\n:::\n:::\n\n\n1. The tuners and learners are the same as in the previous exercise, I'm just\n   defining them again here for clarity.\n2. Set `store_models = TRUE` so that the `AutoTuner` models (fitted on the\n   outer training data) are stored,\n\nNow I aggregate across the three outer folds to get the final performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n13.13362 \n```\n\n\n:::\n:::\n\n\n\nThe inner tuning results can also be accessed, returning the optimal configurations (across all outer folds) and the full tuning archives.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# optimal configurations\nextract_inner_tuning_results(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   iteration  mtry num.trees sample.fraction regr.rmse learner_param_vals\n       <int> <int>     <int>           <num>     <num>             <list>\n1:         1     1     81784       0.6769030  2.884958          <list[4]>\n2:         2     5     73924       0.9194928  2.065022          <list[4]>\n3:         3     3     15719       0.7797092  1.807826          <list[4]>\n    x_domain task_id        learner_id resampling_id\n      <list>  <char>            <char>        <char>\n1: <list[3]>  mtcars regr.ranger.tuned            cv\n2: <list[3]>  mtcars regr.ranger.tuned            cv\n3: <list[3]>  mtcars regr.ranger.tuned            cv\n```\n\n\n:::\n\n```{.r .cell-code}\n# full tuning archives\nextract_inner_tuning_archives(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     iteration  mtry num.trees sample.fraction regr.rmse x_domain_mtry\n         <int> <int>     <int>           <num>     <num>         <int>\n  1:         1     2     15547       0.3219244  5.115809             2\n  2:         1     8     23468       0.3020848  5.113502             8\n  3:         1     5     47564       0.3500007  5.115646             5\n  4:         1     9     14922       0.8105999  3.261807             9\n  5:         1     2     81447       0.1308460  5.124990             2\n ---                                                                  \n146:         3     5     20556       0.5608060  2.081965             5\n147:         3     2      9179       0.8461799  1.867438             2\n148:         3     4     93847       0.1486419  3.733662             4\n149:         3     3     15719       0.7797092  1.807826             3\n150:         3     1     31967       0.2513883  3.739324             1\n     x_domain_num.trees x_domain_sample.fraction runtime_learners\n                  <int>                    <num>            <num>\n  1:              15547                0.3219244            0.066\n  2:              23468                0.3020848            0.099\n  3:              47564                0.3500007            0.204\n  4:              14922                0.8105999            0.116\n  5:              81447                0.1308460            0.392\n ---                                                             \n146:              20556                0.5608060            0.133\n147:               9179                0.8461799            0.057\n148:              93847                0.1486419            0.430\n149:              15719                0.7797092            0.100\n150:              31967                0.2513883            0.126\n               timestamp warnings errors batch_nr  resample_result task_id\n                  <POSc>    <int>  <int>    <int>           <list>  <char>\n  1: 2025-07-27 15:47:27        0      0        1 <ResampleResult>  mtcars\n  2: 2025-07-27 15:47:27        0      0        2 <ResampleResult>  mtcars\n  3: 2025-07-27 15:47:28        0      0        3 <ResampleResult>  mtcars\n  4: 2025-07-27 15:47:28        0      0        4 <ResampleResult>  mtcars\n  5: 2025-07-27 15:47:28        0      0        5 <ResampleResult>  mtcars\n ---                                                                      \n146: 2025-07-27 15:48:10        0      0       46 <ResampleResult>  mtcars\n147: 2025-07-27 15:48:10        0      0       47 <ResampleResult>  mtcars\n148: 2025-07-27 15:48:10        0      0       48 <ResampleResult>  mtcars\n149: 2025-07-27 15:48:11        0      0       49 <ResampleResult>  mtcars\n150: 2025-07-27 15:48:11        0      0       50 <ResampleResult>  mtcars\n            learner_id resampling_id\n                <char>        <char>\n  1: regr.ranger.tuned            cv\n  2: regr.ranger.tuned            cv\n  3: regr.ranger.tuned            cv\n  4: regr.ranger.tuned            cv\n  5: regr.ranger.tuned            cv\n ---                                \n146: regr.ranger.tuned            cv\n147: regr.ranger.tuned            cv\n148: regr.ranger.tuned            cv\n149: regr.ranger.tuned            cv\n150: regr.ranger.tuned            cv\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 2 (nested resampling)\n\nI evaluated the performance of the tuned model using nested resampling to obtain an unbiased estimate.\n\n**Steps I took:**\n\n1. **Resampling strategy**\n   I used:\n   - outer resampling: 3-fold cross-validation,\n   - inner resampling: holdout validation with a 70/30 split.\n\n2. **AutoTuner setup**\n   I reused the same `regr.ranger` learner and parameter ranges as in Question 1, wrapped in an `AutoTuner`. The tuning again used 50 evaluations of random search and MSE as the measure.\n\n3. **Resample execution**\n   I called `resample()` with the outer CV and the `AutoTuner`, setting `store_models = TRUE` to keep the fitted models from each outer fold.\n\n4. **Aggregating performance**\n   I used `$aggregate()` to average the MSE across the outer test folds.\n\n5. **Inspecting inner results**\n   I extracted the best configurations and full tuning logs from each inner loop using `extract_inner_tuning_results()` and `extract_inner_tuning_archives()`.\n\n:::\n\n## Question 3\n\nTune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n\n### Answer\n\nI’ll use the built-in `spam` task -- since the outcome is categorical, this is a classification task.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_spam <- tsk(\"spam\")\n```\n:::\n\n\n\nFirst I'll set up the logistic regression model (with no tuning).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# requires probs to compute the brier score\nlrn_logreg <- lrn(\"classif.log_reg\", predict_type = \"prob\")\n```\n:::\n\n\n\n:::{.column-margin}\n\nThe XGBoost model has lots of hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn(\"classif.xgboost\")$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"alpha\"                       \"approxcontrib\"              \n [3] \"base_score\"                  \"booster\"                    \n [5] \"callbacks\"                   \"colsample_bylevel\"          \n [7] \"colsample_bynode\"            \"colsample_bytree\"           \n [9] \"device\"                      \"disable_default_eval_metric\"\n[11] \"early_stopping_rounds\"       \"eta\"                        \n[13] \"eval_metric\"                 \"feature_selector\"           \n[15] \"gamma\"                       \"grow_policy\"                \n[17] \"interaction_constraints\"     \"iterationrange\"             \n[19] \"lambda\"                      \"lambda_bias\"                \n[21] \"max_bin\"                     \"max_delta_step\"             \n[23] \"max_depth\"                   \"max_leaves\"                 \n[25] \"maximize\"                    \"min_child_weight\"           \n[27] \"missing\"                     \"monotone_constraints\"       \n[29] \"nrounds\"                     \"normalize_type\"             \n[31] \"nthread\"                     \"ntreelimit\"                 \n[33] \"num_parallel_tree\"           \"objective\"                  \n[35] \"one_drop\"                    \"outputmargin\"               \n[37] \"predcontrib\"                 \"predinteraction\"            \n[39] \"predleaf\"                    \"print_every_n\"              \n[41] \"process_type\"                \"rate_drop\"                  \n[43] \"refresh_leaf\"                \"reshape\"                    \n[45] \"seed_per_iteration\"          \"sampling_method\"            \n[47] \"sample_type\"                 \"save_name\"                  \n[49] \"save_period\"                 \"scale_pos_weight\"           \n[51] \"skip_drop\"                   \"strict_shape\"               \n[53] \"subsample\"                   \"top_k\"                      \n[55] \"training\"                    \"tree_method\"                \n[57] \"tweedie_variance_power\"      \"updater\"                    \n[59] \"verbose\"                     \"watchlist\"                  \n[61] \"xgb_model\"                  \n```\n\n\n:::\n:::\n\n\n\nThe main ones are:\n\n| Hyperparameter         | Description                                               | Type     |\n|------------------------|-----------------------------------------------------------|----------|\n| `eta`                  | Learning rate (shrinkage)                                 | numeric  |\n| `max_depth`            | Maximum depth of trees                                    | integer  |\n| `nrounds`              | Number of boosting rounds (trees)                         | integer  |\n| `colsample_bytree`     | Fraction of features randomly sampled per tree            | numeric  |\n| `subsample`            | Fraction of rows sampled per tree                         | numeric  |\n| `min_child_weight`     | Minimum sum of instance weights in a child node           | numeric  |\n| `gamma`                | Minimum loss reduction to make a split                    | numeric  |\n| `lambda`               | L2 regularisation term on weights                         | numeric  |\n| `alpha`                | L1 regularisation term on weights                         | numeric  |\n\nA typical tuning strategy for XGBoost might involve:\n\n1. Starting with basic tree shape and learning rate:\n   - `max_depth`\n   - `eta`\n   - `nrounds`\n\n2. Adding sampling and regularisation to control overfitting:\n   - `subsample`\n   - `colsample_bytree`\n   - `min_child_weight`\n   - `gamma`\n\n3. Fine-tuning regularisation terms if needed:\n   - `lambda` (L2)\n   - `alpha` (L1)\n\n:::\n\nFor the XGBoost learner, I'm going to use a predefined search space from\n`{mlr3tuningspaces}`. First, I'll give a list of these predefined spaces.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_tuning_spaces$keys()[grepl(\"xgboost\", mlr_tuning_spaces$keys())]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"classif.xgboost.default\" \"classif.xgboost.rbv1\"   \n[3] \"classif.xgboost.rbv2\"    \"regr.xgboost.default\"   \n[5] \"regr.xgboost.rbv1\"       \"regr.xgboost.rbv2\"      \n```\n\n\n:::\n:::\n\n\n\nI will use the `classif.xgboost.default` space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspace = lts(\"classif.xgboost.default\")\nspace\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningSpace:classif.xgboost.default>: Classification XGBoost with Default\n                  id lower upper levels logscale\n              <char> <num> <num> <list>   <lgcl>\n1:               eta 1e-04     1 [NULL]     TRUE\n2:           nrounds 1e+00  5000 [NULL]    FALSE\n3:         max_depth 1e+00    20 [NULL]    FALSE\n4:  colsample_bytree 1e-01     1 [NULL]    FALSE\n5: colsample_bylevel 1e-01     1 [NULL]    FALSE\n6:            lambda 1e-03  1000 [NULL]     TRUE\n7:             alpha 1e-03  1000 [NULL]     TRUE\n8:         subsample 1e-01     1 [NULL]    FALSE\n```\n\n\n:::\n:::\n\n\n\nPlugging this into `auto_tuner()` creates an `AutoTuner` object. I'm going to use\n5-fold CV in the inner resampling and a terminator based on run time (of 60\nseconds).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create terminator with time budget of 60 secs\ntrm_rt = trm(\"run_time\")\ntrm_rt$param_set$values$secs = 60\n\n# create xgboost learner with prob predict_type\n# 'prob' required for brier score\nlrn_xgb = lrn(\"classif.xgboost\", predict_type = \"prob\")\n\nat_xgb <- auto_tuner(learner = lrn_xgb,\n                    resampling = rsmp(\"cv\", folds = 5),\n                    measure = msr(\"classif.bbrier\"),\n                    terminator = trm_rt,\n                    tuner = tnr(\"random_search\"),\n                    search_space = space)\nat_xgb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<AutoTuner:classif.xgboost.tuned>\n* Model: list\n* Parameters: list()\n* Packages: mlr3, mlr3tuning, mlr3learners, xgboost\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, missings, multiclass,\n  offset, twoclass, weights\n* Search Space:\n                  id    class     lower       upper nlevels\n              <char>   <char>     <num>       <num>   <num>\n1:             alpha ParamDbl -6.907755    6.907755     Inf\n2: colsample_bylevel ParamDbl  0.100000    1.000000     Inf\n3:  colsample_bytree ParamDbl  0.100000    1.000000     Inf\n4:               eta ParamDbl -9.210340    0.000000     Inf\n5:            lambda ParamDbl -6.907755    6.907755     Inf\n6:         max_depth ParamInt  1.000000   20.000000      20\n7:           nrounds ParamInt  1.000000 5000.000000    5000\n8:         subsample ParamDbl  0.100000    1.000000     Inf\n```\n\n\n:::\n:::\n\n\n\nNow I can set up the outer resampling strategy (4-fold CV).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nouter_rsmp <- rsmp(\"cv\", folds = 4)\n```\n:::\n\n\n\nI can create a benchmark grid and run it for the task to compare the two learners.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Benchmark both learners\ndesign = benchmark_grid(\n  tasks = tsk_spam,\n  learners = list(lrn_logreg, at_xgb),\n  resamplings = outer_rsmp\n)\ndesign\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     task               learner resampling\n   <char>                <char>     <char>\n1:   spam       classif.log_reg         cv\n2:   spam classif.xgboost.tuned         cv\n```\n\n\n:::\n\n```{.r .cell-code}\n# run the benchmark design\nset.seed(101)\nbmr = benchmark(design)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\n# the score for each of the 4-fold CV outer folds\nbmr$score(msr(\"classif.bbrier\"))[, \n         .(learner_id, resampling_id, iteration, classif.bbrier)\n         ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              learner_id resampling_id iteration classif.bbrier\n                  <char>        <char>     <int>          <num>\n1:       classif.log_reg            cv         1     0.06236199\n2:       classif.log_reg            cv         2     0.05802780\n3:       classif.log_reg            cv         3     0.05744753\n4:       classif.log_reg            cv         4     0.05651042\n5: classif.xgboost.tuned            cv         1     0.04560204\n6: classif.xgboost.tuned            cv         2     0.02618333\n7: classif.xgboost.tuned            cv         3     0.04326268\n8: classif.xgboost.tuned            cv         4     0.03606656\n```\n\n\n:::\n\n```{.r .cell-code}\n# the aggregate score for each learner\nbmr$aggregate(msr(\"classif.bbrier\"))[,\n         .(learner_id, resampling_id, classif.bbrier)\n         ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              learner_id resampling_id classif.bbrier\n                  <char>        <char>          <num>\n1:       classif.log_reg            cv     0.05858693\n2: classif.xgboost.tuned            cv     0.03777865\n```\n\n\n:::\n:::\n\n\n\nI can use `autoplot` to plot these results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.bbrier\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nSo, XGBoost performs better than the logistic regression model on the `spam`\ntask. But, the XGBoost model is much more computationally expensive, takes\nlonger to train, and is less interpretable. So, the choice of model is a trade\noff between performance and interpretability.\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 3 (XGBoost vs. logistic regression)\n\nI benchmarked a tuned XGBoost model against an untuned logistic regression model on the `spam` classification task using the Brier score.\n\n**Steps I took:**\n\n1. **Loading the task**  \n   I used the built-in `tsk(\"spam\")`.\n\n2. **Logistic regression setup**  \n   I defined a `classif.log_reg` learner with `predict_type = \"prob\"` to enable Brier score calculation.\n\n3. **XGBoost setup with tuning**  \n   I used `classif.xgboost` with `predict_type = \"prob\"` and the predefined tuning space `lts(\"classif.xgboost.default\")` from `{mlr3tuningspaces}`.\n\n4. **AutoTuner for XGBoost**  \n   I created an `AutoTuner` with:\n   - 5-fold CV for inner resampling,  \n   - 60-second time budget via `trm(\"run_time\")`,  \n   - random search tuner,  \n   - Brier score as the measure.\n\n5. **Outer resampling**  \n   I used 4-fold CV for the outer loop.\n\n6. **Benchmark setup and execution**  \n   I created a benchmark grid comparing both learners on the task, ran the benchmark, and scored the results using the Brier score.\n\n7. **Results**  \n   I looked at individual fold scores using `bmr$score()` and aggregate performance using `bmr$aggregate()`. I also visualised the comparison with `autoplot()`.\n:::\n\n## Question4\n\nWrite a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\n    a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.\n    b. Identify the best configuration.\n    c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).\n    d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.\n\n### Answer\n\n#### Example on `mtcars` dataset\n\nLet's start by trying this out on the `mtcars` dataset, using the `regr.rpart` learner. I'll make it simpler by using the `to_tune()` function, which means I don't have to define the search space manually to be *e.g.* an integer, double *etc.*.\n\nFirst, I'll load the task and learner, and then look at the hyperparameters of this learner.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk(\"mtcars\")\nlearner <- lrn(\"regr.rpart\")\n# look at the hyperparameters\nas.data.table(learner$param_set)[, .(id, class, lower, upper)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                id    class lower upper\n            <char>   <char> <num> <num>\n 1:             cp ParamDbl     0     1\n 2:     keep_model ParamLgl    NA    NA\n 3:     maxcompete ParamInt     0   Inf\n 4:       maxdepth ParamInt     1    30\n 5:   maxsurrogate ParamInt     0   Inf\n 6:      minbucket ParamInt     1   Inf\n 7:       minsplit ParamInt     1   Inf\n 8: surrogatestyle ParamInt     0     1\n 9:   usesurrogate ParamInt     0     2\n10:           xval ParamInt     0   Inf\n```\n\n\n:::\n:::\n\n\n\n:::{.column-margin}\n\n| ID           | Description                                                             | Typical Range           | Notes                                    |\n|--------------|-------------------------------------------------------------------------|--------------------------|-------------------------------------------|\n| `cp`         | Complexity parameter: controls cost of adding splits                    | [0.001, 0.1] (log-scale) | Lower values allow deeper trees           |\n| `maxdepth`   | Maximum depth of the tree                                               | 1–30                     | Prevents trees from growing too deep      |\n| `minsplit`   | Minimum number of observations required to attempt a split              | 2–20                     | Higher values make trees more conservative |\n| `minbucket`  | Minimum number of observations in any terminal node                     | 1–10                     | If not set, defaults to `minsplit / 3`    |\n\n: Common hyperparameters to tune for `regr.rpart` learner.\n\n:::\n\n**Part 1:** creating a search_space and evaluating the learner\n\nI'll now create a search space to tune the four hyperparameters `cp`,\n`maxdepth`, `minsplit`, and `minbucket`, using `to_tune()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use to_tune() to create the search space\nlearner$param_set$values$cp <- to_tune(1e-4, 0.1)\nlearner$param_set$values$maxdepth <- to_tune(1, 30)\nlearner$param_set$values$minsplit <- to_tune(2, 20)\nlearner$param_set$values$minbucket <- to_tune(1, 10)\n\nparam_ids <- names(Filter(function(x) inherits(x, \"TuneToken\"), learner$param_set$values))\n# filter param_set bu param_names\nlearner$param_set$values[param_ids]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cp\nTuning over:\nrange [1e-04, 0.1]\n\n\n$maxdepth\nTuning over:\nrange [1, 30]\n\n\n$minbucket\nTuning over:\nrange [1, 10]\n\n\n$minsplit\nTuning over:\nrange [2, 20]\n```\n\n\n:::\n:::\n\n\n\nNow I'll tune the learner on this search space using a random search with 50 evaluations and 3-fold CV. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create random search tuner\ntuner <- tnr(\"random_search\")\n# create measure\nmeasure <- msr(\"regr.rmse\")\n# create resampling strategy\nresampling <- rsmp(\"cv\", folds = 3)\n# create terminator\nterminator <- trm(\"evals\", n_evals = 50)\n\n# create tuning instance and run tuner\nset.seed(333)\ninstance <- mlr3tuning::tune( # <1>\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measure = measure,\n  terminator = terminator\n)\n```\n:::\n\n\n1. I need to remember to call the correct `tune()` - the one from `{mlr3tuning}` instead of from `{e1701}`. If the latter is used, you get an error like `argument \"train.x\" is missing, with no default`.\n\n**Part 2:** Identifying the best configuration\n\nWe can see the result be looking at the `instance` object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningInstanceBatchSingleCrit>\n* State:  Optimized\n* Objective: <ObjectiveTuningBatch:regr.rpart_on_mtcars>\n* Search Space:\n          id    class lower upper nlevels\n      <char>   <char> <num> <num>   <num>\n1:        cp ParamDbl 1e-04   0.1     Inf\n2:  maxdepth ParamInt 1e+00  30.0      30\n3: minbucket ParamInt 1e+00  10.0      10\n4:  minsplit ParamInt 2e+00  20.0      19\n* Terminator: <TerminatorEvals>\n* Result:\n           cp maxdepth minbucket minsplit regr.rmse\n        <num>    <int>     <int>    <int>     <num>\n1: 0.02942214       18         3        5  3.488251\n* Archive:\n             cp maxdepth minbucket minsplit regr.rmse\n          <num>    <int>     <int>    <int>     <num>\n 1: 0.033376266       30         1        6  3.785812\n 2: 0.040949868       30         7       17  4.029892\n 3: 0.042838685       26        10       15  4.109428\n 4: 0.038569512        1        10        3  4.109428\n 5: 0.043422427        2         9        7  3.979189\n---                                                  \n46: 0.076254764        3         9        9  3.979189\n47: 0.062704666       16         3        4  3.488251\n48: 0.007789675        2         3       11  4.174014\n49: 0.082434254       18         3       10  4.174014\n50: 0.018441356       12         5        7  4.127388\n```\n\n\n:::\n:::\n\n\n\nThe best configuration is stored in the `result_learner_param_vals` slot of the `instance` object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the best configuration\nbest_config1 <- instance$result_learner_param_vals[param_ids]\nunlist(best_config1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         cp    maxdepth   minbucket    minsplit \n 0.02942214 18.00000000  3.00000000  5.00000000 \n```\n\n\n:::\n:::\n\n\n\nThe root mean-squared error is stored in `instance$result_y`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the RMSE\nbest_rmse1 <- instance$result_y\nbest_rmse1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.rmse \n 3.488251 \n```\n\n\n:::\n:::\n\n\n\n\n**Part 3:** create a smaller search space around the best configuration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain new upper and lower bounds\n# helper function to adjust bounds\n\nadjust_bounds <- function(param_id, best_config, learner, shrink = 0.25) {\n  param = learner$param_set$values[[param_id]]$content\n  bounds = list(lower = param$lower, upper = param$upper)\n  best = best_config[[param_id]]\n  range = bounds$upper - bounds$lower\n  lower_bound <- best - shrink * range\n  upper_bound <- best + shrink * range\n  # if class is ParamInt, then set lower to be ceiling and upper to be floor\n  # obtain class of the parameter\n  param_class <- learner$param_set$params[id == param_id, setNames(cls, id)]\n  if (param_class == \"ParamInt\") {\n    lower_bound = ceiling(lower_bound)\n    upper_bound = floor(upper_bound)\n  }\n  lower_new = max(lower_bound, bounds$lower)\n  upper_new = min(upper_bound, bounds$upper)\n  list(lower = lower_new, upper = upper_new)\n}\n\nadjusted_bounds <- lapply(param_ids, \n                          adjust_bounds,\n                          best_config = best_config1,\n                          learner = learner)\n\nnames(adjusted_bounds) <- param_ids\n\n# update learner\nlearner$param_set$values$cp <- to_tune(adjusted_bounds$cp$lower, \n                                       adjusted_bounds$cp$upper)\nlearner$param_set$values$maxdepth <- to_tune(adjusted_bounds$maxdepth$lower,\n                                             adjusted_bounds$maxdepth$upper)\nlearner$param_set$values$minsplit <- to_tune(adjusted_bounds$minsplit$lower,\n                                             adjusted_bounds$minsplit$upper)\nlearner$param_set$values$minbucket <- to_tune(adjusted_bounds$minbucket$lower,\n                                              adjusted_bounds$minbucket$upper)\n# check the new search space\nFilter(function(x) inherits(x, \"TuneToken\"), learner$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cp\nTuning over:\nrange [0.00444714480435941, 0.0543971448043594]\n\n\n$maxdepth\nTuning over:\nrange [11, 25]\n\n\n$minbucket\nTuning over:\nrange [1, 5]\n\n\n$minsplit\nTuning over:\nrange [2, 9]\n```\n\n\n:::\n:::\n\n\n\n**Part 4:** evaluate the learner on the new search space\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new tuning instance and run tuner\nset.seed(222)\ninstance <- mlr3tuning::tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measure = measure,\n  terminator = terminator\n)\n```\n:::\n\n\n\nGet the best configuration from this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the best configuration\nbest_config2 <- instance$result_learner_param_vals[param_ids]\nunlist(best_config2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         cp    maxdepth   minbucket    minsplit \n 0.01176303 15.00000000  1.00000000  4.00000000 \n```\n\n\n:::\n:::\n\n\n\nand the RMSE:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the RMSE\nbest_rmse2 <- instance$result_y\nbest_rmse2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.rmse \n 3.724687 \n```\n\n\n:::\n:::\n\n\n\nCool, so I have a new best configuration. Does it perform better than the first?\n\n:::{.column-margin}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Best configuration from the first tuning\nbest_config1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cp\n[1] 0.02942214\n\n$maxdepth\n[1] 18\n\n$minbucket\n[1] 3\n\n$minsplit\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# RMSE from the first configuration\nbest_rmse1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.rmse \n 3.488251 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Best configuration from the second tuning\nbest_config2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cp\n[1] 0.01176303\n\n$maxdepth\n[1] 15\n\n$minbucket\n[1] 1\n\n$minsplit\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# RMSE from the second configuration\nbest_rmse2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.rmse \n 3.724687 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare the two RMSEs\nbest_rmse2 - best_rmse1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.rmse \n 0.236436 \n```\n\n\n:::\n:::\n\n\n\nNo, it doesn't. I can repeat this process iteratively on smaller subspaces. At\nthe end, choose the best configuration by selecting the one with the lowest\nRMSE.\n\nThe next step is to create a function that does this for me.\n\n#### Creating a function\n\nSo, this function has the following inputs:\n- `task`: the task to tune;\n- `learner`: the learner to tune;\n- `search_space`: the search space to tune;\n    - this should be a data.table of the hyperparameters to tune, with class, and the lower and upper bounds;\n- `resampling`: the resampling strategy to use;\n- `measure`: the measure to use;\n- `random_search_stages`: the number of random search stages to perform;\n- `random_search_size`: the number of random evaluations to perform in each stage.\n\nThe function will return the best configuration found across all stages.\n\nFirst, I created a checker function to make sure all the inputs are valid.^[Actually I created this last, but it's required before my function so I'll put it here]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n.inputChecks <- function(task, learner, search_space, resampling, measure, \n                         random_search_stages, random_search_size) {\n\n        ## error checking\n        # check input classes\n        stopifnot(inherits(task, \"Task\"))\n        stopifnot(inherits(learner, \"Learner\"))\n        stopifnot(inherits(resampling, \"Resampling\"))\n        stopifnot(inherits(measure, \"Measure\"))\n        # check search space: required columns exist and are numeric where needed\n        required_cols <- c(\"id\", \"lower\", \"upper\")\n        missing_cols <- setdiff(required_cols, colnames(search_space))\n        if (length(missing_cols) > 0) {\n          stop(\"search_space must contain columns: \", paste(missing_cols, collapse = \", \"))\n        }\n\n        if (!all(sapply(search_space[, .(lower, upper)], is.numeric))) {\n          stop(\"Columns 'lower' and 'upper' in search_space must be numeric\")\n        }\n        # check ids match those in learner\n        invalid_ids <- setdiff(search_space$id, learner$param_set$ids())\n        if (length(invalid_ids) > 0) {\n          stop(\"Invalid parameter IDs in search_space: \", paste(invalid_ids, collapse = \", \"))\n        }\n        # check iterations inputs\n        stopifnot(is.numeric(random_search_stages), random_search_stages >= 1)\n        stopifnot(is.numeric(random_search_size), random_search_size >= 1)\n}\n```\n:::\n\n\n\nNow, let's create the function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niterative_random_tuner <- function(task, learner, search_space,\n                                   resampling, measure,\n                                   random_search_stages, random_search_size,\n                                   verbose = TRUE, seed = TRUE) {\n\n  .inputChecks(task, learner, search_space, resampling, measure,\n               random_search_stages, random_search_size)\n\n  param_ids <- search_space$id\n  tuner <- tnr(\"random_search\")\n  terminator <- trm(\"evals\", n_evals = random_search_size)\n\n  # initialise search space on learner\n  learner$param_set$values[param_ids] <- Map(to_tune,\n                                             search_space$lower,\n                                             search_space$upper)\n\n  best_config_list <- list()\n\n  if (seed) set.seed(22)\n  for (stage in seq_len(random_search_stages)) {\n    if (stage > 1) {\n      # update search space based on previous best\n      adjusted_bounds <- lapply(param_ids, adjust_bounds,\n                                best_config = best_config,\n                                learner = learner)\n      names(adjusted_bounds) <- param_ids\n      learner$param_set$values[param_ids] <- Map(to_tune,\n                                                 lapply(adjusted_bounds, `[[`, \"lower\"),\n                                                 lapply(adjusted_bounds, `[[`, \"upper\"))\n    }\n\n    # tune\n    instance <- mlr3tuning::tune(\n      tuner = tuner,\n      task = task,\n      learner = learner,\n      resampling = resampling,\n      measure = measure,\n      terminator = terminator\n    )\n\n    best_config <- unlist(instance$result_learner_param_vals[param_ids])\n    best_score <- instance$result_y\n\n    best_config_list[[stage]] <- list(config = best_config, score = best_score)\n\n    if (verbose) cat(sprintf(\"Stage %d score: %.3f\\n\", stage, best_score))\n  }\n\n  # label the list\n  names(best_config_list) <- paste0(\"stage_\", seq_along(best_config_list))\n\n  # return best config only\n  best_index <- if (measure$minimize) {\n    which.min(sapply(best_config_list, `[[`, \"score\"))\n  } else {\n    which.max(sapply(best_config_list, `[[`, \"score\"))\n  }\n\n  best_config_list[[best_index]]\n}\n```\n:::\n\n\n\nNow I can run this function on the `mtcars` dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create search space\nsearch_space <- rbindlist(list(\n  list(id = \"cp\", lower = 1e-4, upper = 1),\n  list(id = \"maxdepth\", lower = 1,    upper = 30),\n  list(id = \"minsplit\", lower = 2,    upper = 20),\n  list(id = \"minbucket\", lower = 1,   upper = 10)\n))\n\nbest_config <- iterative_random_tuner(\n        task = task,\n        learner = learner,\n        search_space = search_space,\n        resampling = rsmp(\"cv\", folds = 3),\n        measure = msr(\"regr.rmse\"),\n        random_search_stages = 5,\n        random_search_size = 50\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStage 1 score: 3.515\nStage 2 score: 3.686\nStage 3 score: 3.573\nStage 4 score: 3.608\nStage 5 score: 4.212\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_config\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$config\n         cp    maxdepth    minsplit   minbucket \n 0.04672705 16.00000000  6.00000000  1.00000000 \n\n$score\nregr.rmse \n 3.515434 \n```\n\n\n:::\n:::\n\n\n\nNice one.\n\n# Summary\n\nGreat. Let's summarise what I've done in this post.\n\n**Exercise 1: Hyperparameter Tuning with Random Search**\n\n- Tunes `regr.ranger` on `mtcars` dataset\n- Parameters tuned: `mtry`, `num.trees`, `sample.fraction`\n- Uses 3-fold CV, 50 random evaluations, MSE as the measure\n- Visualises marginal effects of hyperparameters\n- Retrains final model on full data using best hyperparameters\n\n**Exercise 2: Nested Resampling**\n\n- Evaluates tuned model’s performance with nested resampling\n- Outer loop: 3-fold CV; Inner loop: holdout validation (70/30)\n- Uses `AutoTuner` with same hyperparameter setup as Q1\n- Aggregates performance across outer test folds\n- Extracts inner tuning results and archives\n\n**Exercise 3: Benchmarking XGBoost vs Logistic Regression**\n\n- Task: binary classification on `spam` dataset\n- Logistic regression used as untuned baseline\n- XGBoost tuned using `mlr3tuningspaces::lts(\"classif.xgboost.default\")`\n- Inner loop: 5-fold CV with 60 sec time budget; Outer: 4-fold CV\n- Evaluates models using Brier score\n- Compares learners via tables and visualisation\n\n# Fin\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}