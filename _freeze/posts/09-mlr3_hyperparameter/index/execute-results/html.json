{
  "hash": "031d0c8ffa721cd4f106ce2748db34c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with {mlr3}\"\nsubtitle: \"03 Hyperparameter Optimisation\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-03-21\"\ncategories: [code, r, machine learning, mlr3]\nimage: \"./fig/mlr3_logo.svg\"\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\n# Introduction\n\nI am attempting to learn how to use `{mlr3}` [@lang2019mlr3], by reading through the book\n[Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)\n[@bischl2024usingmlr3].\n\nIn this post, I am working through the exercises given in [Chapter\n4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html)\nof the book [@becker2024hyperparameter], which covers hyperparameter optimisation (HPO).\nThis includes the following:\n\n- **Q1:** Tunes `regr.ranger` on `mtcars` using random search and 3-fold CV\n- **Q2:** Evaluates tuned model with nested resampling (3-fold CV outer, holdout inner)\n- **Q3:** Benchmarks tuned XGBoost vs logistic regression on `spam` using Brier score\n        - Uses `AutoTuner`, predefined tuning spaces, and `benchmark()` for comparison\n\nMy previous posts cover:\n\n- [Part one](../05-mlr3_basic_modelling/index.qmd):\n    - Create a classification tree model to predict diabetes.\n    - Look at the confusion matrix and create measures without using {mlr3measures}.\n    - Change the thresholds in the model.\n- [Part two](../08-mlr3_evaluation_benchmarking/index.qmd):\n    - Repeated cross-validation resampling.\n    - Using a custom resampling strategy.\n    - Creating a function that produces a ROC curve.\n\n\n## Prerequisites\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: paradox\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(mlr3tuningspaces)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n```\n:::\n\n\n\nSuppress all messaging unless it's a warning:^[The packages in `{mlr3}` that\nmake use of optimization, i.e., `{mlr3tuning}` or `{mlr3select}`, use the\nlogger of their base package `{bbotk}`.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n\n# Exercises\n\n1. Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of `lrn(\"regr.ranger\")` on `tsk(\"mtcars\")`. Use a simple random search with 50 evaluations. Evaluate with a 3-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.\n2. Evaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold CV for the outer resampling.\n3. Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n4. (\\*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\n        a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.\n        b. Identify the best configuration.\n        c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).\n        d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.\n\n## Question 1\n\nTune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of\n`lrn(\"regr.ranger\")` on `tsk(\"mtcars\")`. Use a simple random search with 50\nevaluations. Evaluate with a 3-fold CV and the root mean squared error.\nVisualize the effects that each hyperparameter has on the performance via\nsimple marginal plots, which plot a single hyperparameter versus the\ncross-validated MSE.\n\n### Answer\n\nLet's load the task and look at the properties of the `mtry`,\n`sample.fraction`, and `num.trees` parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_mtcars <- tsk(\"mtcars\")\nlrn(\"regr.ranger\")$param_set$data[id %in% c(\"mtry\", \"sample.fraction\", \"num.trees\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                id    class lower upper levels nlevels is_bounded special_vals\n            <char>   <char> <num> <num> <list>   <num>     <lgcl>       <list>\n1:            mtry ParamInt     1   Inf [NULL]     Inf      FALSE    <list[1]>\n2:       num.trees ParamInt     1   Inf [NULL]     Inf      FALSE    <list[0]>\n3: sample.fraction ParamDbl     0     1 [NULL]     Inf       TRUE    <list[0]>\n          default storage_type                   tags\n           <list>       <char>                 <list>\n1: <NoDefault[0]>      integer                  train\n2:            500      integer train,predict,hotstart\n3: <NoDefault[0]>      numeric                  train\n```\n\n\n:::\n:::\n\n\n\nThe hyperparameters I'm looking at are:\n\n- `mtry`: number of variables considered at each tree split.\n- `num.trees`: number of trees in the forest.\n- `sample.fraction`: fraction of observations used to train each tree.\n\nNow I will set up the tuning of the `mtry`, `sample.fraction`, and `num.trees` hyperparameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner <- lrn(\"regr.ranger\",\n               mtry = to_tune(p_int(1, 10)),\n               num.trees = to_tune(20, 2000),\n               sample.fraction = to_tune(0.1, 1))\nlearner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerRegrRanger:regr.ranger>: Random Forest\n* Model: -\n* Parameters: mtry=<ObjectTuneToken>, num.threads=1,\n  num.trees=<RangeTuneToken>, sample.fraction=<RangeTuneToken>\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], se, quantiles\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, missings, oob_error,\n  selected_features, weights\n```\n\n\n:::\n:::\n\n\n\nSetting up an instance to terminate the tuner after 50 evaluations, and to use 3-fold CV.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstance <- ti(task = tsk_mtcars,\n               learner = learner,\n               resampling = rsmp(\"cv\", folds = 3),\n               # rmse gives interpretability in the original units (MPG) rather than squared units\n               measures = msr(\"regr.rmse\"), \n               terminator = trm(\"evals\", n_evals = 50))\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningInstanceBatchSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuningBatch:regr.ranger_on_mtcars>\n* Search Space:\n                id    class lower upper nlevels\n            <char>   <char> <num> <num>   <num>\n1:            mtry ParamInt   1.0    10      10\n2:       num.trees ParamInt  20.0  2000    1981\n3: sample.fraction ParamDbl   0.1     1     Inf\n* Terminator: <TerminatorEvals>\n```\n\n\n:::\n:::\n\n\n\nThe tuning step uses 3-fold cross-validation:\n\n- In each evaluation, two-thirds of the data is used for training,\n- One-third is used for validation (i.e. to compute the RMSE).\n- This is repeated for 50 random configurations (as specified by the terminator).\n\n\nNow I can set up the tuning process (random search).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuner <- tnr(\"random_search\")\n#tuner$param_set\ntuner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TunerBatchRandomSearch>: Random Search\n* Parameters: batch_size=1\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning, bbotk\n```\n\n\n:::\n:::\n\n\n\nAnd trigger the tuning process.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(333)\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   <int>     <int>           <num>             <list>    <list>     <num>\n1:     5       187       0.9761367          <list[4]> <list[3]>  2.371042\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n   <int>     <int>           <num>             <list>    <list>     <num>\n1:     5       187       0.9761367          <list[4]> <list[3]>  2.371042\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$result$learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$num.threads\n[1] 1\n\n[[1]]$mtry\n[1] 5\n\n[[1]]$num.trees\n[1] 187\n\n[[1]]$sample.fraction\n[1] 0.9761367\n```\n\n\n:::\n:::\n\n\n\nAll 50 of the random search evaluations are stored in the `archive` slot of the\n`instance` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(mtry, sample.fraction, num.trees, regr.rmse)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mtry sample.fraction num.trees regr.rmse\n    <int>           <num>     <int>     <num>\n 1:     5       0.9761367       187  2.371042\n 2:     8       0.3760474      1227  3.165695\n 3:     4       0.9020999       959  2.387701\n 4:    10       0.8781863      1283  2.446111\n 5:     2       0.2718788       161  5.976847\n---                                          \n46:     9       0.2032898       720  5.989323\n47:     7       0.6087772        35  2.758711\n48:     4       0.1445110      1487  5.968638\n49:     6       0.7125770      1637  2.508655\n50:     3       0.1578512       302  5.936757\n```\n\n\n:::\n:::\n\n\n\nNow let's visualise the effect of each hyperparameter using marginal plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(instance, type = \"marginal\", cols_x = c(\"mtry\", \"sample.fraction\", \"num.trees\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n\nAs during the HPO stage, I used 3-fold CV, the model has not seen the full\ndata all at once. So, now I'll train the model using the optimised\nhyperparameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_ranger_tuned <- lrn(\"regr.ranger\")\nlrn_ranger_tuned$param_set$values = instance$result_learner_param_vals\nlrn_ranger_tuned$train(tsk_mtcars)$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, mtry = 5L, num.threads = 1L,      num.trees = 187L, sample.fraction = 0.976136745349504) \n\nType:                             Regression \nNumber of trees:                  187 \nSample size:                      32 \nNumber of independent variables:  10 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       5.910838 \nR squared (OOB):                  0.837275 \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 1 (hyperparameter tuning with random search)\n\nI tuned the `regr.ranger` learner on the `mtcars` dataset, focusing on three hyperparameters:\n\n- `mtry`: number of variables considered at each split,\n- `num.trees`: number of trees in the forest,\n- `sample.fraction`: fraction of the dataset used for each tree.\n\n**Steps I took:**\n\n1. **Exploration**\n   I inspected the available parameters for the learner using `$param_set`.\n\n2. **Learner setup**\n   I defined the tuning ranges with `to_tune()`:\n   - `mtry` from 1 to 10,\n   - `num.trees` from 1 to 100,000,\n   - `sample.fraction` from 0.1 to 1.\n\n3. **Tuning instance**\n   I created a `TuningInstanceSingleCrit` using:\n   - the `mtcars` task,\n   - 3-fold cross-validation for resampling,\n   - root mean squared error (RMSE) as the evaluation metric,\n   - a limit of 50 evaluations using a random search strategy.\n\n4. **Running the tuner**\n   I used `tnr(\"random_search\")` and called `$optimize()` to run the search. I then extracted the best hyperparameter combination and the archive of evaluated configurations.\n\n5. **Visualising results**\n   I used marginal plots to visualise the effect of each hyperparameter on the cross-validated RMSE.\n\n6. **Training the final model**\n   I retrained the `regr.ranger` model on the full dataset using the best parameters found.\n\n:::\n\n## Question 2\n\nEvaluate the performance of the model created in Exercise 1 with nested\nresampling. Use a holdout validation for the inner resampling and a 3-fold\nCV for the outer resampling.\n\n### Answer\n\nOK, so here we need an outer and inner resampling strategy.\nThe outer resampling strategy will be a 3-fold CV, and the inner resampling\nstrategy will be a holdout validation.\n\n![An illustration of nested resampling. The large blocks represent 3-fold CV for the outer resampling for model evaluation and the small blocks represent 4-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}\n\n@fig-nested-resampling-example shows an example of a nest resampling strategy\n(with 3-fold CV on the outer and 4-fold CV on the inner nest). Here, we\nneed to do something slightly different as we are using the holdout resampling\nstrategy on the inner nest.\n\n1. Outer resampling start\n   - Perform 3-fold cross-validation on the full dataset.\n   - For each outer fold, split the data into:\n     - Training set (light blue blocks)\n     - Test set (dark blue block)\n\n2. Inner resampling\n   - Within each outer training set, perform holdout validation (assuming 70/30 training-test split).\n   - This inner split is used for tuning hyperparameters (not evaluation).\n\n3. HPO – Hyperparameter tuning\n   - Evaluate different hyperparameter combinations by fitting models on the inner training set and evaluating performance on the inner validation (holdout) set.\n   - Select the best hyperparameter configuration based on performance on the inner holdout set. \n\n4. Training\n   - Fit the model on the entire outer training set using the tuned hyperparameters.\n\n5. Evaluation\n   - Evaluate the trained model on the outer test set (unseen during tuning).\n\n6. Outer resampling repeats\n   - Repeat steps 2–5 for each of the 3 outer folds.\n\n7. Aggregation\n   - Average the 3 outer test performance scores.\n   - This gives an unbiased estimate of the model’s generalisation performance with tuning.\n\nI will use `AutoTuner` to do nested resampling, as that is what is done in\n[Section\n4.3.1](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)\nof the mlr3 tutorial.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create auto_tuner to resample a random forest \n# with 3-fold CV in outer resampling and \n# holdout validation in inner resampling\nat <- auto_tuner(\n         tuner = tnr(\"random_search\"), #<1>\n         learner = lrn(\"regr.ranger\",\n                       mtry = to_tune(1, 1e1),\n                       num.trees = to_tune(1, 1e5),\n                       sample.fraction = to_tune(0.1, 1)),\n         # inner resampling\n         resampling = rsmp(\"holdout\", ratio = 0.7),\n         measure = msr(\"regr.rmse\"),\n         terminator = trm(\"evals\", n_evals = 50)\n        )\n\n# resampling step\nrr <- resample(tsk(\"mtcars\"),\n               at, \n               # outer resampling\n               rsmp(\"cv\", folds = 3), \n               store_models = TRUE) #<2>\n\nrr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 3 resampling iterations\n task_id        learner_id resampling_id iteration  prediction_test warnings\n  mtcars regr.ranger.tuned            cv         1 <PredictionRegr>        0\n  mtcars regr.ranger.tuned            cv         2 <PredictionRegr>        0\n  mtcars regr.ranger.tuned            cv         3 <PredictionRegr>        0\n errors\n      0\n      0\n      0\n```\n\n\n:::\n:::\n\n\n1. The tuners and learners are the same as in the previous exercise, I'm just\n   defining them again here for clarity.\n2. Set `store_models = TRUE` so that the `AutoTuner` models (fitted on the\n   outer training data) are stored,\n\nNow I aggregate across the three outer folds to get the final performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n13.13362 \n```\n\n\n:::\n:::\n\n\n\nThe inner tuning results can also be accessed, returning the optimal configurations (across all outer folds) and the full tuning archives.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# optimal configurations\nextract_inner_tuning_results(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   iteration  mtry num.trees sample.fraction regr.rmse learner_param_vals\n       <int> <int>     <int>           <num>     <num>             <list>\n1:         1     1     81784       0.6769030  2.884958          <list[4]>\n2:         2     5     73924       0.9194928  2.065022          <list[4]>\n3:         3     3     15719       0.7797092  1.807826          <list[4]>\n    x_domain task_id        learner_id resampling_id\n      <list>  <char>            <char>        <char>\n1: <list[3]>  mtcars regr.ranger.tuned            cv\n2: <list[3]>  mtcars regr.ranger.tuned            cv\n3: <list[3]>  mtcars regr.ranger.tuned            cv\n```\n\n\n:::\n\n```{.r .cell-code}\n# full tuning archives\nextract_inner_tuning_archives(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     iteration  mtry num.trees sample.fraction regr.rmse x_domain_mtry\n         <int> <int>     <int>           <num>     <num>         <int>\n  1:         1     2     15547       0.3219244  5.115809             2\n  2:         1     8     23468       0.3020848  5.113502             8\n  3:         1     5     47564       0.3500007  5.115646             5\n  4:         1     9     14922       0.8105999  3.261807             9\n  5:         1     2     81447       0.1308460  5.124990             2\n ---                                                                  \n146:         3     5     20556       0.5608060  2.081965             5\n147:         3     2      9179       0.8461799  1.867438             2\n148:         3     4     93847       0.1486419  3.733662             4\n149:         3     3     15719       0.7797092  1.807826             3\n150:         3     1     31967       0.2513883  3.739324             1\n     x_domain_num.trees x_domain_sample.fraction runtime_learners\n                  <int>                    <num>            <num>\n  1:              15547                0.3219244            0.068\n  2:              23468                0.3020848            0.096\n  3:              47564                0.3500007            0.202\n  4:              14922                0.8105999            0.115\n  5:              81447                0.1308460            0.388\n ---                                                             \n146:              20556                0.5608060            0.126\n147:               9179                0.8461799            0.053\n148:              93847                0.1486419            0.376\n149:              15719                0.7797092            0.096\n150:              31967                0.2513883            0.119\n               timestamp warnings errors batch_nr  resample_result task_id\n                  <POSc>    <int>  <int>    <int>           <list>  <char>\n  1: 2025-04-05 13:18:36        0      0        1 <ResampleResult>  mtcars\n  2: 2025-04-05 13:18:36        0      0        2 <ResampleResult>  mtcars\n  3: 2025-04-05 13:18:36        0      0        3 <ResampleResult>  mtcars\n  4: 2025-04-05 13:18:36        0      0        4 <ResampleResult>  mtcars\n  5: 2025-04-05 13:18:37        0      0        5 <ResampleResult>  mtcars\n ---                                                                      \n146: 2025-04-05 13:19:17        0      0       46 <ResampleResult>  mtcars\n147: 2025-04-05 13:19:17        0      0       47 <ResampleResult>  mtcars\n148: 2025-04-05 13:19:17        0      0       48 <ResampleResult>  mtcars\n149: 2025-04-05 13:19:18        0      0       49 <ResampleResult>  mtcars\n150: 2025-04-05 13:19:18        0      0       50 <ResampleResult>  mtcars\n            learner_id resampling_id\n                <char>        <char>\n  1: regr.ranger.tuned            cv\n  2: regr.ranger.tuned            cv\n  3: regr.ranger.tuned            cv\n  4: regr.ranger.tuned            cv\n  5: regr.ranger.tuned            cv\n ---                                \n146: regr.ranger.tuned            cv\n147: regr.ranger.tuned            cv\n148: regr.ranger.tuned            cv\n149: regr.ranger.tuned            cv\n150: regr.ranger.tuned            cv\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 2 (nested resampling)\n\nI evaluated the performance of the tuned model using nested resampling to obtain an unbiased estimate.\n\n**Steps I took:**\n\n1. **Resampling strategy**\n   I used:\n   - outer resampling: 3-fold cross-validation,\n   - inner resampling: holdout validation with a 70/30 split.\n\n2. **AutoTuner setup**\n   I reused the same `regr.ranger` learner and parameter ranges as in Question 1, wrapped in an `AutoTuner`. The tuning again used 50 evaluations of random search and MSE as the measure.\n\n3. **Resample execution**\n   I called `resample()` with the outer CV and the `AutoTuner`, setting `store_models = TRUE` to keep the fitted models from each outer fold.\n\n4. **Aggregating performance**\n   I used `$aggregate()` to average the MSE across the outer test folds.\n\n5. **Inspecting inner results**\n   I extracted the best configurations and full tuning logs from each inner loop using `extract_inner_tuning_results()` and `extract_inner_tuning_archives()`.\n\n:::\n\n## Question 3\n\nTune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n\n### Answer\n\nI’ll use the built-in `spam` task -- since the outcome is categorical, this is a classification task.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_spam <- tsk(\"spam\")\n```\n:::\n\n\n\nFirst I'll set up the logistic regression model (with no tuning).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# requires probs to compute the brier score\nlrn_logreg <- lrn(\"classif.log_reg\", predict_type = \"prob\")\n```\n:::\n\n\n\n:::{.column-margin}\n\nThe XGBoost model has lots of hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn(\"classif.xgboost\")$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"alpha\"                       \"approxcontrib\"              \n [3] \"base_score\"                  \"booster\"                    \n [5] \"callbacks\"                   \"colsample_bylevel\"          \n [7] \"colsample_bynode\"            \"colsample_bytree\"           \n [9] \"device\"                      \"disable_default_eval_metric\"\n[11] \"early_stopping_rounds\"       \"eta\"                        \n[13] \"eval_metric\"                 \"feature_selector\"           \n[15] \"gamma\"                       \"grow_policy\"                \n[17] \"interaction_constraints\"     \"iterationrange\"             \n[19] \"lambda\"                      \"lambda_bias\"                \n[21] \"max_bin\"                     \"max_delta_step\"             \n[23] \"max_depth\"                   \"max_leaves\"                 \n[25] \"maximize\"                    \"min_child_weight\"           \n[27] \"missing\"                     \"monotone_constraints\"       \n[29] \"nrounds\"                     \"normalize_type\"             \n[31] \"nthread\"                     \"ntreelimit\"                 \n[33] \"num_parallel_tree\"           \"objective\"                  \n[35] \"one_drop\"                    \"outputmargin\"               \n[37] \"predcontrib\"                 \"predinteraction\"            \n[39] \"predleaf\"                    \"print_every_n\"              \n[41] \"process_type\"                \"rate_drop\"                  \n[43] \"refresh_leaf\"                \"reshape\"                    \n[45] \"seed_per_iteration\"          \"sampling_method\"            \n[47] \"sample_type\"                 \"save_name\"                  \n[49] \"save_period\"                 \"scale_pos_weight\"           \n[51] \"skip_drop\"                   \"strict_shape\"               \n[53] \"subsample\"                   \"top_k\"                      \n[55] \"training\"                    \"tree_method\"                \n[57] \"tweedie_variance_power\"      \"updater\"                    \n[59] \"verbose\"                     \"watchlist\"                  \n[61] \"xgb_model\"                  \n```\n\n\n:::\n:::\n\n\n\nThe main ones are:\n\n| Hyperparameter         | Description                                               | Type     |\n|------------------------|-----------------------------------------------------------|----------|\n| `eta`                  | Learning rate (shrinkage)                                 | numeric  |\n| `max_depth`            | Maximum depth of trees                                    | integer  |\n| `nrounds`              | Number of boosting rounds (trees)                         | integer  |\n| `colsample_bytree`     | Fraction of features randomly sampled per tree            | numeric  |\n| `subsample`            | Fraction of rows sampled per tree                         | numeric  |\n| `min_child_weight`     | Minimum sum of instance weights in a child node           | numeric  |\n| `gamma`                | Minimum loss reduction to make a split                    | numeric  |\n| `lambda`               | L2 regularisation term on weights                         | numeric  |\n| `alpha`                | L1 regularisation term on weights                         | numeric  |\n\nA typical tuning strategy for XGBoost might involve:\n\n1. Starting with basic tree shape and learning rate:\n   - `max_depth`\n   - `eta`\n   - `nrounds`\n\n2. Adding sampling and regularisation to control overfitting:\n   - `subsample`\n   - `colsample_bytree`\n   - `min_child_weight`\n   - `gamma`\n\n3. Fine-tuning regularisation terms if needed:\n   - `lambda` (L2)\n   - `alpha` (L1)\n\n:::\n\nFor the XGBoost learner, I'm going to use a predefined search space from\n`{mlr3tuningspaces}`. First, I'll give a list of these predefined spaces.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_tuning_spaces$keys()[grepl(\"xgboost\", mlr_tuning_spaces$keys())]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"classif.xgboost.default\" \"classif.xgboost.rbv1\"   \n[3] \"classif.xgboost.rbv2\"    \"regr.xgboost.default\"   \n[5] \"regr.xgboost.rbv1\"       \"regr.xgboost.rbv2\"      \n```\n\n\n:::\n:::\n\n\n\nI will use the `classif.xgboost.default` space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspace = lts(\"classif.xgboost.default\")\nspace\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningSpace:classif.xgboost.default>: Classification XGBoost with Default\n                  id lower upper levels logscale\n              <char> <num> <num> <list>   <lgcl>\n1:               eta 1e-04     1 [NULL]     TRUE\n2:           nrounds 1e+00  5000 [NULL]    FALSE\n3:         max_depth 1e+00    20 [NULL]    FALSE\n4:  colsample_bytree 1e-01     1 [NULL]    FALSE\n5: colsample_bylevel 1e-01     1 [NULL]    FALSE\n6:            lambda 1e-03  1000 [NULL]     TRUE\n7:             alpha 1e-03  1000 [NULL]     TRUE\n8:         subsample 1e-01     1 [NULL]    FALSE\n```\n\n\n:::\n:::\n\n\n\nPlugging this into `auto_tuner()` creates an `AutoTuner` object. I'm going to use\n5-fold CV in the inner resampling and a terminator based on run time (of 60\nseconds).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create terminator with time budget of 60 secs\ntrm_rt = trm(\"run_time\")\ntrm_rt$param_set$values$secs = 60\n\n# create xgboost learner with prob predict_type\n# 'prob' required for brier score\nlrn_xgb = lrn(\"classif.xgboost\", predict_type = \"prob\")\n\nat_xgb <- auto_tuner(learner = lrn_xgb,\n                    resampling = rsmp(\"cv\", folds = 5),\n                    measure = msr(\"classif.bbrier\"),\n                    terminator = trm_rt,\n                    tuner = tnr(\"random_search\"),\n                    search_space = space)\nat_xgb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<AutoTuner:classif.xgboost.tuned>\n* Model: list\n* Parameters: list()\n* Packages: mlr3, mlr3tuning, mlr3learners, xgboost\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, missings, multiclass,\n  offset, twoclass, weights\n* Search Space:\n                  id    class     lower       upper nlevels\n              <char>   <char>     <num>       <num>   <num>\n1:             alpha ParamDbl -6.907755    6.907755     Inf\n2: colsample_bylevel ParamDbl  0.100000    1.000000     Inf\n3:  colsample_bytree ParamDbl  0.100000    1.000000     Inf\n4:               eta ParamDbl -9.210340    0.000000     Inf\n5:            lambda ParamDbl -6.907755    6.907755     Inf\n6:         max_depth ParamInt  1.000000   20.000000      20\n7:           nrounds ParamInt  1.000000 5000.000000    5000\n8:         subsample ParamDbl  0.100000    1.000000     Inf\n```\n\n\n:::\n:::\n\n\n\nNow I can set up the outer resampling strategy (4-fold CV).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nouter_rsmp <- rsmp(\"cv\", folds = 4)\n```\n:::\n\n\n\nI can create a benchmark grid and run it for the task to compare the two learners.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Benchmark both learners\ndesign = benchmark_grid(\n  tasks = tsk_spam,\n  learners = list(lrn_logreg, at_xgb),\n  resamplings = outer_rsmp\n)\ndesign\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     task               learner resampling\n   <char>                <char>     <char>\n1:   spam       classif.log_reg         cv\n2:   spam classif.xgboost.tuned         cv\n```\n\n\n:::\n\n```{.r .cell-code}\n# run the benchmark design\nset.seed(101)\nbmr = benchmark(design)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\n# the score for each of the 4-fold CV outer folds\nbmr$score(msr(\"classif.bbrier\"))[, \n         .(learner_id, resampling_id, iteration, classif.bbrier)\n         ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              learner_id resampling_id iteration classif.bbrier\n                  <char>        <char>     <int>          <num>\n1:       classif.log_reg            cv         1     0.06236199\n2:       classif.log_reg            cv         2     0.05802780\n3:       classif.log_reg            cv         3     0.05744753\n4:       classif.log_reg            cv         4     0.05651042\n5: classif.xgboost.tuned            cv         1     0.04560204\n6: classif.xgboost.tuned            cv         2     0.02618333\n7: classif.xgboost.tuned            cv         3     0.04326268\n8: classif.xgboost.tuned            cv         4     0.03606656\n```\n\n\n:::\n\n```{.r .cell-code}\n# the aggregate score for each learner\nbmr$aggregate(msr(\"classif.bbrier\"))[,\n         .(learner_id, resampling_id, classif.bbrier)\n         ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              learner_id resampling_id classif.bbrier\n                  <char>        <char>          <num>\n1:       classif.log_reg            cv     0.05858693\n2: classif.xgboost.tuned            cv     0.03777865\n```\n\n\n:::\n:::\n\n\n\nI can use `autoplot` to plot these results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.bbrier\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nSo, XGBoost performs better than the logistic regression model on the `spam`\ntask. But, the XGBoost model is much more computationally expensive, takes\nlonger to train, and is less interpretable. So, the choice of model is a trade\noff between performance and interpretability.\n\n:::{.callout-note collapse=\"true\"}\n# Summary of question 3 (XGBoost vs. logistic regression)\n\nI benchmarked a tuned XGBoost model against an untuned logistic regression model on the `spam` classification task using the Brier score.\n\n**Steps I took:**\n\n1. **Loading the task**  \n   I used the built-in `tsk(\"spam\")`.\n\n2. **Logistic regression setup**  \n   I defined a `classif.log_reg` learner with `predict_type = \"prob\"` to enable Brier score calculation.\n\n3. **XGBoost setup with tuning**  \n   I used `classif.xgboost` with `predict_type = \"prob\"` and the predefined tuning space `lts(\"classif.xgboost.default\")` from `{mlr3tuningspaces}`.\n\n4. **AutoTuner for XGBoost**  \n   I created an `AutoTuner` with:\n   - 5-fold CV for inner resampling,  \n   - 60-second time budget via `trm(\"run_time\")`,  \n   - random search tuner,  \n   - Brier score as the measure.\n\n5. **Outer resampling**  \n   I used 4-fold CV for the outer loop.\n\n6. **Benchmark setup and execution**  \n   I created a benchmark grid comparing both learners on the task, ran the benchmark, and scored the results using the Brier score.\n\n7. **Results**  \n   I looked at individual fold scores using `bmr$score()` and aggregate performance using `bmr$aggregate()`. I also visualised the comparison with `autoplot()`.\n:::\n\n## Question4\n\nWrite a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\n        a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.\n        b. Identify the best configuration.\n        c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).\n        d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.\n\n# Summary\n\nGreat. Let's summarise what I've done in this post.\n\n**Exercise 1: Hyperparameter Tuning with Random Search**\n\n- Tunes `regr.ranger` on `mtcars` dataset\n- Parameters tuned: `mtry`, `num.trees`, `sample.fraction`\n- Uses 3-fold CV, 50 random evaluations, MSE as the measure\n- Visualises marginal effects of hyperparameters\n- Retrains final model on full data using best hyperparameters\n\n**Exercise 2: Nested Resampling**\n\n- Evaluates tuned model’s performance with nested resampling\n- Outer loop: 3-fold CV; Inner loop: holdout validation (70/30)\n- Uses `AutoTuner` with same hyperparameter setup as Q1\n- Aggregates performance across outer test folds\n- Extracts inner tuning results and archives\n\n**Exercise 3: Benchmarking XGBoost vs Logistic Regression**\n\n- Task: binary classification on `spam` dataset\n- Logistic regression used as untuned baseline\n- XGBoost tuned using `mlr3tuningspaces::lts(\"classif.xgboost.default\")`\n- Inner loop: 5-fold CV with 60 sec time budget; Outer: 4-fold CV\n- Evaluates models using Brier score\n- Compares learners via tables and visualisation\n\n# Fin\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}