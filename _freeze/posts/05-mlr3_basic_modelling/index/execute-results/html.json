{
  "hash": "d34f7b3393c3da3285630c9da51644ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with {mlr3}\"\nsubtitle: \"01 Data and Basic Modeling\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-03-03\"\ncategories: [code, r, machine learning, mlr3]\nimage: \"./fig/mlr3_logo.svg\"\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\n# Introduction\n\nI am attempting to learn how to use `{mlr3}` [@lang2019mlr3], by reading through the book\n[Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)\n[@bischl2024usingmlr3].\n\nIn this first blog post, I am going through the exercises given in Section\n2 [@foss2024data]. This involves creating a classification tree model, on the\n`PimaIndiansDiabetes2` (from the `{mlbench}` package), to predict whether\na person has diabetes or not. No (proper) evaluation or validation is done here -- that'll be for a later post.\n\n## Prerequisites\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n```\n:::\n\n\n\n# Exercises\n\n## Question 1 {#sec-question-one}\nTrain a classification model with the `\"classif.rpart\"` learner on the Pima\nIndians Diabetes dataset. Do this without using `tsk(\"pima\")`, and instead by\nconstructing a task from the dataset in the `mlbench` package:\n`data(PimaIndiansDiabetes2, package = \"mlbench\")`.\n\n:::{.callout-note collapse = true}\n# Missing data\n\nNote: The dataset has `NA`s\nin its features. You can either rely on `rpart`'s capability to handle them\ninternally (surrogate splits) or remove them from the initial `data.frame`\nusing `na.omit()`.\n\nThe rpart algorithm has a built-in method called surrogate splits, which allows\nit to handle missing values without removing data. If a feature value is\nmissing at a particular split, rpart:\n\n1.\tTries to use an alternative feature (a surrogate variable) that closely\n    mimics the main splitting feature.\n2.\tIf no good surrogate is found, it assigns the most common class (for\n    classification) or the mean value (for regression) within that split.\n\n:::\n\n  - Make sure to define the `pos` outcome as the positive class.\n  - Train the model on a random 80% subset of the given data and evaluate its\n    performance with the classification error measure on the remaining data.\n\n### Answer\n\nLoading the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(PimaIndiansDiabetes2, package = \"mlbench\")\npima <- as.data.table(PimaIndiansDiabetes2)\npima\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n        <num>   <num>    <num>   <num>   <num> <num>    <num> <num>   <fctr>\n  1:        6     148       72      35      NA  33.6    0.627    50      pos\n  2:        1      85       66      29      NA  26.6    0.351    31      neg\n  3:        8     183       64      NA      NA  23.3    0.672    32      pos\n  4:        1      89       66      23      94  28.1    0.167    21      neg\n  5:        0     137       40      35     168  43.1    2.288    33      pos\n ---                                                                        \n764:       10     101       76      48     180  32.9    0.171    63      neg\n765:        2     122       70      27      NA  36.8    0.340    27      neg\n766:        5     121       72      23     112  26.2    0.245    30      neg\n767:        1     126       60      NA      NA  30.1    0.349    47      pos\n768:        1      93       70      31      NA  30.4    0.315    23      neg\n```\n\n\n:::\n:::\n\n\n\nI want to predict whether each person has diabetes, using a CART\n('classification and regression tree').\n\n#### Creating a task \n\nFirst, I create the `task`. I am defining `pos` to be the positive class in\nthis step. It can also be done later by setting `tsk_pima$positive = \"pos\"`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_pima <- as_task_classif(pima, target = \"diabetes\", positive = \"pos\")\ntsk_pima\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:pima> (768 x 9)\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n    triceps\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#autoplot(tsk_pima, type = \"duo\") +\n  #theme(strip.text.y = element_text(angle = -0, size = 8))\n\nautoplot(tsk_pima, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![A pairs plot of the `pima` dataset. Note that it is unbalanced, as there are more negative diabetes outcomes than positive.](index_files/figure-html/fig-pima-pairs-1.png){#fig-pima-pairs width=720}\n:::\n:::\n\n\n\nLet's see how unbalanced the data is...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npima[, .N, by = \"diabetes\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   diabetes     N\n     <fctr> <int>\n1:      pos   268\n2:      neg   500\n```\n\n\n:::\n:::\n\n\n\n#### Splitting the data\n\nCreate a split of $80\\%$ training and $20\\%$ test data.\n\n:::{.callout-important}\nI know this is bad practice. Most of the time (see below for caveats), all the\ndata should be used to fit the model, and then internal validation done via resampling (*e.g.* using\nbootstrap or cross-validation).\n\nFrom Frank Harrell's [blog](https://www.fharrell.com/post/split-val/),\n\n> data splitting is an unstable method for validating models or classifiers,\nespecially when the number of subjects is less than about 20,000 (fewer if\nsignal:noise ratio is high). This is because were you to split the data again,\ndevelop a new model on the training sample, and test it on the holdout sample,\nthe results are likely to vary significantly. Data splitting requires\na significantly larger sample size than resampling to work acceptably well\n\nAlso see @steyerberg2018validation.\n\nTo chose whether to do internal or external validation, see the *Biostatistics\nfor Biomedical Research*\n[summary](https://hbiostat.org/bbr/reg.html#summary-choosing-internal-vs.-external-validation).\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(52)\nsplits <- partition(tsk_pima, ratio = 0.8)\nsplits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$train\n  [1]   1   2   3   4   5   8   9  10  11  12  14  18  19  20  21  22  23  24\n [19]  25  27  28  29  30  31  32  34  35  36  37  38  39  40  41  42  43  44\n [37]  46  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64\n [55]  65  66  69  70  71  72  77  78  79  80  81  82  83  85  86  88  89  90\n [73]  91  92  93  94  97  98  99 100 101 103 104 105 106 107 108 109 110 111\n [91] 112 113 114 116 117 118 119 120 121 122 123 124 125 128 129 130 132 133\n[109] 135 136 137 138 139 140 142 143 144 145 146 148 149 151 152 153 157 158\n[127] 159 160 161 162 163 164 165 166 167 169 170 171 173 174 175 177 178 179\n[145] 180 181 182 183 184 185 186 187 188 189 190 191 194 195 196 197 198 199\n[163] 200 201 202 203 204 205 206 207 209 210 212 213 215 216 217 218 220 221\n[181] 222 224 225 226 228 229 231 233 234 235 236 237 238 243 244 245 246 247\n[199] 248 251 252 255 256 257 258 259 261 262 263 264 266 267 268 269 270 271\n[217] 273 275 276 277 279 280 281 282 283 284 285 286 287 290 291 292 293 294\n[235] 295 297 298 299 301 302 305 306 307 309 310 311 312 313 314 315 316 318\n[253] 319 320 321 322 324 326 327 328 329 330 331 332 333 334 335 336 337 339\n[271] 340 341 343 344 346 347 349 350 351 352 353 354 355 356 357 358 359 360\n[289] 361 364 365 366 367 368 369 370 371 372 373 374 375 376 378 379 380 381\n[307] 382 384 386 387 389 390 391 392 393 394 395 396 397 398 399 401 402 403\n[325] 404 405 406 407 408 409 410 411 412 414 415 416 417 418 420 421 422 423\n[343] 424 425 427 429 430 433 436 437 439 440 441 442 443 444 445 446 447 448\n[361] 449 450 451 452 453 454 455 456 457 459 461 463 464 465 466 469 470 471\n[379] 472 473 474 476 477 478 482 483 484 485 486 487 488 489 490 491 493 494\n[397] 495 497 498 499 500 501 503 504 505 506 508 509 510 511 512 513 514 515\n[415] 516 517 518 520 521 522 523 524 526 527 528 529 530 532 533 534 535 536\n[433] 537 538 539 542 543 544 545 548 549 550 551 552 553 554 555 556 557 560\n[451] 562 563 564 565 566 568 569 570 572 573 574 575 576 578 579 580 581 582\n[469] 583 584 585 586 588 589 590 591 592 594 595 596 597 598 600 601 602 603\n[487] 604 605 606 607 608 609 610 611 612 615 616 617 618 619 620 621 623 625\n[505] 627 628 629 630 632 634 635 636 637 639 640 641 642 643 644 645 646 647\n[523] 649 650 651 654 656 658 659 660 661 662 663 665 666 667 669 670 672 674\n[541] 676 677 678 681 683 684 686 687 688 689 690 692 693 695 697 698 700 701\n[559] 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 722 723\n[577] 724 725 726 727 728 729 730 732 735 736 737 738 739 740 741 742 743 744\n[595] 746 748 749 750 751 752 753 754 756 757 758 759 760 761 762 763 764 765\n[613] 766 767\n\n$test\n  [1]   6   7  13  15  16  17  26  33  45  47  67  68  73  74  75  76  84  87\n [19]  95  96 102 115 126 127 131 134 141 147 150 154 155 156 168 172 176 192\n [37] 193 208 211 214 219 223 227 230 232 239 240 241 242 249 250 253 254 260\n [55] 265 272 274 278 288 289 296 300 303 304 308 317 323 325 338 342 345 348\n [73] 362 363 377 383 385 388 400 413 419 426 428 431 432 434 435 438 458 460\n [91] 462 467 468 475 479 480 481 492 496 502 507 519 525 531 540 541 546 547\n[109] 558 559 561 567 571 577 587 593 599 613 614 622 624 626 631 633 638 648\n[127] 652 653 655 657 664 668 671 673 675 679 680 682 685 691 694 696 699 718\n[145] 719 720 721 731 733 734 745 747 755 768\n\n$validation\ninteger(0)\n```\n\n\n:::\n:::\n\n\n\n#### Training the model\n\nNow, I will train the classification tree on the training data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# loading the learners\nlrn_featureless <- lrn(\"classif.featureless\", predict_type = \"prob\")\nlrn_rpart <- lrn(\"classif.rpart\", predict_type = \"prob\") # 'prob' is the default prediction type\nlrn_rpart\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n\n\n:::\n\n```{.r .cell-code}\n# training the learners\nlrn_featureless$train(tsk_pima, splits$train)\nlrn_rpart$train(tsk_pima, splits$train)\nlrn_rpart\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: rpart\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n\n\n:::\n:::\n\n\n\n\n#### Evaluating the model {#sec-question-one-evaluating}\n\nHere, I'm evaluating the model on the test data (and comparing against the\nfeatureless learner).\n\nI will consider the Brier, log-loss and accuracy `measures`.\nThe [Brier score](https://en.wikipedia.org/wiki/Brier_score) lies between $[0,\n1]$, where $0$ is best. The log-loss is the negative logarithm of the predicted\nprobability for the true class, and the accuracy is the number of correct\npredictions divided by total number of predictions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load accuracy measures\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\n\n# predicting using the featureless learner\nprediction_featureless <- lrn_featureless$predict(tsk_pima, splits$test)\nprediction_featureless \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 154 observations:\n row_ids truth response  prob.pos  prob.neg\n       6   neg      neg 0.3485342 0.6514658\n       7   pos      neg 0.3485342 0.6514658\n      13   neg      neg 0.3485342 0.6514658\n     ---   ---      ---       ---       ---\n     747   pos      neg 0.3485342 0.6514658\n     755   pos      neg 0.3485342 0.6514658\n     768   neg      neg 0.3485342 0.6514658\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtaining score of featureless learner\nprediction_featureless$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.mbrier classif.logloss     classif.acc \n      0.4553977       0.6478575       0.6493506 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# predicting using the classification tree\nprediction_rpart <- lrn_rpart$predict(tsk_pima, splits$test)\nprediction_rpart \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 154 observations:\n row_ids truth response   prob.pos  prob.neg\n       6   neg      neg 0.00000000 1.0000000\n       7   pos      neg 0.08675799 0.9132420\n      13   neg      neg 0.24390244 0.7560976\n     ---   ---      ---        ---       ---\n     747   pos      pos 0.77777778 0.2222222\n     755   pos      pos 0.76744186 0.2325581\n     768   neg      neg 0.08675799 0.9132420\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtaining score of the classification tree\nprediction_rpart$score(measures) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.mbrier classif.logloss     classif.acc \n      0.2763229       0.8516860       0.8246753 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# confusion matrix\nprediction_rpart$confusion # <1>\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n```\n\n\n:::\n:::\n\n\n1. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_plot <- autoplot(prediction_rpart) + ggtitle(\"Default\")\nprediction_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n## Question 2\nCalculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in Exercise 1.\n\n  - Try to solve this in two ways:\n    1. Using `mlr3measures`-predefined measure objects.\n    2. Without using `mlr3` tools by directly working on the ground truth and prediction vectors.\n  - Compare the results.\n\n### Answer\n\nI've already started this in Question 1 (@sec-question-one-evaluating), but I will\nreiterate here. The confusion matrix gives the number of predictions that are\ncorrect (true positives or negatives) on the diagonal, and those that are incorrect (false\npositives and negatives) on the top right and bottom left, respectively\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confusion matrix\nconf_matrix <- prediction_rpart$confusion\nconf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n```\n\n\n:::\n:::\n\n\n\nI want to obtain the *rates*, both using the\n[`mlr3measures`](https://mlr3.mlr-org.com/reference/mlr_measures.html) objects,\nand without.\n\n:::{.column-margin}\n\nSensitivity\n: (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\n\nSpecificity\n: (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.\n\n:::\n\n#### Using `mlr3measures`\n\nFirst, let's figure out the measures we need...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(mlr_measures)[task_type == \"classif\" & predict_type == \"response\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <key>\n                    key                         label task_type\n                 <char>                        <char>    <char>\n 1:         classif.acc       Classification Accuracy   classif\n 2:        classif.bacc             Balanced Accuracy   classif\n 3:          classif.ce          Classification Error   classif\n 4:       classif.costs Cost-sensitive Classification   classif\n 5:         classif.dor         Diagnostic Odds Ratio   classif\n---                                                            \n19: classif.specificity                   Specificity   classif\n20:          classif.tn                True Negatives   classif\n21:         classif.tnr            True Negative Rate   classif\n22:          classif.tp                True Positives   classif\n23:         classif.tpr            True Positive Rate   classif\n             packages predict_type properties task_properties\n               <list>       <char>     <list>          <list>\n 1: mlr3,mlr3measures     response                           \n 2: mlr3,mlr3measures     response                           \n 3: mlr3,mlr3measures     response                           \n 4:              mlr3     response                           \n 5: mlr3,mlr3measures     response                   twoclass\n---                                                          \n19: mlr3,mlr3measures     response                   twoclass\n20: mlr3,mlr3measures     response                   twoclass\n21: mlr3,mlr3measures     response                   twoclass\n22: mlr3,mlr3measures     response                   twoclass\n23: mlr3,mlr3measures     response                   twoclass\n```\n\n\n:::\n:::\n\n\n\nOK, so we need to use the measures `classif.tpr` `classif.fpr` `classif.tnr`\nand `classif.fnr`, for the true positive, false positive, true negative and\nfalse negative rates, respectively.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures <- msrs(c(\"classif.tpr\", \"classif.fpr\", \"classif.tnr\", \"classif.fnr\"))\nprediction_rpart$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.fpr classif.tnr classif.fnr \n  0.7407407   0.1300000   0.8700000   0.2592593 \n```\n\n\n:::\n:::\n\n\n\n\n#### Without using `mlr3measures`\n\nI can obtain these rates directly from the confusion matrix.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 'table' int [1:2, 1:2] 40 14 13 87\n - attr(*, \"dimnames\")=List of 2\n  ..$ response: chr [1:2] \"pos\" \"neg\"\n  ..$ truth   : chr [1:2] \"pos\" \"neg\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# true positive rate / sensitivity\ntpr <- conf_matrix[1, 1]/ sum(conf_matrix[, 1])\n# false positive rate\nfpr <- conf_matrix[1, 2]/ sum(conf_matrix[, 2])\n\n# true negative rate / specificity\ntnr <- conf_matrix[2, 2]/ sum(conf_matrix[, 2])\n# false negative rate\nfnr <- conf_matrix[2, 1]/ sum(conf_matrix[, 1])\n\ndata.table(\n  classif.tpr = tpr,\n  classif.fpr = fpr,\n  classif.tnr = tnr,\n  classif.fnr = fnr\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   classif.tpr classif.fpr classif.tnr classif.fnr\n         <num>       <num>       <num>       <num>\n1:   0.7407407        0.13        0.87   0.2592593\n```\n\n\n:::\n:::\n\n\n\n\n## Question 3\nChange the threshold of the model from Question 1 such that the false positive rate is lower than the false negative rate.\n\n- What is one reason you might do this in practice?\n\n### Answer\n\nOne reason I might want a lower false positive rate than false negative rate is it the damage done by a false positive is higher than that done by a false negative. That if, if classifying the outcome as positive when it is actually negative is more damaging than the other way round. For example, if I am building a model to predict fraud for a bank, and a false positive would result in a customer transaction being wrongly declined. Lots of false positives could result in annoyed customers and a loss of trust.\n\n#### Inverse weights\n\nLet's first change the thresholds such that they account for the inbalanced data. I'm not considering false positives here.\n\nFrom @fig-pima-pairs, it's clear that the data is unbalanced (more people with\nnegative diabetes than positive). I can account for this by changing the\nthresholds using inverse weightings.\n\nFirst, let's use the training data to obtain new thresholds.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_thresh = proportions(table(tsk_pima$truth(splits$train)))\nnew_thresh\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n      pos       neg \n0.3485342 0.6514658 \n```\n\n\n:::\n:::\n\n\n\nAnd then I'll use these thresholds to reweight the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_rpart$set_threshold(new_thresh)\nprediction_rpart$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse pos neg\n     pos  40  13\n     neg  14  87\n```\n\n\n:::\n\n```{.r .cell-code}\nprediction_plot_newt <- autoplot(prediction_rpart) +\n                                ggtitle(\"Inverse weighting thresholds\")\nprediction_plot + prediction_plot_newt +\n        plot_layout(guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nOh, it doesn't make a difference!\n\n#### Reducing false positive rate\n\nThis can be achieved by making it more difficult for the model to predict a positive result.\n\nSo, let's create thresholds where the `pos` result is penalised.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_thresh <- c(\"pos\" = 0.7, \"neg\" = 0.3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_rpart$set_threshold(new_thresh)\nprediction_rpart$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse pos neg\n     pos  35  10\n     neg  19  90\n```\n\n\n:::\n\n```{.r .cell-code}\nmeasures <- msrs(c(\"classif.tpr\", \"classif.fpr\", \"classif.tnr\", \"classif.fnr\"))\nprediction_rpart$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.fpr classif.tnr classif.fnr \n  0.6481481   0.1000000   0.9000000   0.3518519 \n```\n\n\n:::\n\n```{.r .cell-code}\nprediction_plot_newt <- autoplot(prediction_rpart) +\n                                ggtitle(\"New thresholds\")\nprediction_plot + prediction_plot_newt +\n        plot_layout(guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nHere, the false positive rate has decreased, but the false negative has increased (as expected).\n\n\n# Fin\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}