{
  "hash": "8ec72bedb578a6f475b332286dbc1ebe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with {mlr3}\"\nsubtitle: \"02 Evaluation and Benchmarking\"\nauthor:\n  - name: Paul Smith\ndate: \"2025-03-17\"\ncategories: [code, r, machine learning, mlr3]\nimage: \"./fig/mlr3_logo.svg\"\nformat:\n  html:\n    code-fold: false\nexecute:\n  df-print: default\n---\n\n\n\n# Introduction\n\nI am attempting to learn how to use `{mlr3}` [@lang2019mlr3], by reading through the book\n[Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)\n[@bischl2024usingmlr3].\n\nMy previous posts include:\n\n- [Part one](../05-mlr3_basic_modelling/index.qmd):\n    - Create a classification tree model to predict diabetes.\n    - Look at the confusion matrix and create measures without using {mlr3measures}.\n    - Change the thresholds in the model.\n\nIn this second blog post, I am going through the exercises\ngiven in Section 3 [@casalicchio2024evaluation].\nThis involves using repeated cross-validation resampling, using a custom\nresampling strategy, and creating a function that produces a ROC.\n\n\n## Prerequisites\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3viz)\nlibrary(mlr3learners)\nlibrary(mlr3data)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\noptions(datatable.print.nrows = 20)\n```\n:::\n\n\n\n# Exercises\n\n1. Apply a repeated cross-validation resampling strategy on `tsk(\"mtcars\")` and evaluate the performance of `lrn(\"regr.rpart\")`.\n   - Use five repeats of three folds each.\n   - Calculate the MSE for each iteration and visualize the result.\n   - Finally, calculate the aggregated performance score.\n\n2. Use `tsk(\"spam\")` and five-fold CV to benchmark `lrn(\"classif.ranger\")`, `lrn(\"classif.log_reg\")`, and `lrn(\"classif.xgboost\", nrounds = 100)` with respect to AUC.\n   - Which learner appears to perform best?\n   - How confident are you in your conclusion?\n   - Think about the stability of results and investigate this by re-running the experiment with different seeds.\n   - What can be done to improve this?\n\n3. A colleague reports a 93.1% classification accuracy using `lrn(\"classif.rpart\")` on `tsk(\"penguins_simple\")`.\n   - You want to reproduce their results and ask them about their resampling strategy.\n   - They said they used a custom three-fold CV with folds assigned as `factor(task$row_ids %% 3)`.\n   - See if you can reproduce their results.\n\n4. (\\*) Program your own ROC plotting function **without** using `mlr3`'s `autoplot()` function.\n   - The signature of your function should be `my_roc_plot(task, learner, train_indices, test_indices)`.\n   - Your function should use the `$set_threshold()` method of `Prediction`, as well as `mlr3measures`.\n\nFirst, let's suppress all messaging unless it's a warning:^[See [Section 10.3](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-logging) of the tutorial for more information about mlr3 logging output)]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n\n\n## Question 1\n\nApply a repeated cross-validation resampling strategy on `tsk(\"mtcars\")` and evaluate the performance of `lrn(\"regr.rpart\")`.\n\n- Use five repeats of three folds each.\n- Calculate the MSE for each iteration and visualize the result.\n- Finally, calculate the aggregated performance score.\n\n### Answer\n\nFirst, I'll load the `Task`, `Learner`, and create the `rsmp()` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_mtcars <- tsk(\"mtcars\")\ntsk_mtcars\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n\n\n:::\n\n```{.r .cell-code}\n# load learner\nlrn_rpart <- lrn(\"regr.rpart\")\nlrn_rpart\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerRegrRpart:regr.rpart>: Regression Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, selected_features, weights\n```\n\n\n:::\n\n```{.r .cell-code}\n# load resampling method: 5 lots of three-fold CV\nrcv53 = rsmp(\"repeated_cv\", repeats = 5, folds = 3)\nrcv53\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResamplingRepeatedCV>: Repeated Cross-Validation\n* Iterations: 15\n* Instantiated: FALSE\n* Parameters: folds=3, repeats=5\n```\n\n\n:::\n:::\n\n\n\nNow, I'll use the `resample()` function to run the resampling strategy.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr <- resample(tsk_mtcars, lrn_rpart, rcv53)\nrr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 15 resampling iterations\n task_id learner_id resampling_id iteration  prediction_test warnings errors\n  mtcars regr.rpart   repeated_cv         1 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         2 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         3 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         4 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         5 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         6 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         7 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         8 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv         9 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        10 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        11 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        12 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        13 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        14 <PredictionRegr>        0      0\n  mtcars regr.rpart   repeated_cv        15 <PredictionRegr>        0      0\n```\n\n\n:::\n:::\n\n\n\nCalculating the MSE for each iteration requires running `$score()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr_mse <- rr$score(msr(\"regr.mse\"))\nrr_mse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    task_id learner_id resampling_id iteration  regr.mse\n     <char>     <char>        <char>     <int>     <num>\n 1:  mtcars regr.rpart   repeated_cv         1 14.547959\n 2:  mtcars regr.rpart   repeated_cv         2 20.500724\n 3:  mtcars regr.rpart   repeated_cv         3 12.901045\n 4:  mtcars regr.rpart   repeated_cv         4 17.847028\n 5:  mtcars regr.rpart   repeated_cv         5 13.558418\n 6:  mtcars regr.rpart   repeated_cv         6 13.856750\n 7:  mtcars regr.rpart   repeated_cv         7 22.489190\n 8:  mtcars regr.rpart   repeated_cv         8 19.020020\n 9:  mtcars regr.rpart   repeated_cv         9  9.775333\n10:  mtcars regr.rpart   repeated_cv        10 31.090450\n11:  mtcars regr.rpart   repeated_cv        11 20.193302\n12:  mtcars regr.rpart   repeated_cv        12 33.121017\n13:  mtcars regr.rpart   repeated_cv        13 27.174212\n14:  mtcars regr.rpart   repeated_cv        14 29.727324\n15:  mtcars regr.rpart   repeated_cv        15  8.674896\nHidden columns: task, learner, resampling, prediction_test\n```\n\n\n:::\n:::\n\n\n\nLet's plot this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(rr, measure = msr(\"regr.mse\"), type = \"boxplot\") +\nautoplot(rr, measure = msr(\"regr.mse\"), type = \"histogram\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\n\nAggregating the MSE scores (using *macro* aggregation) gives:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"regr.mse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n19.63184 \n```\n\n\n:::\n:::\n\n\n\n## Question 2\n\nUse `tsk(\"spam\")` and five-fold CV to benchmark `lrn(\"classif.ranger\")`, `lrn(\"classif.log_reg\")`, and `lrn(\"classif.xgboost\", nrounds = 100)` with respect to AUC.\n\n- Which learner appears to perform best?\n- How confident are you in your conclusion?\n- Think about the stability of results and investigate this by re-running the experiment with different seeds.\n- What can be done to improve this?\n\n### Answer\n\nLet's load the task, learners and resampling method.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_spam <- tsk(\"spam\")\ntsk_spam\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:spam> (4601 x 58): HP Spam Detection\n* Target: type\n* Properties: twoclass\n* Features (57):\n  - dbl (57): address, addresses, all, business, capitalAve,\n    capitalLong, capitalTotal, charDollar, charExclamation, charHash,\n    charRoundbracket, charSemicolon, charSquarebracket, conference,\n    credit, cs, data, direct, edu, email, font, free, george, hp, hpl,\n    internet, lab, labs, mail, make, meeting, money, num000, num1999,\n    num3d, num415, num650, num85, num857, order, original, our, over,\n    parts, people, pm, project, re, receive, remove, report, table,\n    technology, telnet, will, you, your\n```\n\n\n:::\n\n```{.r .cell-code}\n# set up leaners\n# first set up the 'lrns()' then modify the xgboost 'nrounds' argument\nlearners <- lrns(c(\"classif.ranger\", \"classif.log_reg\", \"classif.xgboost\"), \n                 predict_type = \"prob\")\n# adjust 'nrounds' argument for xgboost\nlearners$classif.xgboost$param_set$values$nrounds <- 100\nlearners\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$classif.ranger\n<LearnerClassifRanger:classif.ranger>: Random Forest\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, missings, multiclass,\n  oob_error, selected_features, twoclass, weights\n\n$classif.log_reg\n<LearnerClassifLogReg:classif.log_reg>: Logistic Regression\n* Model: -\n* Parameters: use_pred_offset=TRUE\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: offset, twoclass, weights\n\n$classif.xgboost\n<LearnerClassifXgboost:classif.xgboost>: Extreme Gradient Boosting\n* Model: -\n* Parameters: nrounds=100, nthread=1, verbose=0\n* Validate: NULL\n* Packages: mlr3, mlr3learners, xgboost\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, internal_tuning, missings,\n  multiclass, offset, twoclass, validation, weights\n```\n\n\n:::\n\n```{.r .cell-code}\n# set up resampling\ncv5 <- rsmp(\"cv\", folds = 5)\ncv5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResamplingCV>: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n```\n\n\n:::\n:::\n\n\n\nNow we can set up the benchmark grid.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\ndesign <- benchmark_grid(tsk_spam, learners, cv5)\ndesign\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     task         learner resampling\n   <char>          <char>     <char>\n1:   spam  classif.ranger         cv\n2:   spam classif.log_reg         cv\n3:   spam classif.xgboost         cv\n```\n\n\n:::\n:::\n\n\n\nNow, see how well these perform in terms of AUC.^[**Recall:** AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmr <- benchmark(design)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\nbmr$score(msr(\"classif.auc\"))[, .(learner_id, iteration, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         learner_id iteration classif.auc\n             <char>     <int>       <num>\n 1:  classif.ranger         1   0.9905328\n 2:  classif.ranger         2   0.9807452\n 3:  classif.ranger         3   0.9861848\n 4:  classif.ranger         4   0.9818771\n 5:  classif.ranger         5   0.9914047\n 6: classif.log_reg         1   0.9729649\n 7: classif.log_reg         2   0.9604636\n 8: classif.log_reg         3   0.9831002\n 9: classif.log_reg         4   0.9646964\n10: classif.log_reg         5   0.9757900\n11: classif.xgboost         1   0.9896168\n12: classif.xgboost         2   0.9844034\n13: classif.xgboost         3   0.9909272\n14: classif.xgboost         4   0.9853751\n15: classif.xgboost         5   0.9919197\n```\n\n\n:::\n:::\n\n\n\nAnd let's aggregate by `Learner`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmr$aggregate(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr task_id      learner_id resampling_id iters classif.auc\n   <int>  <char>          <char>        <char> <int>       <num>\n1:     1    spam  classif.ranger            cv     5   0.9861489\n2:     2    spam classif.log_reg            cv     5   0.9714030\n3:     3    spam classif.xgboost            cv     5   0.9884484\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.auc\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\nSo, from a naive look at this, it appears that the XGBoost model performs the\nbest (highest AUC). However, the results from all three of these models appear\nvery similar, and I would maybe prefer \"simpler\" models over more flexible ones\nin this case (here, the logistic regression model).\n\nIf we run this 5 times with different seeds, let's see how the AUC varies.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmr_auc <- rbindlist(lapply(seq_len(5), function(i) {\n                         tmp_seed <- i * 100\n                         set.seed(tmp_seed)\n                         design <- benchmark_grid(tsk_spam, learners, cv5)\n                         bmr <- benchmark(design)\n                         data.table(\n                                       seed = tmp_seed,\n                                       auc = bmr$aggregate(msr(\"classif.auc\"))\n                                       )\n                    })\n                )\n\n\nbmr_auc[, .(seed, auc.learner_id, auc.classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     seed  auc.learner_id auc.classif.auc\n    <num>          <char>           <num>\n 1:   100  classif.ranger       0.9866242\n 2:   100 classif.log_reg       0.9708618\n 3:   100 classif.xgboost       0.9883961\n 4:   200  classif.ranger       0.9864776\n 5:   200 classif.log_reg       0.9705954\n 6:   200 classif.xgboost       0.9879994\n 7:   300  classif.ranger       0.9855379\n 8:   300 classif.log_reg       0.9710461\n 9:   300 classif.xgboost       0.9878387\n10:   400  classif.ranger       0.9866107\n11:   400 classif.log_reg       0.9697749\n12:   400 classif.xgboost       0.9888800\n13:   500  classif.ranger       0.9862768\n14:   500 classif.log_reg       0.9702488\n15:   500 classif.xgboost       0.9879613\n```\n\n\n:::\n\n```{.r .cell-code}\n# some summary stats\nbmr_auc[, as.list(summary(auc.classif.auc)), by = auc.learner_id]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    auc.learner_id      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n            <char>     <num>     <num>     <num>     <num>     <num>     <num>\n1:  classif.ranger 0.9855379 0.9862768 0.9864776 0.9863055 0.9866107 0.9866242\n2: classif.log_reg 0.9697749 0.9702488 0.9705954 0.9705054 0.9708618 0.9710461\n3: classif.xgboost 0.9878387 0.9879613 0.9879994 0.9882151 0.9883961 0.9888800\n```\n\n\n:::\n:::\n\n\n\nAlthough XGBoost achieved the highest AUC on average, the difference compared\nto ranger was minimal across repeated runs (although XGBoost always very\nslighly outperforms the random forest model, after aggregation). Confidence\nintervals or additional repeats could provide better insight into whether the\nobserved difference is meaningful. The choice of model will depend on how\nimportant that small difference is in the AUC compared to model complexity.\n\n## Question 3\n\nA colleague reports a $93.1\\%$ classification accuracy using `lrn(\"classif.rpart\")` on `tsk(\"penguins_simple\")`.\n\n- You want to reproduce their results and ask them about their resampling strategy.\n- They said they used a custom three-fold CV with folds assigned as `factor(task$row_ids %% 3)`.\n- See if you can reproduce their results.\n\n### Answer\n\nLet's have a look at the `Task`. This task doesn't seem to be in included in the default {mlr3} package, but is referenced in the {mlr3data} [@becker2024mlr3data] [docs](https://mlr3data.mlr-org.com/reference/penguins_simple.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_penguins <- tsk(\"penguins_simple\")\ntsk_penguins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:penguins> (333 x 11): Simplified Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (10):\n  - dbl (7): bill_depth, bill_length, island.Biscoe, island.Dream,\n    island.Torgersen, sex.female, sex.male\n  - int (3): body_mass, flipper_length, year\n```\n\n\n:::\n:::\n\n\n\nOK, so this is a multi-class classification task, using 10 features to predict the species of the penguin.\n\nThey said they used a custom three-fold CV, so let's try and reproduce this. By\nlooking at `factor(tsk_penguins$row_ids %% 3)`, we can see that the CV is\nputting every third observation into the same fold. This feels weird and wrong,\nbut fine.^[ This folding strategy does not ensure class balance within each\nfold and may lead to biased performance estimates, particularly in smaller\ndatasets.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load learner\nlrn_rpart <- lrn(\"classif.rpart\")\nlrn_rpart\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n\n\n:::\n\n```{.r .cell-code}\n# create custom resampling strategy\nrsmp_custom = rsmp(\"custom_cv\")\nfolds <- factor(tsk_penguins$row_ids %% 3)\nrsmp_custom$instantiate(tsk_penguins, f = folds)\nrr <- resample(tsk_penguins, lrn_rpart, rsmp_custom)\nrr$predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n<PredictionClassif> for 111 observations:\n row_ids     truth  response\n       3    Adelie    Adelie\n       6    Adelie    Adelie\n       9    Adelie    Adelie\n     ---       ---       ---\n     327 Chinstrap Chinstrap\n     330 Chinstrap    Adelie\n     333 Chinstrap Chinstrap\n\n[[2]]\n<PredictionClassif> for 111 observations:\n row_ids     truth  response\n       1    Adelie    Adelie\n       4    Adelie    Adelie\n       7    Adelie    Adelie\n     ---       ---       ---\n     325 Chinstrap Chinstrap\n     328 Chinstrap Chinstrap\n     331 Chinstrap Chinstrap\n\n[[3]]\n<PredictionClassif> for 111 observations:\n row_ids     truth response\n       2    Adelie   Adelie\n       5    Adelie   Adelie\n       8    Adelie   Adelie\n     ---       ---      ---\n     326 Chinstrap   Gentoo\n     329 Chinstrap   Gentoo\n     332 Chinstrap   Gentoo\n```\n\n\n:::\n\n```{.r .cell-code}\nrr$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    task_id    learner_id resampling_id iteration classif.acc\n     <char>        <char>        <char>     <int>       <num>\n1: penguins classif.rpart     custom_cv         1   0.9369369\n2: penguins classif.rpart     custom_cv         2   0.9189189\n3: penguins classif.rpart     custom_cv         3   0.9369369\nHidden columns: task, learner, resampling, prediction_test\n```\n\n\n:::\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9309309 \n```\n\n\n:::\n:::\n\n\n\nSo, we get a model with $93.1\\%$ accuracy, as required.^[Would the results\nchange much with, for example, [grouped\nresampling](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-strat-group)?\nI should look at this at some point.]\n\n## Question 4\n\n(\\*) Program your own ROC plotting function **without** using `mlr3`'s `autoplot()` function.\n\n- The signature of your function should be `my_roc_plot(task, learner, train_indices, test_indices)`.\n- Your function should use the `$set_threshold()` method of `Prediction`, as well as `mlr3measures`.\n\n### Answer\n\nLet's first have a look at the output from using `autoplot()`. I'll use the `german_credit` task.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_german = tsk(\"german_credit\")\ntsk_german\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:german_credit> (1000 x 21): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker,\n    housing, job, other_debtors, other_installment_plans,\n    people_liable, personal_status_sex, property, purpose, savings,\n    status, telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n```\n\n\n:::\n\n```{.r .cell-code}\nlrn_ranger = lrn(\"classif.ranger\", predict_type = \"prob\")\nsplits = partition(tsk_german, ratio = 0.8)\n\nlrn_ranger$train(tsk_german, splits$train)\nprediction = lrn_ranger$predict(tsk_german, splits$test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(prediction, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nFirst, I'll do all the steps to create the ROC, then I'll wrap this in a function, `my_roc_plot()`.\n\n#### Creating the ROC\n\nOK -- so I need to use [`$set_threshold()`](https://mlr3.mlr-org.com/reference/PredictionClassif.html#method-PredictionClassif-set_threshold) to obtain predictions over the range of thresholds. Then, I need to use {mlr3measures} [@lang2024mlr3measures] to compute the TPR (Sensitivity) and FPR ($1 -$ Specificity) and plot these all on a lovely graph.\n\n:::{.column-margin}\n\nSensitivity\n: (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\n\nSpecificity\n: (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.\n\n:::\n\nI'll first check to see which is the `positive` outcome in the `Task`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsk_german$positive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"good\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# also, by looking at the help file 'prediction$help()'\n# can see that the positive class is the first level of '$truth', i.e.\nlevels(prediction$truth)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"good\"\n```\n\n\n:::\n:::\n\n\n\nSo having `good` credit is the positive outcome here.\n\nNow, I'll create a vector of thresholds^[Thresholds were discussed in [Section\n2.5.4](https://mlr3book.mlr-org.com/chapters/chapter2/data_and_basic_modeling.html#sec-classif-prediction)\nof the mlr3 tutorial, and I looked at them in Question 3 of my [previous\npost](../05-mlr3_basic_modelling/index.qmd)] and then obtain predictions and\ncalculate the measures. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npositive_class <- levels(prediction$truth)[1]\nthresholds <- seq(0, 1, length = 101)\ntsk_german_measures <- rbindlist(\n         lapply(thresholds, function(j) {\n                prediction$set_threshold(j)\n                tpr_tmp <- mlr3measures::tpr(truth = prediction$truth,\n                                             response = prediction$response,\n                                             positive = positive_class)\n                fpr_tmp <- mlr3measures::fpr(truth = prediction$truth,\n                                             response = prediction$response,\n                                             positive = positive_class)\n                data.table(threshold = j,\n                           tpr = tpr_tmp,\n                           fpr = fpr_tmp)\n                    }\n                 )\n         )\n\n# order by increasing fpr, and tpr\n# s.t. the step function avoids spikes\n# spikes are happening as seed not set in $set_threshold(),\n# so possible to get non-monotonic tpr/ fpr\n# also put them in descending threshold order, just to make the data look nicer.\ntsk_german_measures <- tsk_german_measures[order(fpr, tpr, -threshold)]\ntsk_german_measures\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     threshold         tpr   fpr\n         <num>       <num> <num>\n  1:      1.00 0.000000000     0\n  2:      0.99 0.000000000     0\n  3:      0.98 0.006896552     0\n  4:      0.97 0.013793103     0\n  5:      0.96 0.013793103     0\n ---                            \n 97:      0.04 1.000000000     1\n 98:      0.03 1.000000000     1\n 99:      0.02 1.000000000     1\n100:      0.01 1.000000000     1\n101:      0.00 1.000000000     1\n```\n\n\n:::\n:::\n\n\n\nOK, I think I've got everything required to plot the ROC.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tsk_german_measures, aes(x = fpr, y = tpr)) +\n        geom_step() +\n        geom_abline(intercept = 0, slope = 1,\n                    linetype = \"dotted\", colour = \"grey\") +\n        labs(x = \"1 - Specificity\",\n             y = \"Sensitivity\") +\n        theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n#### Making the function `my_roc_plot()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_roc_plot <- function(task, learner, train_indices, test_indices) {\n        # task: a 'Task' object\n        # learner: a 'Learner' object\n\n        # train the learner on the task\n        learner$train(task, row_ids = train_indices)\n        # create the prediction object\n        prediction <- learner$predict(task, row_ids = test_indices)\n\n        # find TPR and FPR over a seq of thresholds\n        positive_class <- levels(prediction$truth)[1]\n        thresholds <- seq(0, 1, length = 101)\n        tpr_fpr_thresholds <- rbindlist(\n                 lapply(thresholds, function(j) {\n                        prediction$set_threshold(j)\n                        tpr_tmp <- mlr3measures::tpr(truth = prediction$truth,\n                                                     response = prediction$response,\n                                                     positive = positive_class)\n                        fpr_tmp <- mlr3measures::fpr(truth = prediction$truth,\n                                                     response = prediction$response,\n                                                     positive = positive_class)\n                        data.table(threshold = j,\n                                   tpr = tpr_tmp,\n                                   fpr = fpr_tmp)\n                            }\n                         )\n                 )\n\n        tpr_fpr_thresholds <- tpr_fpr_thresholds[order(fpr, tpr, -threshold)]\n\n        # and plot\n        ggplot(tpr_fpr_thresholds, aes(x = fpr, y = tpr)) +\n                geom_step() +\n                geom_abline(intercept = 0, slope = 1,\n                            linetype = \"dotted\", colour = \"grey\") +\n                labs(x = \"1 - Specificity\",\n                     y = \"Sensitivity\") +\n                theme_minimal()\n\n}\n```\n:::\n\n\n\nLet's test it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_roc_plot(task = tsk_german,\n            learner = lrn(\"classif.ranger\", predict_type = \"prob\"),\n            train_indices = splits$train,\n            test_indices = splits$test)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\nCool, looks good!\n\n# Fin\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}